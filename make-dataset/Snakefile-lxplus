configfile: 'config.yaml'

rule copy_root_from_eos:
    ''' Copy root tuples from eos to local dir before accessing them
        due to regular issues accessing eos files directly '''
    input:
        config['path'] + config['eos_root_samples_1']
    output:
        directory('output/root_samples/')
    shell:
        'for i in {{1..334}}; do \n\
            echo "cp {input}L1Ntuple_$i.root {output}L1Ntuple_$i.root" \n\
            cp {input}L1Ntuple_$i.root {output}L1Ntuple_$i.root || : \n\
        done'
 
rule convert:
    ''' Convert given root tuple to h5 format '''
    input:
        script = 'convert_to_h5.py',
        input_file = config['path'] + config['eos_root_samples_1'] +'L1Ntuple_{id}.root'
    output:
        output_file = 'output/h5_samples/QCD_1/L1Ntuple_{id}.h5'
    shell:
        'cd /afs/cern.ch/work/e/egovorko/CMSSW_11_0_2/src; eval `scramv1 runtime -sh`; cd -;'
        'python {input.script} --input-file {input.input_file} \
                               --output-file {output}'

IDS = [i for i in range(1,334) if i!=249] # list(range(1,334))
rule convert_all:
    ''' Convert all root tuples to h5 format '''
    input:
        expand(rules.convert.output, id=IDS)

rule merge_h5_tuples:
    ''' Merge all converted to h5 format tuples into one file '''
    input:
        script = 'merge_h5_tuples.py',
        input_files = expand(rules.convert.output, id=IDS)
    output:
        file = 'output/L1Ntuple_QCD_1.h5'
    shell:
        'python {input.script} --input-files {input.input_files} \
                               --output-file {output.file}'

rule preprocess:
    ''' Concatenate and normalize input dataset '''
    input:
        script = 'preprocess.py',
        file = rules.merge_h5_tuples.output.file
    output:
        h5 = 'output/QCD_preprocessed.h5'
    shell:
        'python {input.script} --input-file {input.file} \
                               --output-file {output.h5}'

rule convert_bsm:
    ''' Convert given root tuple to h5 format '''
    input:
        script = 'convert_to_h5.py',
        input_file = lambda wildcards: config['path'] + config[wildcards.bsm_type] + 'L1Ntuple_{id}.root'
    output:
        output_file = 'output/h5_samples/{bsm_type}/L1Ntuple_{id}.h5'
    params:
        tree_name = 'l1UpgradeTree/L1UpgradeTree'
    shell:
        'cd /afs/cern.ch/work/e/egovorko/CMSSW_11_0_2/src; eval `scramv1 runtime -sh`; cd -;'
        'python {input.script} --input-file {input.input_file} \
                               --output-file {output} \
                               --tree-name {params.tree_name}'

IDS_BSM = {
    'VectorZPrimeToQQ__M50': list(range(1,7)),
    'VectorZPrimeToQQ__M100': list(range(1,3)),
    'VectorZPrimeToQQ__M200': list(range(1,40)),
    'VBF_HToInvisible_M125': list(range(1,292)),
    'VBF_HToInvisible_M125_private': list(range(1,488))+[489],
    'ZprimeToZH_MZprime1000': list(range(1,51)),
    'ZprimeToZH_MZprime800': list(range(1,51)),
    'ZprimeToZH_MZprime600': list(range(1,51)),
    'GluGluToHHTo4B': list(range(1,51)),
    'HTo2LongLivedTo4mu_1000': list(range(1,41)),
    'HTo2LongLivedTo4mu_125_12': list(range(1,41)),
    'HTo2LongLivedTo4mu_125_25': list(range(1,14)),
    'HTo2LongLivedTo4mu_125_50': list(range(1,41)),
    'VBFHToTauTau': list(range(1,301)),
    'VBF_HH': list(range(1,31))
    }

rule merge_h5_bsm_type:
    ''' Merge all converted to h5 format tuples into one file '''
    input:
        script = 'merge_h5_tuples.py',
        input_files = lambda wildcards: expand(rules.convert_bsm.output.output_file,
          id=IDS_BSM[wildcards.bsm_type],
          bsm_type=wildcards.bsm_type)
    output:
        file = 'output/L1Ntuple_{bsm_type}.h5'
    shell:
        'python {input.script} --input-files {input.input_files} \
                               --output-file {output.file}'

rule preprocess_bsm:
    ''' Concatenate and normalize input dataset '''
    input:
        script = 'preprocess.py',
        file = lambda wildcards: expand(rules.merge_h5_bsm_type.output,
            bsm_type=wildcards.bsm_type)
    output:
        'output/BSM_preprocessed_{bsm_type}.h5'
    shell:
        'python {input.script} --input-file {input.file} \
                               --output-file {output}'

rule merge_all_bsm_types:
    ''' Merge all converted to h5 format tuples into one file '''
    input:
        script = 'merge_h5_tuples.py',
        input_files = expand(rules.preprocess_bsm.output,
            bsm_type=IDS_BSM.keys())
    output:
        file = 'output/BSM_preprocessed.h5'
    shell:
        'python {input.script} --input-files {input.input_files} \
                               --output-file {output.file} \
                               --bsm'

rule prepare_data:
    ''' Prepare data for training'''
    input:
        script = 'prepare_data.py',
        qcd = config['samples']+'QCD_preprocessed.h5',
    output:
        'output/data_{events}.pickle'
    params:
        bsm = rules.merge_all_bsm_types.output
    shell:
        'python {input.script} --output-file {output} \
                               --input-file {input.qcd} \
                               --events {wildcards.events} \
                               --input-bsm {params.bsm} '

rule train:
    ''' Train specified model, save result in h5 file '''
    input:
    output:
        h5 = 'output/model-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.h5',
        json = 'output/model-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.json',
        history = 'output/history-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.pickle'
    shell:
        'python train.py --output-model-h5 {output.h5} \
                         --output-model-json {output.json} \
                         --output-history {output.history} \
                         --batch-size 1024 \
                         --quant-size {wildcards.quant_size} \
                         --n-epochs 100 \
                         --pruning {wildcards.pruning} \
                         --latent-dim {wildcards.latent_dim} \
                         --model-type {wildcards.model} \
                         --beta {wildcards.beta}'

rule evaluate:
    input:
        script = 'evaluate.py',
        data = expand(rules.prepare_data.output, events=-1),
        h5 = 'output/model-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.h5',
        json = 'output/model-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.json',
        history = 'output/history-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.pickle'
    output:
        result = 'output/result-{model}-{latent_dim}-b{beta}-q{quant_size}-{pruning}.h5'
    shell:
        'python {input.script} --input-file {input.data} \
                               --input-h5 {input.h5} \
                               --input-json {input.json} \
                               --input-history {input.history} \
                               --output-result {output.result}'

rule plot:
    ''' Plot the result of the training '''
    input:
        script = 'plot.py',
        trained_vae = 'output/'
    output:
        directory('plots/')
    shell:
        'python {input.script} --model cae \
                               --loss-type split_3D \
                               --input-dir {input.trained_vae} \
                               --output-dir {output}'
