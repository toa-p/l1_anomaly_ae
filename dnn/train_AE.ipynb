{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import h5py\n",
    "import setGPU\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import load_model, save_model, make_mse_loss\n",
    "import pickle\n",
    "from autoencoder_classes import AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = (N,19,3,1).flatten()\n",
    "with open('/afs/cern.ch/work/e/egovorko/public/data_20000000.pickle', 'rb') as f:\n",
    "    _, _, X_test_flatten, X_test_scaled, all_bsm_data, pt_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train_flatten.reshape(X_train_flatten.shape[0], X_train_flatten.shape[1]*X_train_flatten.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1]*X_train_scaled.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flatten = X_test_flatten.reshape(X_test_flatten.shape[0], X_test_flatten.shape[1]*X_test_flatten.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1]*X_test_scaled.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = BatchNormalization()(inputArray)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# encoder = LeakyReLU(alpha=0.3)(x)\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AE(autoencoder)\n",
    "ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "callbacks=[]\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(NeptuneMonitor())\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING\")\n",
    "history = ae.fit(X_train_flatten, X_train_scaled, epochs = EPOCHS, batch_size = BATCH_SIZE,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_model('AE_notpruned_CMS', ae.autoencoder)\n",
    "autoencoder = load_model('AE_notpruned_CMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'][:], label='Training loss')\n",
    "plt.plot(history.history['val_loss'][:], label='Validation loss')\n",
    "plt.title('Training and validation loss - MSE')\n",
    "#plt.yscale('log', nonposy='clip')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction - background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_prediction = autoencoder.predict(X_test_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction - Beyond Standard Model samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bsm_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_labels = ['VectorZPrimeToQQ__M50',\n",
    "                     'VectorZPrimeToQQ__M100',\n",
    "                     'VectorZPrimeToQQ__M200',\n",
    "                     'VBF_HToInvisible_M125',\n",
    "                     'VBF_HToInvisible_M125_private',\n",
    "                     'ZprimeToZH_MZprime1000',\n",
    "                     'ZprimeToZH_MZprime800',\n",
    "                     'ZprimeToZH_MZprime600',\n",
    "                     'GluGluToHHTo4B',\n",
    "                     'HTo2LongLivedTo4mu_1000',\n",
    "                     'HTo2LongLivedTo4mu_125_12',\n",
    "                     'HTo2LongLivedTo4mu_125_25',\n",
    "                     'HTo2LongLivedTo4mu_125_50',\n",
    "                     'VBFHToTauTau',\n",
    "                     'VBF_HH'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_scaled_data=[]\n",
    "for bsm in all_bsm_data:\n",
    "    bsm = bsm.reshape(bsm.shape[0],19,3,1)\n",
    "    bsm = np.squeeze(bsm, axis=-1)\n",
    "    bsm_data_target = np.copy(bsm)\n",
    "    bsm_data_target[:,:,0] = pt_scaler.transform(bsm_data_target[:,:,0])\n",
    "    bsm_data_target[:,:,0] = np.multiply(bsm_data_target[:,:,0], np.not_equal(bsm[:,:,0],0))\n",
    "    bsm_data_target.reshape(bsm_data_target.shape[0], bsm_data_target.shape[1]*bsm_data_target.shape[2])\n",
    "    bsm_scaled_data.append(bsm_data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bsm_scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_results = []\n",
    "\n",
    "for i, label in enumerate(bsm_labels):\n",
    "    bsm_prediction = autoencoder(all_bsm_data[i].reshape(all_bsm_data[i].shape[0], all_bsm_data[i].shape[1]*all_bsm_data[i].shape[2]))\n",
    "    \n",
    "    bsm_results.append([label, bsm_scaled_data[i], bsm_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_result = '/eos/user/e/epuljak/AE_result_CMSdata_notpruned.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(output_result, 'w')\n",
    "h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "h5f.create_dataset('QCD_target', data=X_test_scaled)\n",
    "h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "for i, bsm in enumerate(bsm_results):\n",
    "    h5f.create_dataset('%s_scaled' %bsm[0], data=bsm[1])\n",
    "    h5f.create_dataset('%s_input' %bsm[0], data=all_bsm_data[i].reshape(all_bsm_data[i].shape[0], all_bsm_data[i].shape[1]*all_bsm_data[i].shape[2]))\n",
    "    h5f.create_dataset('predicted_%s' %bsm[0], data=bsm[2])\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "neptune": {
   "notebookId": "db31bc2a-7b92-40f5-8253-dfc65381a6fb",
   "projectVersion": 1
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
