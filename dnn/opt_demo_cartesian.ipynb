{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2822e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "hep.style.use(hep.style.ROOT)\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "# from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "# from losses import mse_split_loss, radius, kl_loss\n",
    "# from functions import make_mse_loss_numpy, save_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# from data_preprocessing import prepare_data\n",
    "# from model import build_AE, build_VAE, build_QVAE\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "tsk = tfmot.sparsity.keras\n",
    "\n",
    "from qkeras.quantizers import quantized_bits\n",
    "\n",
    "from keras.utils import tf_utils\n",
    "\n",
    "quantize=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf90996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GluGluToHHTo4B_cHHH1\n",
      "TT\n",
      "haa4b_ma15_powheg\n"
     ]
    }
   ],
   "source": [
    "input_hardqcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-ZB-h5-extended-v2/ZB5_preprocessed.h5\"\n",
    "input_bsm = \"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2-120X/BSM_preprocessed.h5\"\n",
    "events=500000\n",
    "norm = 'std'\n",
    "output = {}\n",
    "\n",
    "\n",
    "with h5py.File(input_qcd, 'r') as h5f:\n",
    "    output['ZeroBias'] = {}\n",
    "    \n",
    "    data = np.array(h5f['full_data_cyl'][:events], dtype=np.float16)\n",
    "    ET = np.array(h5f['ET'][:events], dtype=np.float16)\n",
    "    L1bit = np.array(h5f['L1bit'][:events], dtype=np.int8)\n",
    "\n",
    "    #mask saturated ET\n",
    "    mask_ET = ET<2047.5\n",
    "    ET = ET[mask_ET]\n",
    "    data = data[mask_ET]\n",
    "    L1bit = L1bit[mask_ET]\n",
    "    \n",
    "    #mask saturated PT\n",
    "    mask_0  = data[:,0,0]<2047.5\n",
    "    mask_1_9  = data[:,1:9,0]<255.5\n",
    "    mask_9_20  = data[:,9:20,0]<1023.5\n",
    "    mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "    data = data*mask[:,:,np.newaxis]\n",
    "\n",
    "    pt = np.copy(data[:,:,0])\n",
    "    eta = np.copy(data[:,:,1])\n",
    "    phi = np.copy(data[:,:,2])\n",
    "    \n",
    "    data[:,:,0] = pt*np.cos(phi)\n",
    "    data[:,:,1] = pt*np.sin(phi)\n",
    "    data[:,:,2] = pt*np.sinh(eta)\n",
    "    data_target = np.copy(data)\n",
    "\n",
    "    del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "\n",
    "    if(norm=='ET'):\n",
    "        data_target[:,:,:] = data[:,:,:]/ET[:,None,None]\n",
    "        std_xy = (np.std(data_target[:,:,0])+np.std(data_target[:,:,1]))/2\n",
    "        std_z = np.std(data_target[:,:,2])\n",
    "        data_target[:,:,2] = data_target[:,:,2]*(std_xy/std_z)\n",
    "    elif(norm=='std'):\n",
    "        mean_qcd = np.mean(data_target, axis=0)\n",
    "        std_qcd = np.std(data_target, axis=0)\n",
    "        data_target = (data_target[:,:,:] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "        data_target[:,0,2] = 0\n",
    "    else:\n",
    "        data_target[:,0,:] = data[:,0,:]/2048\n",
    "        data_target[:,1:9,:] = data[:,1:9,:]/256\n",
    "        data_target[:,9:20,:] = data[:,9:20,:]/1024\n",
    "        \n",
    "\n",
    "    X_train, output['ZeroBias']['data'], Y_train, output['ZeroBias']['target'], _ , output['ZeroBias']['ET'], _ ,output['ZeroBias']['L1bit'] =  train_test_split( data, data_target, ET,L1bit, test_size=0.5)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[1]*Y_train.shape[2])\n",
    "\n",
    "del data, data_target, ET, L1bit\n",
    "\n",
    "\n",
    "with h5py.File(input_hardqcd, 'r') as h5f:\n",
    "    output['QCD'] = {}\n",
    "    \n",
    "    data = np.array(h5f['full_data_cyl'][:events], dtype=np.float16)\n",
    "    ET = np.array(h5f['ET'][:events], dtype=np.float16)\n",
    "    L1bit = np.array(h5f['L1bit'][:events], dtype=np.int8)\n",
    "\n",
    "    #mask saturated ET\n",
    "    mask_ET = ET<2047.5\n",
    "    ET = ET[mask_ET]\n",
    "    data = data[mask_ET]\n",
    "    L1bit = L1bit[mask_ET]\n",
    "    \n",
    "    #mask saturated PT\n",
    "    mask_0  = data[:,0,0]<2047.5\n",
    "    mask_1_9  = data[:,1:9,0]<255.5\n",
    "    mask_9_20  = data[:,9:20,0]<1023.5\n",
    "    mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "    data = data*mask[:,:,np.newaxis]\n",
    "\n",
    "    pt = np.copy(data[:,:,0])\n",
    "    eta = np.copy(data[:,:,1])\n",
    "    phi = np.copy(data[:,:,2])\n",
    "    \n",
    "    data[:,:,0] = pt*np.cos(phi)\n",
    "    data[:,:,1] = pt*np.sin(phi)\n",
    "    data[:,:,2] = pt*np.sinh(eta)\n",
    "    data_target = np.copy(data)\n",
    "\n",
    "    del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "\n",
    "    \n",
    "    # print(h5f.keys())\n",
    "\n",
    "    if(norm=='ET'):\n",
    "        data_target[:,:,:] = data[:,:,:]/ET[:,None,None]\n",
    "        data_target[:,:,2] = data_target[:,:,2]*(std_xy/std_z)\n",
    "    elif(norm=='std'):\n",
    "\n",
    "        data_target = (data_target[:,:,:] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "        # data_target[:,:,0] = (data_target[:,:,0]-mean_qcd[0])/std_qcd[0]\n",
    "        # data_target[:,:,1] = (data_target[:,:,1]-mean_qcd[1])/std_qcd[1]\n",
    "        # data_target[:,:,2] = (data_target[:,:,2]-mean_qcd[2])/std_qcd[2] \n",
    "        data_target[:,0,2] = 0\n",
    "    else:\n",
    "        data_target[:,0,:] = data[:,0,:]/2048\n",
    "        data_target[:,1:9,:] = data[:,1:9,:]/256\n",
    "        data_target[:,9:20,:] = data[:,9:20,:]/1024\n",
    "        \n",
    "\n",
    "    output['QCD']['data'], output['QCD']['target'], output['QCD']['ET'],output['QCD']['L1bit'] =   data, data_target, ET,L1bit\n",
    "\n",
    "del data, data_target, ET, L1bit\n",
    "\n",
    "with h5py.File(input_bsm,'r') as h5f2:\n",
    "    for key in h5f2.keys():\n",
    "        if('TT' not in key[:2]) and ('haa4b_ma15_powheg' not in key) and ('GluGluToHHTo4B_cHHH1' not in key): continue\n",
    "        if len(h5f2[key].shape) < 3: continue\n",
    "        print(key)\n",
    "        output[str(key)] = {}\n",
    "        output[str(key)]['data'] = np.array(h5f2[str(key)][:events,:,:],dtype=np.float16)\n",
    "        output[str(key)]['ET'] = np.array(h5f2[str(key)+'_ET'][:events],dtype=np.float16)\n",
    "        output[str(key)]['L1bit'] = np.array(h5f2[str(key)+'_l1bit'][:events],dtype=np.int8)\n",
    "\n",
    "        #mask saturated ET\n",
    "        mask_ET = output[str(key)]['ET']<2047.5\n",
    "        output[str(key)]['ET'] = output[str(key)]['ET'][mask_ET]\n",
    "        output[str(key)]['data'] = output[str(key)]['data'][mask_ET]\n",
    "        output[str(key)]['L1bit'] = output[str(key)]['L1bit'][mask_ET]\n",
    "        \n",
    "        #mask saturated PT\n",
    "        mask_0  = output[str(key)]['data'][:,0,0]<2047.5\n",
    "        mask_1_9  = output[str(key)]['data'][:,1:9,0]<255.5\n",
    "        mask_9_20  = output[str(key)]['data'][:,9:20,0]<1023.5\n",
    "        mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "        output[str(key)]['data'] = output[str(key)]['data']*mask[:,:,np.newaxis]\n",
    "\n",
    "        pt = np.copy(output[str(key)]['data'][:,:,0])\n",
    "        eta = np.copy(output[str(key)]['data'][:,:,1])\n",
    "        phi = np.copy(output[str(key)]['data'][:,:,2])\n",
    "        \n",
    "        output[str(key)]['data'][:,:,0] = pt*np.cos(phi)\n",
    "        output[str(key)]['data'][:,:,1] = pt*np.sin(phi)\n",
    "        output[str(key)]['data'][:,:,2] = pt*np.sinh(eta)\n",
    "\n",
    "        del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "\n",
    "\n",
    "        output[str(key)]['target'] = np.copy(output[str(key)]['data'])\n",
    "        if(norm=='ET'):\n",
    "            output[str(key)]['target'] = output[str(key)]['data']/output[str(key)]['ET'][:,None,None]\n",
    "            output[str(key)]['target'][:,:,2] = output[str(key)]['target'][:,:,2]*(std_xy/std_z)\n",
    "        elif(norm=='std'):\n",
    "            output[str(key)]['target'] = (output[str(key)]['target'] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "            # output[str(key)]['target'][:,:,0]= (output[str(key)]['data'][:,:,0]-mean_qcd[0])/std_qcd[0]\n",
    "            # output[str(key)]['target'][:,:,1]= (output[str(key)]['data'][:,:,1]-mean_qcd[1])/std_qcd[1]\n",
    "            # output[str(key)]['target'][:,:,2]= (output[str(key)]['data'][:,:,2]-mean_qcd[2])/std_qcd[2]\n",
    "            output[str(key)]['target'][:,0,2] = 0\n",
    "        elif(norm=='max_PT'):\n",
    "            output[str(key)]['target'][:,0,:] = output[str(key)]['data'][:,0,:]/2048\n",
    "            output[str(key)]['target'][:,1:9,:] = output[str(key)]['data'][:,1:9,:]/256\n",
    "            output[str(key)]['target'][:,9:20,:] = output[str(key)]['data'][:,9:20,:]/1024\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15b57a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_scale = 1000\n",
    "#beta = 0.8\n",
    "\n",
    "def mse_loss(inputs, outputs):\n",
    "    # remove last dimension\n",
    "    inputs = tf.reshape(inputs, (tf.shape(inputs)[0],19,3))\n",
    "    outputs = tf.reshape(outputs, (tf.shape(outputs)[0],19,3))\n",
    "    \n",
    "    mask0 = tf.math.not_equal(inputs[:,:,0],0)\n",
    "    mask1 = tf.math.not_equal(inputs[:,:,1],0)\n",
    "    mask2 = tf.math.not_equal(inputs[:,:,2],0)\n",
    "    mask = tf.math.logical_and(mask0, mask1)\n",
    "    mask = tf.math.logical_and(mask, mask2)\n",
    "    # tf.print(mask)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.reshape(mask, (tf.shape(mask)[0],19,1))\n",
    "\n",
    "    # remove zero entries\n",
    "    loss = reco_scale*tf.reduce_mean(tf.square(inputs[:,:,:]-outputs[:,:,:])*mask)\n",
    "    return loss\n",
    "\n",
    "def mse_loss_numpy(inputs, outputs):\n",
    "    # remove last dimension\n",
    "    inputs = np.reshape(inputs, (inputs.shape[0],19,3))\n",
    "    outputs = np.reshape(outputs, (outputs.shape[0],19,3))\n",
    "    \n",
    "    mask0 = inputs[:,:,0]!=0\n",
    "    mask1 = inputs[:,:,1]!=0\n",
    "    mask2 = inputs[:,:,2]!=0\n",
    "    mask = (mask0 + mask1 + mask2)*1\n",
    "    mask = np.reshape(mask, (mask.shape[0],19,1))\n",
    "    inputs = inputs*mask\n",
    "    outputs = outputs*mask\n",
    "\n",
    "    # remove zero entries\n",
    "    loss = np.mean(np.square(inputs.reshape(inputs.shape[0],57)-outputs.reshape(outputs.shape[0],57)),axis=1)\n",
    "    return loss\n",
    "\n",
    "def radius(mean, logvar):\n",
    "    sigma = np.sqrt(np.exp(logvar))\n",
    "    radius = mean*mean/sigma/sigma\n",
    "    return np.sum(radius, axis=-1)\n",
    "\n",
    "def kl_loss(mu, logvar, beta=None):\n",
    "    kl_loss = 1 + logvar - np.square(mu) - np.exp(logvar)\n",
    "    kl_loss = np.mean(kl_loss, axis=-1) # mean over latent dimensions\n",
    "    kl_loss *= -0.5\n",
    "    if beta!=None: return beta*kl_loss\n",
    "    else: return kl_loss\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.total_val_loss_tracker = keras.metrics.Mean(name=\"total_val_loss\")\n",
    "        self.reconstruction_val_loss_tracker = keras.metrics.Mean(name=\"reconstruction_val_loss\")\n",
    "        self.kl_val_loss_tracker = keras.metrics.Mean(name=\"kl_val_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.total_val_loss_tracker,\n",
    "            self.reconstruction_val_loss_tracker,\n",
    "            self.kl_val_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        data_in, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data_in)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = (1-beta)*mse_loss(target, reconstruction) #one value\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = beta*tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # tf.print(reconstruction_loss,kl_loss)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "   \n",
    "    def test_step(self, data):\n",
    "        data_in, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data_in)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = (1-beta)*mse_loss(target, reconstruction) #one value\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = beta*tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.total_val_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_val_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_val_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_val_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_val_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_val_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def predict(self, data_in, batch_size=32, return_latent=False):\n",
    "        # print(data_in)\n",
    "        data_batch = tf.data.Dataset.from_tensor_slices((data_in))\n",
    "        output, z_mean, z_logvar = [],[],[]\n",
    "        for data in data_batch.batch(batch_size):\n",
    "            # print(data.shape)\n",
    "            z_mean_, z_logvar_, z_ = self.encoder(data)\n",
    "            output_ = self.decoder(z_)\n",
    "            output.append(output_)\n",
    "            z_mean.append(z_mean_)\n",
    "            z_logvar.append(z_logvar_)\n",
    "        # print(len(output),len(z_mean),len(z_logvar))\n",
    "        # print(output[0])\n",
    "        output = tf.concat(output,0)\n",
    "        z_mean = tf.concat(z_mean,0)\n",
    "        z_logvar = tf.concat(z_logvar,0)\n",
    "    \n",
    "        if(return_latent):\n",
    "            return tf_utils.sync_to_numpy_or_python_type(output), tf_utils.sync_to_numpy_or_python_type(z_mean), tf_utils.sync_to_numpy_or_python_type(z_logvar)\n",
    "        else:\n",
    "            return tf_utils.sync_to_numpy_or_python_type(output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3224952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hyperparameters):\n",
    "    \n",
    "    latent_dim = int(hyperparameters[:,0])\n",
    "    outer_layer_width = int(hyperparameters[:,1])\n",
    "    inner_layer_width = int(hyperparameters[:,2])\n",
    "    beta = hyperparameters[:,3]\n",
    "    #batch_size = int(hyperparameters[:,4])\n",
    "\n",
    "    #latent_dim=4\n",
    "    input_shape=57\n",
    "    inputs = keras.Input(shape=(input_shape,))\n",
    "    x = layers.Dense(outer_layer_width,kernel_initializer='lecun_uniform', activation='relu')(inputs)\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(inner_layer_width,kernel_initializer='lecun_uniform', activation='relu')(x)\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    z_mean = layers.Dense(latent_dim,kernel_initializer='zeros')(x)\n",
    "    z_logvar = layers.Dense(latent_dim,kernel_initializer='zeros')(x)\n",
    "    z = Sampling()([z_mean, z_logvar])\n",
    "    encoder = keras.Model(inputs, [z_mean, z_logvar, z], name=\"encoder\")\n",
    "    encoder.summary()\n",
    "\n",
    "\n",
    "    latent_inputs = keras.Input(latent_dim,)\n",
    "    y = layers.Dense(16,kernel_initializer='lecun_uniform', activation='relu')(latent_inputs)\n",
    "    y = layers.Dense(32,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "    y = layers.Dense(64,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "    y = layers.Dense(128,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "    # y = layers.Dense(256,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "    decoded = layers.Dense(input_shape)(y)\n",
    "    decoder = keras.Model(latent_inputs, decoded, name=\"decoder\")\n",
    "    decoder.summary()\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.1, cooldown=2, min_lr=1e-6))\n",
    "    callbacks.append(TerminateOnNaN())\n",
    "    # callbacks.append(NeptuneMonitor())\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=2, restore_best_weights=True))\n",
    "    \n",
    "    history = vae.fit(Y_train, Y_train,validation_split=0.2, epochs=500, batch_size=512, verbose=1, callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    for key in output.keys():\n",
    "        output[key]['prediction'] = output[key]['prediction'][np.isfinite(output[key]['prediction'])]\n",
    "        output[key]['reco_loss'] = output[key]['reco_loss'][np.isfinite(output[key]['reco_loss'])]\n",
    "        output[key]['kl_loss'] = output[key]['kl_loss'][np.isfinite(output[key]['kl_loss'])]\n",
    "        output[key]['radius'] = output[key]['radius'][np.isfinite(output[key]['radius'])]\n",
    "        output[key]['total_loss'] = output[key]['total_loss'][np.isfinite(output[key]['total_loss'])]\n",
    "\n",
    "    for key in output.keys():\n",
    "        Y_predict, z_mean , z_logvar = vae.predict(output[key]['target'].reshape(output[key]['target'].shape[0],57),batch_size=1024*4,return_latent=True)\n",
    "        Y_predict = Y_predict.reshape(Y_predict.shape[0],19,3)\n",
    "        output[key]['prediction'] = Y_predict\n",
    "        output[key]['reco_loss'] = mse_loss_numpy(output[key]['target'], Y_predict)\n",
    "        output[key]['kl_loss'] = kl_loss(z_mean, z_logvar)\n",
    "        output[key]['radius'] = radius(z_mean, z_logvar)\n",
    "        output[key]['total_loss'] = output[key]['reco_loss'] + output[key]['kl_loss']\n",
    "    \n",
    "    # Evaluate the KL term\n",
    "    qcd = output['ZeroBias']['target']\n",
    "    bsm = output['TT']['target']\n",
    "\n",
    "    kl_qcd = output['ZeroBias']['kl_loss']\n",
    "    kl_bsm = output['TT']['kl_loss']\n",
    "\n",
    "    kl_true_val = np.concatenate((np.ones(kl_bsm.shape[0]), np.zeros(kl_qcd.shape[0])))\n",
    "    kl_pred_val = np.nan_to_num(np.concatenate((kl_bsm, kl_qcd)))\n",
    "\n",
    "    kl_fpr_loss, kl_tpr_loss, kl_threshold_loss = roc_curve(kl_true_val, kl_pred_val)\n",
    "    kl_objective = np.interp(10**(-5), kl_fpr_loss, kl_tpr_loss)\n",
    "\n",
    "    # Evaluate the reco term (MSE)\n",
    "    mse_qcd = output['ZeroBias']['reco_loss']\n",
    "    mse_bsm = output['TT']['reco_loss']\n",
    "\n",
    "    mse_true_val = np.concatenate((np.ones(bsm.shape[0]), np.zeros(qcd.shape[0])))\n",
    "    mse_pred_val = np.concatenate((mse_bsm, mse_qcd))\n",
    "\n",
    "    mse_fpr_loss, mse_tpr_loss, mse_threshold_loss = roc_curve(mse_true_val, mse_pred_val)\n",
    "    mse_objective = np.interp(10**(-5), mse_fpr_loss, mse_tpr_loss)\n",
    "\n",
    "\n",
    "    # The objective is the difference between the two\n",
    "    objective = abs(mse_objective - kl_objective)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb81ee5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_111 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_490 (Dense)              (None, 16)           928         ['input_111[0][0]']              \n",
      "                                                                                                  \n",
      " dense_491 (Dense)              (None, 16)           272         ['dense_490[0][0]']              \n",
      "                                                                                                  \n",
      " dense_492 (Dense)              (None, 4)            68          ['dense_491[0][0]']              \n",
      "                                                                                                  \n",
      " dense_493 (Dense)              (None, 4)            68          ['dense_491[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_55 (Sampling)         (None, 4)            0           ['dense_492[0][0]',              \n",
      "                                                                  'dense_493[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,336\n",
      "Trainable params: 1,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_112 (InputLayer)      [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 16)                80        \n",
      "                                                                 \n",
      " dense_495 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_496 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,409\n",
      "Trainable params: 18,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 207.9679 - reconstruction_loss: 191.5061 - kl_loss: 0.0187 - val_loss: 175.0660 - val_reconstruction_loss: 175.0506 - val_kl_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 185.3980 - reconstruction_loss: 189.8368 - kl_loss: 0.4435 - val_loss: 172.6720 - val_reconstruction_loss: 171.7549 - val_kl_loss: 0.9171 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 187.0795 - reconstruction_loss: 184.6753 - kl_loss: 1.3723 - val_loss: 168.7049 - val_reconstruction_loss: 166.8362 - val_kl_loss: 1.8687 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 177.9259 - reconstruction_loss: 178.1705 - kl_loss: 2.1165 - val_loss: 166.2464 - val_reconstruction_loss: 164.2144 - val_kl_loss: 2.0320 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 178.2888 - reconstruction_loss: 172.8994 - kl_loss: 2.3812 - val_loss: 162.8719 - val_reconstruction_loss: 160.3827 - val_kl_loss: 2.4892 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 169.4700 - reconstruction_loss: 167.4978 - kl_loss: 3.2028 - val_loss: 159.7406 - val_reconstruction_loss: 156.6373 - val_kl_loss: 3.1032 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 188.0878 - reconstruction_loss: 167.3963 - kl_loss: 3.2721 - val_loss: 157.3189 - val_reconstruction_loss: 154.1245 - val_kl_loss: 3.1944 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 170.3940 - reconstruction_loss: 160.0652 - kl_loss: 3.4172 - val_loss: 155.9275 - val_reconstruction_loss: 152.6326 - val_kl_loss: 3.2950 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 158.0482 - reconstruction_loss: 159.5075 - kl_loss: 3.4552 - val_loss: 156.0597 - val_reconstruction_loss: 152.7413 - val_kl_loss: 3.3184 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "382/391 [============================>.] - ETA: 0s - loss: 160.7555 - reconstruction_loss: 163.7216 - kl_loss: 3.4056\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.9112 - reconstruction_loss: 163.2337 - kl_loss: 3.4032 - val_loss: 156.9615 - val_reconstruction_loss: 153.5967 - val_kl_loss: 3.3649 - lr: 0.0010\n",
      "Epoch 00010: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_113 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_499 (Dense)              (None, 64)           3712        ['input_113[0][0]']              \n",
      "                                                                                                  \n",
      " dense_500 (Dense)              (None, 8)            520         ['dense_499[0][0]']              \n",
      "                                                                                                  \n",
      " dense_501 (Dense)              (None, 6)            54          ['dense_500[0][0]']              \n",
      "                                                                                                  \n",
      " dense_502 (Dense)              (None, 6)            54          ['dense_500[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_56 (Sampling)         (None, 6)            0           ['dense_501[0][0]',              \n",
      "                                                                  'dense_502[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,340\n",
      "Trainable params: 4,340\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_114 (InputLayer)      [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 16)                112       \n",
      "                                                                 \n",
      " dense_504 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_505 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_506 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,441\n",
      "Trainable params: 18,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 186.7973 - reconstruction_loss: 191.4773 - kl_loss: 0.0485 - val_loss: 174.5067 - val_reconstruction_loss: 174.1967 - val_kl_loss: 0.3100 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 179.4435 - reconstruction_loss: 186.6323 - kl_loss: 1.3830 - val_loss: 169.7820 - val_reconstruction_loss: 167.8947 - val_kl_loss: 1.8873 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 174.7122 - reconstruction_loss: 182.3872 - kl_loss: 1.9760 - val_loss: 166.8794 - val_reconstruction_loss: 164.8303 - val_kl_loss: 2.0491 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 169.5309 - reconstruction_loss: 178.2763 - kl_loss: 2.1830 - val_loss: 164.8335 - val_reconstruction_loss: 162.6871 - val_kl_loss: 2.1464 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 175.1169 - reconstruction_loss: 175.3119 - kl_loss: 2.5034 - val_loss: 161.8390 - val_reconstruction_loss: 159.0042 - val_kl_loss: 2.8348 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 168.3457 - reconstruction_loss: 171.3737 - kl_loss: 3.1774 - val_loss: 160.5502 - val_reconstruction_loss: 157.4341 - val_kl_loss: 3.1161 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 190.5002 - reconstruction_loss: 170.4559 - kl_loss: 3.2908 - val_loss: 159.2126 - val_reconstruction_loss: 156.0228 - val_kl_loss: 3.1898 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 157.7157 - reconstruction_loss: 166.4966 - kl_loss: 3.3571 - val_loss: 157.5642 - val_reconstruction_loss: 154.3228 - val_kl_loss: 3.2414 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 167.9296 - reconstruction_loss: 162.6476 - kl_loss: 3.4237 - val_loss: 155.4021 - val_reconstruction_loss: 152.0401 - val_kl_loss: 3.3621 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 164.3043 - reconstruction_loss: 160.0014 - kl_loss: 3.5468 - val_loss: 155.5489 - val_reconstruction_loss: 152.1003 - val_kl_loss: 3.4486 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 182.0077 - reconstruction_loss: 163.8409 - kl_loss: 3.4649 - val_loss: 153.7691 - val_reconstruction_loss: 150.3614 - val_kl_loss: 3.4077 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 166.6328 - reconstruction_loss: 158.7055 - kl_loss: 3.5375 - val_loss: 152.4068 - val_reconstruction_loss: 148.8571 - val_kl_loss: 3.5496 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 153.3683 - reconstruction_loss: 152.9867 - kl_loss: 3.6425 - val_loss: 152.5156 - val_reconstruction_loss: 149.0674 - val_kl_loss: 3.4483 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.9871 - reconstruction_loss: 156.2820 - kl_loss: 3.6373 - val_loss: 151.2035 - val_reconstruction_loss: 147.6546 - val_kl_loss: 3.5489 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 173.2484 - reconstruction_loss: 158.6316 - kl_loss: 3.6203 - val_loss: 150.5947 - val_reconstruction_loss: 146.9915 - val_kl_loss: 3.6032 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.3489 - reconstruction_loss: 153.7283 - kl_loss: 3.6761 - val_loss: 150.0273 - val_reconstruction_loss: 146.3893 - val_kl_loss: 3.6379 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 156.9751 - reconstruction_loss: 149.2419 - kl_loss: 3.7346 - val_loss: 150.1826 - val_reconstruction_loss: 146.5151 - val_kl_loss: 3.6674 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "379/391 [============================>.] - ETA: 0s - loss: 2676.8140 - reconstruction_loss: 719.2943 - kl_loss: 463.4845\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2626.6620 - reconstruction_loss: 702.0142 - kl_loss: 449.3695 - val_loss: 152.3226 - val_reconstruction_loss: 148.8318 - val_kl_loss: 3.4909 - lr: 0.0010\n",
      "Epoch 00018: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_115 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_508 (Dense)              (None, 64)           3712        ['input_115[0][0]']              \n",
      "                                                                                                  \n",
      " dense_509 (Dense)              (None, 8)            520         ['dense_508[0][0]']              \n",
      "                                                                                                  \n",
      " dense_510 (Dense)              (None, 3)            27          ['dense_509[0][0]']              \n",
      "                                                                                                  \n",
      " dense_511 (Dense)              (None, 3)            27          ['dense_509[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_57 (Sampling)         (None, 3)            0           ['dense_510[0][0]',              \n",
      "                                                                  'dense_511[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,286\n",
      "Trainable params: 4,286\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_116 (InputLayer)      [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_512 (Dense)           (None, 16)                64        \n",
      "                                                                 \n",
      " dense_513 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_514 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_515 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_516 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,393\n",
      "Trainable params: 18,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 212.2879 - reconstruction_loss: 191.1125 - kl_loss: 0.1259 - val_loss: 173.0680 - val_reconstruction_loss: 172.1990 - val_kl_loss: 0.8690 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 194.1022 - reconstruction_loss: 184.1413 - kl_loss: 1.6705 - val_loss: 168.0650 - val_reconstruction_loss: 166.1432 - val_kl_loss: 1.9219 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 182.0957 - reconstruction_loss: 177.0808 - kl_loss: 2.2240 - val_loss: 164.8372 - val_reconstruction_loss: 162.6316 - val_kl_loss: 2.2056 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 175.7628 - reconstruction_loss: 174.6602 - kl_loss: 2.4180 - val_loss: 164.1018 - val_reconstruction_loss: 161.8692 - val_kl_loss: 2.2326 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 193.1961 - reconstruction_loss: 171.4707 - kl_loss: 2.4579 - val_loss: 161.6962 - val_reconstruction_loss: 159.2683 - val_kl_loss: 2.4280 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 168.7940 - reconstruction_loss: 166.6489 - kl_loss: 3.1038 - val_loss: 159.4299 - val_reconstruction_loss: 156.3629 - val_kl_loss: 3.0671 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 169.7015 - reconstruction_loss: 162.8032 - kl_loss: 3.3846 - val_loss: 158.1262 - val_reconstruction_loss: 154.8099 - val_kl_loss: 3.3163 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 158.0950 - reconstruction_loss: 161.6547 - kl_loss: 3.5094 - val_loss: 156.5062 - val_reconstruction_loss: 153.1661 - val_kl_loss: 3.3401 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 159.4088 - reconstruction_loss: 157.7872 - kl_loss: 3.5480 - val_loss: 154.6754 - val_reconstruction_loss: 151.3092 - val_kl_loss: 3.3663 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 159.4405 - reconstruction_loss: 153.3807 - kl_loss: 3.6157 - val_loss: 153.9216 - val_reconstruction_loss: 150.4615 - val_kl_loss: 3.4602 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 156.1409 - reconstruction_loss: 152.0171 - kl_loss: 3.6547 - val_loss: 152.8673 - val_reconstruction_loss: 149.4246 - val_kl_loss: 3.4428 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 152.8239 - reconstruction_loss: 151.5769 - kl_loss: 3.7475 - val_loss: 152.3662 - val_reconstruction_loss: 148.7736 - val_kl_loss: 3.5926 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 167.6028 - reconstruction_loss: 155.5643 - kl_loss: 3.7671 - val_loss: 152.6037 - val_reconstruction_loss: 148.9892 - val_kl_loss: 3.6145 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 159.2718 - reconstruction_loss: 150.9704 - kl_loss: 3.8059 - val_loss: 150.6280 - val_reconstruction_loss: 146.9618 - val_kl_loss: 3.6661 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 145.5157 - reconstruction_loss: 145.2276 - kl_loss: 3.8720 - val_loss: 150.5607 - val_reconstruction_loss: 146.8383 - val_kl_loss: 3.7224 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 148.9987 - reconstruction_loss: 143.0952 - kl_loss: 3.9093 - val_loss: 148.9215 - val_reconstruction_loss: 145.1993 - val_kl_loss: 3.7223 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 144.1768 - reconstruction_loss: 142.2656 - kl_loss: 3.9495 - val_loss: 147.7982 - val_reconstruction_loss: 144.0814 - val_kl_loss: 3.7168 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 147.8982 - reconstruction_loss: 141.8311 - kl_loss: 4.0062 - val_loss: 147.5894 - val_reconstruction_loss: 143.8148 - val_kl_loss: 3.7746 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 150.9287 - reconstruction_loss: 144.5062 - kl_loss: 4.0225 - val_loss: 147.2487 - val_reconstruction_loss: 143.5170 - val_kl_loss: 3.7317 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 146.5971 - reconstruction_loss: 143.0498 - kl_loss: 4.0323 - val_loss: 147.4755 - val_reconstruction_loss: 143.7187 - val_kl_loss: 3.7568 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 144.6423 - reconstruction_loss: 140.5061 - kl_loss: 4.0843 - val_loss: 147.1139 - val_reconstruction_loss: 143.2483 - val_kl_loss: 3.8657 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 157.4089 - reconstruction_loss: 143.2281 - kl_loss: 4.1547 - val_loss: 146.8199 - val_reconstruction_loss: 142.8685 - val_kl_loss: 3.9515 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.4148 - reconstruction_loss: 144.5332 - kl_loss: 4.1254 - val_loss: 145.7828 - val_reconstruction_loss: 141.8589 - val_kl_loss: 3.9240 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 145.2985 - reconstruction_loss: 140.6827 - kl_loss: 4.1590 - val_loss: 145.6663 - val_reconstruction_loss: 141.8066 - val_kl_loss: 3.8597 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 141.7114 - reconstruction_loss: 136.4206 - kl_loss: 4.1647 - val_loss: 145.5387 - val_reconstruction_loss: 141.6007 - val_kl_loss: 3.9379 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 137.4537 - reconstruction_loss: 135.9825 - kl_loss: 4.1900 - val_loss: 144.5865 - val_reconstruction_loss: 140.6395 - val_kl_loss: 3.9469 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 140.7664 - reconstruction_loss: 135.5556 - kl_loss: 4.2201 - val_loss: 144.5129 - val_reconstruction_loss: 140.4817 - val_kl_loss: 4.0312 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 144.6832 - reconstruction_loss: 137.8291 - kl_loss: 4.2199 - val_loss: 143.4763 - val_reconstruction_loss: 139.5135 - val_kl_loss: 3.9628 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 137.2109 - reconstruction_loss: 133.8475 - kl_loss: 4.2515 - val_loss: 143.3613 - val_reconstruction_loss: 139.3723 - val_kl_loss: 3.9890 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 142.6122 - reconstruction_loss: 136.7043 - kl_loss: 4.2593 - val_loss: 145.4122 - val_reconstruction_loss: 141.3676 - val_kl_loss: 4.0446 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "382/391 [============================>.] - ETA: 0s - loss: 142.2668 - reconstruction_loss: 139.6188 - kl_loss: 4.2108\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 142.3059 - reconstruction_loss: 139.6118 - kl_loss: 4.2082 - val_loss: 144.8401 - val_reconstruction_loss: 140.8848 - val_kl_loss: 3.9553 - lr: 0.0010\n",
      "Epoch 00031: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_117 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_517 (Dense)              (None, 32)           1856        ['input_117[0][0]']              \n",
      "                                                                                                  \n",
      " dense_518 (Dense)              (None, 8)            264         ['dense_517[0][0]']              \n",
      "                                                                                                  \n",
      " dense_519 (Dense)              (None, 7)            63          ['dense_518[0][0]']              \n",
      "                                                                                                  \n",
      " dense_520 (Dense)              (None, 7)            63          ['dense_518[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_58 (Sampling)         (None, 7)            0           ['dense_519[0][0]',              \n",
      "                                                                  'dense_520[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,246\n",
      "Trainable params: 2,246\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_118 (InputLayer)      [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_521 (Dense)           (None, 16)                128       \n",
      "                                                                 \n",
      " dense_522 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_524 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_525 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,457\n",
      "Trainable params: 18,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 188.6876 - reconstruction_loss: 191.6487 - kl_loss: 0.0418 - val_loss: 174.8921 - val_reconstruction_loss: 174.8059 - val_kl_loss: 0.0862 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 188.7884 - reconstruction_loss: 186.8481 - kl_loss: 1.3072 - val_loss: 170.5621 - val_reconstruction_loss: 168.7829 - val_kl_loss: 1.7793 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 193.5890 - reconstruction_loss: 182.5260 - kl_loss: 2.0000 - val_loss: 166.8994 - val_reconstruction_loss: 164.8104 - val_kl_loss: 2.0890 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 174.1148 - reconstruction_loss: 178.2277 - kl_loss: 2.2774 - val_loss: 163.9499 - val_reconstruction_loss: 161.6088 - val_kl_loss: 2.3411 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 176.0700 - reconstruction_loss: 172.9149 - kl_loss: 2.6746 - val_loss: 160.5158 - val_reconstruction_loss: 157.5552 - val_kl_loss: 2.9606 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 171.8664 - reconstruction_loss: 168.9345 - kl_loss: 5.9471 - val_loss: 157.8705 - val_reconstruction_loss: 154.4580 - val_kl_loss: 3.4126 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 178.7480 - reconstruction_loss: 165.3707 - kl_loss: 3.7889 - val_loss: 155.4802 - val_reconstruction_loss: 151.8367 - val_kl_loss: 3.6434 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 165.4947 - reconstruction_loss: 163.4893 - kl_loss: 3.9419 - val_loss: 154.8319 - val_reconstruction_loss: 150.9637 - val_kl_loss: 3.8681 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 161.4276 - reconstruction_loss: 158.1419 - kl_loss: 4.0940 - val_loss: 152.7706 - val_reconstruction_loss: 148.7701 - val_kl_loss: 4.0005 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 159.3545 - reconstruction_loss: 153.8537 - kl_loss: 4.3319 - val_loss: 151.1994 - val_reconstruction_loss: 146.8596 - val_kl_loss: 4.3399 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 153.8900 - reconstruction_loss: 150.9770 - kl_loss: 4.4977 - val_loss: 149.9693 - val_reconstruction_loss: 145.4558 - val_kl_loss: 4.5135 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 156.7751 - reconstruction_loss: 148.3394 - kl_loss: 4.6201 - val_loss: 148.3557 - val_reconstruction_loss: 143.7035 - val_kl_loss: 4.6521 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 149.8651 - reconstruction_loss: 147.8552 - kl_loss: 4.6927 - val_loss: 147.1951 - val_reconstruction_loss: 142.5299 - val_kl_loss: 4.6652 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 147.8662 - reconstruction_loss: 146.3695 - kl_loss: 4.7684 - val_loss: 147.9413 - val_reconstruction_loss: 143.4415 - val_kl_loss: 4.4999 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 147.8392 - reconstruction_loss: 144.3699 - kl_loss: 4.7489 - val_loss: 145.1736 - val_reconstruction_loss: 140.4771 - val_kl_loss: 4.6965 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 150.0357 - reconstruction_loss: 143.0202 - kl_loss: 4.7925 - val_loss: 144.7854 - val_reconstruction_loss: 140.0982 - val_kl_loss: 4.6872 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 148.8910 - reconstruction_loss: 142.4984 - kl_loss: 4.7997 - val_loss: 143.8747 - val_reconstruction_loss: 139.1542 - val_kl_loss: 4.7205 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 149.2319 - reconstruction_loss: 145.6808 - kl_loss: 4.8267 - val_loss: 143.4078 - val_reconstruction_loss: 138.6809 - val_kl_loss: 4.7268 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 182.8875 - reconstruction_loss: 150.7706 - kl_loss: 4.8615 - val_loss: 142.1657 - val_reconstruction_loss: 137.3741 - val_kl_loss: 4.7916 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 144.6295 - reconstruction_loss: 138.7899 - kl_loss: 4.8383 - val_loss: 141.9482 - val_reconstruction_loss: 137.1877 - val_kl_loss: 4.7605 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 142.5656 - reconstruction_loss: 136.6595 - kl_loss: 4.8894 - val_loss: 140.5861 - val_reconstruction_loss: 135.7980 - val_kl_loss: 4.7882 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 138.4104 - reconstruction_loss: 135.3558 - kl_loss: 4.9471 - val_loss: 141.0877 - val_reconstruction_loss: 136.2837 - val_kl_loss: 4.8040 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 143.6078 - reconstruction_loss: 135.9003 - kl_loss: 4.9772 - val_loss: 139.4710 - val_reconstruction_loss: 134.6258 - val_kl_loss: 4.8452 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 140.6570 - reconstruction_loss: 139.2378 - kl_loss: 4.9704 - val_loss: 139.9003 - val_reconstruction_loss: 134.9840 - val_kl_loss: 4.9163 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "377/391 [===========================>..] - ETA: 0s - loss: 140.8599 - reconstruction_loss: 134.1763 - kl_loss: 4.9751\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 140.8064 - reconstruction_loss: 134.5406 - kl_loss: 4.9763 - val_loss: 140.8859 - val_reconstruction_loss: 135.9651 - val_kl_loss: 4.9207 - lr: 0.0010\n",
      "Epoch 00025: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_119 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_526 (Dense)              (None, 16)           928         ['input_119[0][0]']              \n",
      "                                                                                                  \n",
      " dense_527 (Dense)              (None, 16)           272         ['dense_526[0][0]']              \n",
      "                                                                                                  \n",
      " dense_528 (Dense)              (None, 4)            68          ['dense_527[0][0]']              \n",
      "                                                                                                  \n",
      " dense_529 (Dense)              (None, 4)            68          ['dense_527[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_59 (Sampling)         (None, 4)            0           ['dense_528[0][0]',              \n",
      "                                                                  'dense_529[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,336\n",
      "Trainable params: 1,336\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_120 (InputLayer)      [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_530 (Dense)           (None, 16)                80        \n",
      "                                                                 \n",
      " dense_531 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_532 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_533 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_534 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,409\n",
      "Trainable params: 18,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 201.4157 - reconstruction_loss: 191.3289 - kl_loss: 0.0902 - val_loss: 174.0612 - val_reconstruction_loss: 173.5919 - val_kl_loss: 0.4692 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 241.2116 - reconstruction_loss: 187.5774 - kl_loss: 0.9614 - val_loss: 170.6261 - val_reconstruction_loss: 169.2584 - val_kl_loss: 1.3676 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 182.7186 - reconstruction_loss: 181.8953 - kl_loss: 2.0059 - val_loss: 166.3161 - val_reconstruction_loss: 164.1633 - val_kl_loss: 2.1528 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 176.9818 - reconstruction_loss: 178.9146 - kl_loss: 2.1895 - val_loss: 164.4079 - val_reconstruction_loss: 162.1434 - val_kl_loss: 2.2645 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 208.2485 - reconstruction_loss: 176.2202 - kl_loss: 2.3501 - val_loss: 162.5752 - val_reconstruction_loss: 160.1888 - val_kl_loss: 2.3864 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 180.4439 - reconstruction_loss: 173.9376 - kl_loss: 2.4753 - val_loss: 161.2298 - val_reconstruction_loss: 158.7293 - val_kl_loss: 2.5005 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 174.9561 - reconstruction_loss: 171.2821 - kl_loss: 2.9282 - val_loss: 159.4279 - val_reconstruction_loss: 156.2012 - val_kl_loss: 3.2267 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 163.5785 - reconstruction_loss: 167.4016 - kl_loss: 3.4065 - val_loss: 158.5634 - val_reconstruction_loss: 155.1128 - val_kl_loss: 3.4506 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 165.8944 - reconstruction_loss: 164.3043 - kl_loss: 3.4885 - val_loss: 157.0119 - val_reconstruction_loss: 153.6515 - val_kl_loss: 3.3604 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 157.5486 - reconstruction_loss: 161.9423 - kl_loss: 3.5433 - val_loss: 157.2421 - val_reconstruction_loss: 153.7676 - val_kl_loss: 3.4744 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "378/391 [============================>.] - ETA: 0s - loss: 160.6312 - reconstruction_loss: 157.5997 - kl_loss: 3.5998\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 160.6408 - reconstruction_loss: 157.1595 - kl_loss: 3.6029 - val_loss: 158.0926 - val_reconstruction_loss: 154.6902 - val_kl_loss: 3.4025 - lr: 0.0010\n",
      "Epoch 00011: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_121 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_535 (Dense)              (None, 64)           3712        ['input_121[0][0]']              \n",
      "                                                                                                  \n",
      " dense_536 (Dense)              (None, 8)            520         ['dense_535[0][0]']              \n",
      "                                                                                                  \n",
      " dense_537 (Dense)              (None, 6)            54          ['dense_536[0][0]']              \n",
      "                                                                                                  \n",
      " dense_538 (Dense)              (None, 6)            54          ['dense_536[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_60 (Sampling)         (None, 6)            0           ['dense_537[0][0]',              \n",
      "                                                                  'dense_538[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,340\n",
      "Trainable params: 4,340\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_122 (InputLayer)      [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_539 (Dense)           (None, 16)                112       \n",
      "                                                                 \n",
      " dense_540 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_541 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_542 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_543 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,441\n",
      "Trainable params: 18,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 3s 6ms/step - loss: 200.1606 - reconstruction_loss: 191.4818 - kl_loss: 0.0248 - val_loss: 175.0421 - val_reconstruction_loss: 174.9886 - val_kl_loss: 0.0536 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 186.0370 - reconstruction_loss: 186.8062 - kl_loss: 1.2248 - val_loss: 169.8994 - val_reconstruction_loss: 167.9908 - val_kl_loss: 1.9085 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 189.2522 - reconstruction_loss: 180.0106 - kl_loss: 2.4947 - val_loss: 165.2634 - val_reconstruction_loss: 162.4299 - val_kl_loss: 2.8334 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 173.9154 - reconstruction_loss: 173.8293 - kl_loss: 3.2186 - val_loss: 162.2260 - val_reconstruction_loss: 159.0563 - val_kl_loss: 3.1697 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 173.6816 - reconstruction_loss: 171.0799 - kl_loss: 3.3891 - val_loss: 160.8681 - val_reconstruction_loss: 157.5047 - val_kl_loss: 3.3634 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 170.9909 - reconstruction_loss: 167.5948 - kl_loss: 3.5621 - val_loss: 157.9046 - val_reconstruction_loss: 154.5337 - val_kl_loss: 3.3709 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 162.8735 - reconstruction_loss: 162.4464 - kl_loss: 3.5788 - val_loss: 156.6587 - val_reconstruction_loss: 153.2218 - val_kl_loss: 3.4368 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 164.5483 - reconstruction_loss: 159.1246 - kl_loss: 3.6888 - val_loss: 157.4711 - val_reconstruction_loss: 154.0183 - val_kl_loss: 3.4528 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 172.3327 - reconstruction_loss: 159.0547 - kl_loss: 3.6648 - val_loss: 154.9207 - val_reconstruction_loss: 151.4330 - val_kl_loss: 3.4877 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 158.2215 - reconstruction_loss: 155.8485 - kl_loss: 3.8183 - val_loss: 155.5266 - val_reconstruction_loss: 152.1216 - val_kl_loss: 3.4050 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 159.9391 - reconstruction_loss: 154.8489 - kl_loss: 3.6798 - val_loss: 153.6073 - val_reconstruction_loss: 150.0462 - val_kl_loss: 3.5611 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 154.0779 - reconstruction_loss: 152.2406 - kl_loss: 3.7704 - val_loss: 153.9665 - val_reconstruction_loss: 150.4608 - val_kl_loss: 3.5058 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 219.4781 - reconstruction_loss: 160.8720 - kl_loss: 3.7563 - val_loss: 152.7353 - val_reconstruction_loss: 149.1377 - val_kl_loss: 3.5976 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 151.0146 - reconstruction_loss: 150.7328 - kl_loss: 3.8124 - val_loss: 151.6128 - val_reconstruction_loss: 148.0522 - val_kl_loss: 3.5606 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 148.7172 - reconstruction_loss: 148.7853 - kl_loss: 3.9000 - val_loss: 151.9988 - val_reconstruction_loss: 148.3173 - val_kl_loss: 3.6816 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "385/391 [============================>.] - ETA: 0s - loss: 150.9714 - reconstruction_loss: 156.5154 - kl_loss: 3.9393\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 151.1389 - reconstruction_loss: 156.3873 - kl_loss: 3.9381 - val_loss: 152.1224 - val_reconstruction_loss: 148.2213 - val_kl_loss: 3.9010 - lr: 0.0010\n",
      "Epoch 00016: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_123 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_544 (Dense)              (None, 64)           3712        ['input_123[0][0]']              \n",
      "                                                                                                  \n",
      " dense_545 (Dense)              (None, 8)            520         ['dense_544[0][0]']              \n",
      "                                                                                                  \n",
      " dense_546 (Dense)              (None, 4)            36          ['dense_545[0][0]']              \n",
      "                                                                                                  \n",
      " dense_547 (Dense)              (None, 4)            36          ['dense_545[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_61 (Sampling)         (None, 4)            0           ['dense_546[0][0]',              \n",
      "                                                                  'dense_547[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,304\n",
      "Trainable params: 4,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_124 (InputLayer)      [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_548 (Dense)           (None, 16)                80        \n",
      "                                                                 \n",
      " dense_549 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_550 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_551 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_552 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,409\n",
      "Trainable params: 18,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 187.4317 - reconstruction_loss: 191.4639 - kl_loss: 0.0172 - val_loss: 174.9576 - val_reconstruction_loss: 174.9147 - val_kl_loss: 0.0429 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 193.6809 - reconstruction_loss: 188.2551 - kl_loss: 0.8596 - val_loss: 171.7507 - val_reconstruction_loss: 170.5216 - val_kl_loss: 1.2290 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 184.6507 - reconstruction_loss: 182.7431 - kl_loss: 2.0085 - val_loss: 166.5972 - val_reconstruction_loss: 164.2220 - val_kl_loss: 2.3753 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 182.1288 - reconstruction_loss: 175.0886 - kl_loss: 2.9850 - val_loss: 162.3162 - val_reconstruction_loss: 159.2546 - val_kl_loss: 3.0615 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 166.3047 - reconstruction_loss: 170.4628 - kl_loss: 3.2799 - val_loss: 160.5603 - val_reconstruction_loss: 157.3705 - val_kl_loss: 3.1898 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 168.9669 - reconstruction_loss: 166.2594 - kl_loss: 3.4460 - val_loss: 157.6546 - val_reconstruction_loss: 154.2081 - val_kl_loss: 3.4465 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 169.3273 - reconstruction_loss: 161.1148 - kl_loss: 4.0105 - val_loss: 154.5586 - val_reconstruction_loss: 150.4022 - val_kl_loss: 4.1564 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 163.9065 - reconstruction_loss: 156.5323 - kl_loss: 4.4022 - val_loss: 150.7766 - val_reconstruction_loss: 146.4913 - val_kl_loss: 4.2853 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 170.4017 - reconstruction_loss: 154.7099 - kl_loss: 4.5457 - val_loss: 151.1158 - val_reconstruction_loss: 146.9323 - val_kl_loss: 4.1834 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 158.6693 - reconstruction_loss: 151.5513 - kl_loss: 4.5513 - val_loss: 147.8401 - val_reconstruction_loss: 143.3998 - val_kl_loss: 4.4403 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 150.2874 - reconstruction_loss: 147.5262 - kl_loss: 4.6603 - val_loss: 150.8810 - val_reconstruction_loss: 146.5750 - val_kl_loss: 4.3060 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 160.4728 - reconstruction_loss: 152.7092 - kl_loss: 4.6104 - val_loss: 145.6113 - val_reconstruction_loss: 141.0601 - val_kl_loss: 4.5512 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 148.5785 - reconstruction_loss: 144.0552 - kl_loss: 4.7188 - val_loss: 146.5392 - val_reconstruction_loss: 142.0303 - val_kl_loss: 4.5088 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "376/391 [===========================>..] - ETA: 0s - loss: 143.4356 - reconstruction_loss: 142.1906 - kl_loss: 4.6989\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 143.6149 - reconstruction_loss: 143.2805 - kl_loss: 4.7684 - val_loss: 148.5660 - val_reconstruction_loss: 144.1009 - val_kl_loss: 4.4651 - lr: 0.0010\n",
      "Epoch 00014: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_125 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_553 (Dense)              (None, 64)           3712        ['input_125[0][0]']              \n",
      "                                                                                                  \n",
      " dense_554 (Dense)              (None, 8)            520         ['dense_553[0][0]']              \n",
      "                                                                                                  \n",
      " dense_555 (Dense)              (None, 4)            36          ['dense_554[0][0]']              \n",
      "                                                                                                  \n",
      " dense_556 (Dense)              (None, 4)            36          ['dense_554[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_62 (Sampling)         (None, 4)            0           ['dense_555[0][0]',              \n",
      "                                                                  'dense_556[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,304\n",
      "Trainable params: 4,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_126 (InputLayer)      [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_557 (Dense)           (None, 16)                80        \n",
      "                                                                 \n",
      " dense_558 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_559 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_560 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_561 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,409\n",
      "Trainable params: 18,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 189.6095 - reconstruction_loss: 191.3563 - kl_loss: 0.0424 - val_loss: 174.6387 - val_reconstruction_loss: 174.3214 - val_kl_loss: 0.3172 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 186.6011 - reconstruction_loss: 187.9889 - kl_loss: 0.9847 - val_loss: 171.3824 - val_reconstruction_loss: 169.7730 - val_kl_loss: 1.6095 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 184.8987 - reconstruction_loss: 179.6813 - kl_loss: 2.6388 - val_loss: 165.2790 - val_reconstruction_loss: 162.3719 - val_kl_loss: 2.9071 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 182.5050 - reconstruction_loss: 176.2797 - kl_loss: 3.2926 - val_loss: 163.0915 - val_reconstruction_loss: 160.0232 - val_kl_loss: 3.0683 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 174.4514 - reconstruction_loss: 171.2883 - kl_loss: 3.2403 - val_loss: 159.2499 - val_reconstruction_loss: 156.0233 - val_kl_loss: 3.2266 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 162.9995 - reconstruction_loss: 163.7958 - kl_loss: 3.3961 - val_loss: 156.6867 - val_reconstruction_loss: 153.3754 - val_kl_loss: 3.3114 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 161.8369 - reconstruction_loss: 160.0261 - kl_loss: 3.5240 - val_loss: 155.0638 - val_reconstruction_loss: 151.6892 - val_kl_loss: 3.3746 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.7320 - reconstruction_loss: 156.5658 - kl_loss: 3.5858 - val_loss: 153.3928 - val_reconstruction_loss: 150.0043 - val_kl_loss: 3.3885 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 154.2280 - reconstruction_loss: 152.7226 - kl_loss: 3.6775 - val_loss: 151.8691 - val_reconstruction_loss: 148.4095 - val_kl_loss: 3.4597 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 159.8053 - reconstruction_loss: 155.0576 - kl_loss: 3.6288 - val_loss: 151.3131 - val_reconstruction_loss: 147.8230 - val_kl_loss: 3.4901 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 158.7272 - reconstruction_loss: 150.3327 - kl_loss: 3.7317 - val_loss: 150.7043 - val_reconstruction_loss: 147.2147 - val_kl_loss: 3.4896 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.3265 - reconstruction_loss: 149.9739 - kl_loss: 3.7922 - val_loss: 150.8381 - val_reconstruction_loss: 147.3155 - val_kl_loss: 3.5226 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 166.9799 - reconstruction_loss: 153.1903 - kl_loss: 3.7402 - val_loss: 149.3272 - val_reconstruction_loss: 145.6876 - val_kl_loss: 3.6395 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 155.1658 - reconstruction_loss: 147.3785 - kl_loss: 3.8440 - val_loss: 148.1466 - val_reconstruction_loss: 144.4876 - val_kl_loss: 3.6589 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 151.8497 - reconstruction_loss: 144.6221 - kl_loss: 3.9793 - val_loss: 147.3609 - val_reconstruction_loss: 143.5240 - val_kl_loss: 3.8369 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 149.9709 - reconstruction_loss: 144.0986 - kl_loss: 4.3130 - val_loss: 146.0809 - val_reconstruction_loss: 141.7724 - val_kl_loss: 4.3086 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 147.1387 - reconstruction_loss: 142.7091 - kl_loss: 4.5992 - val_loss: 145.7930 - val_reconstruction_loss: 141.3463 - val_kl_loss: 4.4467 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 151.6282 - reconstruction_loss: 144.5643 - kl_loss: 4.6137 - val_loss: 145.0001 - val_reconstruction_loss: 140.4995 - val_kl_loss: 4.5006 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 143.0157 - reconstruction_loss: 139.3570 - kl_loss: 4.6964 - val_loss: 144.5008 - val_reconstruction_loss: 139.9939 - val_kl_loss: 4.5069 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 143.4937 - reconstruction_loss: 138.6518 - kl_loss: 4.6957 - val_loss: 143.9713 - val_reconstruction_loss: 139.4948 - val_kl_loss: 4.4765 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 146.0840 - reconstruction_loss: 143.4635 - kl_loss: 4.6966 - val_loss: 143.4354 - val_reconstruction_loss: 138.9076 - val_kl_loss: 4.5278 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 142.8980 - reconstruction_loss: 136.5810 - kl_loss: 4.7694 - val_loss: 142.8270 - val_reconstruction_loss: 138.2634 - val_kl_loss: 4.5637 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 142.9516 - reconstruction_loss: 135.9087 - kl_loss: 4.7611 - val_loss: 142.8595 - val_reconstruction_loss: 138.3419 - val_kl_loss: 4.5176 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "388/391 [============================>.] - ETA: 0s - loss: 140.4659 - reconstruction_loss: 135.4495 - kl_loss: 4.7693\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 140.4635 - reconstruction_loss: 135.4141 - kl_loss: 4.7701 - val_loss: 142.7805 - val_reconstruction_loss: 138.2558 - val_kl_loss: 4.5247 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 141.9965 - reconstruction_loss: 138.6409 - kl_loss: 4.7347 - val_loss: 142.1392 - val_reconstruction_loss: 137.5685 - val_kl_loss: 4.5706 - lr: 1.0000e-04\n",
      "Epoch 26/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 138.3492 - reconstruction_loss: 135.7082 - kl_loss: 4.7708 - val_loss: 141.9355 - val_reconstruction_loss: 137.3554 - val_kl_loss: 4.5801 - lr: 1.0000e-04\n",
      "Epoch 27/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 139.5154 - reconstruction_loss: 134.5845 - kl_loss: 4.7817 - val_loss: 141.7836 - val_reconstruction_loss: 137.1867 - val_kl_loss: 4.5969 - lr: 1.0000e-04\n",
      "Epoch 28/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 142.4846 - reconstruction_loss: 133.5058 - kl_loss: 4.8067 - val_loss: 141.6479 - val_reconstruction_loss: 137.0593 - val_kl_loss: 4.5886 - lr: 1.0000e-04\n",
      "Epoch 29/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 140.5269 - reconstruction_loss: 132.8479 - kl_loss: 4.8052 - val_loss: 141.5701 - val_reconstruction_loss: 136.9722 - val_kl_loss: 4.5980 - lr: 1.0000e-04\n",
      "Epoch 30/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 136.0961 - reconstruction_loss: 132.1786 - kl_loss: 4.8203 - val_loss: 141.5269 - val_reconstruction_loss: 136.9125 - val_kl_loss: 4.6144 - lr: 1.0000e-04\n",
      "Epoch 31/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.1437 - reconstruction_loss: 131.7234 - kl_loss: 4.8360 - val_loss: 141.4566 - val_reconstruction_loss: 136.8489 - val_kl_loss: 4.6077 - lr: 1.0000e-04\n",
      "Epoch 32/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.8829 - reconstruction_loss: 131.3252 - kl_loss: 4.8367 - val_loss: 141.3798 - val_reconstruction_loss: 136.7607 - val_kl_loss: 4.6191 - lr: 1.0000e-04\n",
      "Epoch 33/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 142.3781 - reconstruction_loss: 131.1061 - kl_loss: 4.8518 - val_loss: 141.3405 - val_reconstruction_loss: 136.7190 - val_kl_loss: 4.6215 - lr: 1.0000e-04\n",
      "Epoch 34/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 135.8007 - reconstruction_loss: 130.9516 - kl_loss: 4.8535 - val_loss: 141.2519 - val_reconstruction_loss: 136.6301 - val_kl_loss: 4.6219 - lr: 1.0000e-04\n",
      "Epoch 35/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 134.3236 - reconstruction_loss: 130.8112 - kl_loss: 4.8618 - val_loss: 141.2399 - val_reconstruction_loss: 136.6266 - val_kl_loss: 4.6132 - lr: 1.0000e-04\n",
      "Epoch 36/500\n",
      "376/391 [===========================>..] - ETA: 0s - loss: 136.8679 - reconstruction_loss: 130.0400 - kl_loss: 4.8541\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 136.8064 - reconstruction_loss: 130.6493 - kl_loss: 4.8578 - val_loss: 141.1655 - val_reconstruction_loss: 136.5347 - val_kl_loss: 4.6308 - lr: 1.0000e-04\n",
      "Epoch 37/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 133.4882 - reconstruction_loss: 130.4162 - kl_loss: 4.8632 - val_loss: 141.1326 - val_reconstruction_loss: 136.5068 - val_kl_loss: 4.6258 - lr: 1.0000e-05\n",
      "Epoch 38/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 136.5223 - reconstruction_loss: 130.4129 - kl_loss: 4.8638 - val_loss: 141.0982 - val_reconstruction_loss: 136.4738 - val_kl_loss: 4.6243 - lr: 1.0000e-05\n",
      "Epoch 39/500\n",
      "382/391 [============================>.] - ETA: 0s - loss: 136.9282 - reconstruction_loss: 129.9752 - kl_loss: 4.8639\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 136.8807 - reconstruction_loss: 130.3676 - kl_loss: 4.8613 - val_loss: 141.1616 - val_reconstruction_loss: 136.5377 - val_kl_loss: 4.6240 - lr: 1.0000e-05\n",
      "Epoch 40/500\n",
      "386/391 [============================>.] - ETA: 0s - loss: 132.8736 - reconstruction_loss: 130.2486 - kl_loss: 4.8605Restoring model weights from the end of the best epoch: 38.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 132.9091 - reconstruction_loss: 130.3512 - kl_loss: 4.8603 - val_loss: 141.1364 - val_reconstruction_loss: 136.5125 - val_kl_loss: 4.6239 - lr: 1.0000e-06\n",
      "Epoch 00040: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_127 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_562 (Dense)              (None, 64)           3712        ['input_127[0][0]']              \n",
      "                                                                                                  \n",
      " dense_563 (Dense)              (None, 8)            520         ['dense_562[0][0]']              \n",
      "                                                                                                  \n",
      " dense_564 (Dense)              (None, 3)            27          ['dense_563[0][0]']              \n",
      "                                                                                                  \n",
      " dense_565 (Dense)              (None, 3)            27          ['dense_563[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_63 (Sampling)         (None, 3)            0           ['dense_564[0][0]',              \n",
      "                                                                  'dense_565[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,286\n",
      "Trainable params: 4,286\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_128 (InputLayer)      [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_566 (Dense)           (None, 16)                64        \n",
      "                                                                 \n",
      " dense_567 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_568 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_569 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_570 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,393\n",
      "Trainable params: 18,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 194.6973 - reconstruction_loss: 191.1959 - kl_loss: 0.1560 - val_loss: 173.0493 - val_reconstruction_loss: 172.1261 - val_kl_loss: 0.9231 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 177.8361 - reconstruction_loss: 185.5789 - kl_loss: 1.5714 - val_loss: 169.3849 - val_reconstruction_loss: 167.5403 - val_kl_loss: 1.8446 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 203.2487 - reconstruction_loss: 182.5425 - kl_loss: 1.9408 - val_loss: 166.7377 - val_reconstruction_loss: 164.6969 - val_kl_loss: 2.0407 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 195.9572 - reconstruction_loss: 179.1356 - kl_loss: 2.1162 - val_loss: 165.0810 - val_reconstruction_loss: 162.9516 - val_kl_loss: 2.1294 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 181.0844 - reconstruction_loss: 176.1977 - kl_loss: 2.5334 - val_loss: 161.8009 - val_reconstruction_loss: 158.8132 - val_kl_loss: 2.9877 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 173.1570 - reconstruction_loss: 171.0779 - kl_loss: 3.1497 - val_loss: 159.7699 - val_reconstruction_loss: 156.7121 - val_kl_loss: 3.0577 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 180.3646 - reconstruction_loss: 166.7736 - kl_loss: 3.2582 - val_loss: 157.3060 - val_reconstruction_loss: 154.1350 - val_kl_loss: 3.1710 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 162.5001 - reconstruction_loss: 164.6075 - kl_loss: 3.3476 - val_loss: 158.6185 - val_reconstruction_loss: 155.5019 - val_kl_loss: 3.1167 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.1656 - reconstruction_loss: 163.7102 - kl_loss: 3.3849 - val_loss: 155.5612 - val_reconstruction_loss: 152.2398 - val_kl_loss: 3.3213 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 165.2117 - reconstruction_loss: 159.5645 - kl_loss: 3.5044 - val_loss: 152.5296 - val_reconstruction_loss: 149.1107 - val_kl_loss: 3.4189 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 160.7789 - reconstruction_loss: 156.1585 - kl_loss: 3.6084 - val_loss: 152.4278 - val_reconstruction_loss: 148.9117 - val_kl_loss: 3.5161 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.4002 - reconstruction_loss: 152.6655 - kl_loss: 3.6985 - val_loss: 151.1544 - val_reconstruction_loss: 147.6625 - val_kl_loss: 3.4920 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.0492 - reconstruction_loss: 150.2143 - kl_loss: 3.8091 - val_loss: 153.6067 - val_reconstruction_loss: 150.1935 - val_kl_loss: 3.4132 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 152.8529 - reconstruction_loss: 151.7853 - kl_loss: 3.7978 - val_loss: 150.6814 - val_reconstruction_loss: 147.0171 - val_kl_loss: 3.6642 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 154.8681 - reconstruction_loss: 148.3566 - kl_loss: 3.8804 - val_loss: 149.9503 - val_reconstruction_loss: 146.2621 - val_kl_loss: 3.6882 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 153.2503 - reconstruction_loss: 145.8342 - kl_loss: 3.9488 - val_loss: 151.3137 - val_reconstruction_loss: 147.6548 - val_kl_loss: 3.6589 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "378/391 [============================>.] - ETA: 0s - loss: 151.7008 - reconstruction_loss: 150.7594 - kl_loss: 3.9000\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 151.7989 - reconstruction_loss: 150.3763 - kl_loss: 3.9063 - val_loss: 150.2500 - val_reconstruction_loss: 146.5582 - val_kl_loss: 3.6918 - lr: 0.0010\n",
      "Epoch 00017: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_129 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_571 (Dense)              (None, 64)           3712        ['input_129[0][0]']              \n",
      "                                                                                                  \n",
      " dense_572 (Dense)              (None, 16)           1040        ['dense_571[0][0]']              \n",
      "                                                                                                  \n",
      " dense_573 (Dense)              (None, 3)            51          ['dense_572[0][0]']              \n",
      "                                                                                                  \n",
      " dense_574 (Dense)              (None, 3)            51          ['dense_572[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_64 (Sampling)         (None, 3)            0           ['dense_573[0][0]',              \n",
      "                                                                  'dense_574[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,854\n",
      "Trainable params: 4,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_130 (InputLayer)      [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_575 (Dense)           (None, 16)                64        \n",
      "                                                                 \n",
      " dense_576 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_577 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_578 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_579 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,393\n",
      "Trainable params: 18,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 185.8517 - reconstruction_loss: 190.4583 - kl_loss: 0.3337 - val_loss: 172.6024 - val_reconstruction_loss: 171.6588 - val_kl_loss: 0.9436 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 179.0554 - reconstruction_loss: 186.0900 - kl_loss: 1.1270 - val_loss: 170.1756 - val_reconstruction_loss: 169.0284 - val_kl_loss: 1.1472 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 176.7468 - reconstruction_loss: 182.1072 - kl_loss: 1.8004 - val_loss: 167.1823 - val_reconstruction_loss: 165.0942 - val_kl_loss: 2.0881 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 175.0812 - reconstruction_loss: 177.0735 - kl_loss: 2.6068 - val_loss: 163.3943 - val_reconstruction_loss: 160.3661 - val_kl_loss: 3.0281 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 201.1867 - reconstruction_loss: 272.2306 - kl_loss: 9.4722 - val_loss: 164.1426 - val_reconstruction_loss: 161.0450 - val_kl_loss: 3.0975 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 173.0456 - reconstruction_loss: 167.9307 - kl_loss: 3.4747 - val_loss: 160.5154 - val_reconstruction_loss: 157.2898 - val_kl_loss: 3.2256 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 179.4637 - reconstruction_loss: 161.9837 - kl_loss: 3.5704 - val_loss: 157.9442 - val_reconstruction_loss: 154.5549 - val_kl_loss: 3.3893 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 159.0515 - reconstruction_loss: 158.7081 - kl_loss: 3.6605 - val_loss: 157.3097 - val_reconstruction_loss: 153.8461 - val_kl_loss: 3.4636 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 161.2368 - reconstruction_loss: 159.0032 - kl_loss: 3.7268 - val_loss: 156.4603 - val_reconstruction_loss: 153.0376 - val_kl_loss: 3.4227 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 160.8986 - reconstruction_loss: 161.8447 - kl_loss: 3.6584 - val_loss: 156.7997 - val_reconstruction_loss: 153.3467 - val_kl_loss: 3.4530 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 155.0145 - reconstruction_loss: 156.4319 - kl_loss: 3.8333 - val_loss: 155.8343 - val_reconstruction_loss: 152.4650 - val_kl_loss: 3.3693 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 157.1037 - reconstruction_loss: 153.7791 - kl_loss: 3.8099 - val_loss: 153.6895 - val_reconstruction_loss: 150.1891 - val_kl_loss: 3.5004 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 158.2199 - reconstruction_loss: 151.0084 - kl_loss: 3.8481 - val_loss: 153.5707 - val_reconstruction_loss: 150.0370 - val_kl_loss: 3.5338 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 154.9807 - reconstruction_loss: 152.4550 - kl_loss: 3.8115 - val_loss: 153.6394 - val_reconstruction_loss: 150.0323 - val_kl_loss: 3.6070 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 152.8999 - reconstruction_loss: 148.3157 - kl_loss: 3.9118 - val_loss: 152.8587 - val_reconstruction_loss: 149.2731 - val_kl_loss: 3.5856 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 170.3306 - reconstruction_loss: 153.9359 - kl_loss: 3.8274 - val_loss: 149.8366 - val_reconstruction_loss: 146.1808 - val_kl_loss: 3.6559 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.9909 - reconstruction_loss: 147.8693 - kl_loss: 3.9980 - val_loss: 149.5789 - val_reconstruction_loss: 145.9067 - val_kl_loss: 3.6722 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 150.6128 - reconstruction_loss: 146.5334 - kl_loss: 3.9771 - val_loss: 149.5356 - val_reconstruction_loss: 145.9158 - val_kl_loss: 3.6198 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 167.2286 - reconstruction_loss: 150.4237 - kl_loss: 3.8989 - val_loss: 147.9409 - val_reconstruction_loss: 144.2098 - val_kl_loss: 3.7312 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 146.4213 - reconstruction_loss: 143.3315 - kl_loss: 3.9716 - val_loss: 147.3979 - val_reconstruction_loss: 143.6195 - val_kl_loss: 3.7784 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 145.0447 - reconstruction_loss: 142.3383 - kl_loss: 4.0674 - val_loss: 146.9839 - val_reconstruction_loss: 143.2892 - val_kl_loss: 3.6947 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 144.5291 - reconstruction_loss: 143.7881 - kl_loss: 3.9668 - val_loss: 147.5927 - val_reconstruction_loss: 143.8778 - val_kl_loss: 3.7149 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 146.7804 - reconstruction_loss: 141.0003 - kl_loss: 4.0526 - val_loss: 146.1402 - val_reconstruction_loss: 142.4550 - val_kl_loss: 3.6851 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 145.3248 - reconstruction_loss: 145.8483 - kl_loss: 3.9558 - val_loss: 145.3575 - val_reconstruction_loss: 141.6762 - val_kl_loss: 3.6812 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 142.8308 - reconstruction_loss: 139.9694 - kl_loss: 4.1051 - val_loss: 144.8086 - val_reconstruction_loss: 141.0600 - val_kl_loss: 3.7487 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 141.6487 - reconstruction_loss: 137.3161 - kl_loss: 4.0920 - val_loss: 144.6642 - val_reconstruction_loss: 140.8681 - val_kl_loss: 3.7961 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 138.6902 - reconstruction_loss: 137.6712 - kl_loss: 4.0979 - val_loss: 143.0586 - val_reconstruction_loss: 139.3481 - val_kl_loss: 3.7105 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 140.0724 - reconstruction_loss: 138.7235 - kl_loss: 4.0231 - val_loss: 144.3894 - val_reconstruction_loss: 140.6893 - val_kl_loss: 3.7000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "385/391 [============================>.] - ETA: 0s - loss: 140.8555 - reconstruction_loss: 137.7116 - kl_loss: 4.1184\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 140.8714 - reconstruction_loss: 137.6668 - kl_loss: 4.1144 - val_loss: 145.5968 - val_reconstruction_loss: 141.7917 - val_kl_loss: 3.8051 - lr: 0.0010\n",
      "Epoch 00029: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_131 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_580 (Dense)              (None, 64)           3712        ['input_131[0][0]']              \n",
      "                                                                                                  \n",
      " dense_581 (Dense)              (None, 16)           1040        ['dense_580[0][0]']              \n",
      "                                                                                                  \n",
      " dense_582 (Dense)              (None, 3)            51          ['dense_581[0][0]']              \n",
      "                                                                                                  \n",
      " dense_583 (Dense)              (None, 3)            51          ['dense_581[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_65 (Sampling)         (None, 3)            0           ['dense_582[0][0]',              \n",
      "                                                                  'dense_583[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,854\n",
      "Trainable params: 4,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_132 (InputLayer)      [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_584 (Dense)           (None, 16)                64        \n",
      "                                                                 \n",
      " dense_585 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_586 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_587 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_588 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,393\n",
      "Trainable params: 18,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 208.3694 - reconstruction_loss: 190.8133 - kl_loss: 0.1938 - val_loss: 172.9825 - val_reconstruction_loss: 172.0193 - val_kl_loss: 0.9633 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 186.7260 - reconstruction_loss: 184.2262 - kl_loss: 1.7589 - val_loss: 167.9059 - val_reconstruction_loss: 165.8740 - val_kl_loss: 2.0319 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 186.7153 - reconstruction_loss: 178.5401 - kl_loss: 2.5154 - val_loss: 164.0466 - val_reconstruction_loss: 161.0912 - val_kl_loss: 2.9553 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 188.6477 - reconstruction_loss: 173.2224 - kl_loss: 3.1218 - val_loss: 159.6202 - val_reconstruction_loss: 156.5637 - val_kl_loss: 3.0565 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 163.6756 - reconstruction_loss: 168.6823 - kl_loss: 3.3075 - val_loss: 157.2622 - val_reconstruction_loss: 153.9581 - val_kl_loss: 3.3040 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 163.3145 - reconstruction_loss: 164.0511 - kl_loss: 3.4752 - val_loss: 154.3976 - val_reconstruction_loss: 150.9754 - val_kl_loss: 3.4223 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.6800 - reconstruction_loss: 159.8028 - kl_loss: 3.5937 - val_loss: 154.9613 - val_reconstruction_loss: 151.5535 - val_kl_loss: 3.4078 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 156.8092 - reconstruction_loss: 155.5782 - kl_loss: 3.6646 - val_loss: 151.0316 - val_reconstruction_loss: 147.5216 - val_kl_loss: 3.5100 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 153.1918 - reconstruction_loss: 151.8764 - kl_loss: 3.7196 - val_loss: 147.7890 - val_reconstruction_loss: 144.1954 - val_kl_loss: 3.5936 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 154.7338 - reconstruction_loss: 149.1218 - kl_loss: 3.7882 - val_loss: 146.5864 - val_reconstruction_loss: 143.0034 - val_kl_loss: 3.5830 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.1503 - reconstruction_loss: 152.9816 - kl_loss: 3.8191 - val_loss: 146.5949 - val_reconstruction_loss: 142.8206 - val_kl_loss: 3.7743 - lr: 0.0010\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381/391 [============================>.] - ETA: 0s - loss: 154.6994 - reconstruction_loss: 154.5613 - kl_loss: 3.7220\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 154.7952 - reconstruction_loss: 154.3580 - kl_loss: 3.7179 - val_loss: 147.9372 - val_reconstruction_loss: 144.2745 - val_kl_loss: 3.6627 - lr: 0.0010\n",
      "Epoch 00012: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_133 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_589 (Dense)              (None, 64)           3712        ['input_133[0][0]']              \n",
      "                                                                                                  \n",
      " dense_590 (Dense)              (None, 8)            520         ['dense_589[0][0]']              \n",
      "                                                                                                  \n",
      " dense_591 (Dense)              (None, 3)            27          ['dense_590[0][0]']              \n",
      "                                                                                                  \n",
      " dense_592 (Dense)              (None, 3)            27          ['dense_590[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_66 (Sampling)         (None, 3)            0           ['dense_591[0][0]',              \n",
      "                                                                  'dense_592[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,286\n",
      "Trainable params: 4,286\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_134 (InputLayer)      [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_593 (Dense)           (None, 16)                64        \n",
      "                                                                 \n",
      " dense_594 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_595 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_596 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_597 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,393\n",
      "Trainable params: 18,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 177.1661 - reconstruction_loss: 190.5949 - kl_loss: 0.2410 - val_loss: 172.4777 - val_reconstruction_loss: 171.4015 - val_kl_loss: 1.0761 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 198.8581 - reconstruction_loss: 183.0972 - kl_loss: 1.8320 - val_loss: 168.3008 - val_reconstruction_loss: 166.3947 - val_kl_loss: 1.9060 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 191.9523 - reconstruction_loss: 179.0883 - kl_loss: 2.1521 - val_loss: 165.0896 - val_reconstruction_loss: 162.9507 - val_kl_loss: 2.1390 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 175.4698 - reconstruction_loss: 173.9092 - kl_loss: 2.3695 - val_loss: 165.4979 - val_reconstruction_loss: 163.1489 - val_kl_loss: 2.3490 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 173.1226 - reconstruction_loss: 169.6234 - kl_loss: 3.0634 - val_loss: 163.3749 - val_reconstruction_loss: 160.2564 - val_kl_loss: 3.1185 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 172.1125 - reconstruction_loss: 167.0700 - kl_loss: 3.3333 - val_loss: 160.1918 - val_reconstruction_loss: 156.9933 - val_kl_loss: 3.1986 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 165.2773 - reconstruction_loss: 162.0231 - kl_loss: 3.3965 - val_loss: 159.5555 - val_reconstruction_loss: 156.3017 - val_kl_loss: 3.2538 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.2872 - reconstruction_loss: 160.1907 - kl_loss: 3.4150 - val_loss: 157.5775 - val_reconstruction_loss: 154.2782 - val_kl_loss: 3.2993 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 176.9429 - reconstruction_loss: 161.1160 - kl_loss: 3.5720 - val_loss: 156.7401 - val_reconstruction_loss: 153.3802 - val_kl_loss: 3.3599 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 159.7585 - reconstruction_loss: 162.8483 - kl_loss: 3.5228 - val_loss: 155.8817 - val_reconstruction_loss: 152.5166 - val_kl_loss: 3.3652 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 165.8078 - reconstruction_loss: 157.2710 - kl_loss: 3.5605 - val_loss: 154.8175 - val_reconstruction_loss: 151.3649 - val_kl_loss: 3.4525 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 155.2640 - reconstruction_loss: 154.4670 - kl_loss: 3.6450 - val_loss: 154.7003 - val_reconstruction_loss: 151.2137 - val_kl_loss: 3.4866 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 162.8526 - reconstruction_loss: 153.5210 - kl_loss: 3.6692 - val_loss: 155.8273 - val_reconstruction_loss: 152.3003 - val_kl_loss: 3.5270 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 152.7002 - reconstruction_loss: 151.5522 - kl_loss: 3.7246 - val_loss: 152.9520 - val_reconstruction_loss: 149.4098 - val_kl_loss: 3.5422 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 155.9792 - reconstruction_loss: 153.5561 - kl_loss: 3.7342 - val_loss: 152.4531 - val_reconstruction_loss: 148.8295 - val_kl_loss: 3.6236 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.3356 - reconstruction_loss: 151.0693 - kl_loss: 3.7940 - val_loss: 152.6147 - val_reconstruction_loss: 148.9577 - val_kl_loss: 3.6570 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 155.3546 - reconstruction_loss: 154.0777 - kl_loss: 3.8103 - val_loss: 151.5215 - val_reconstruction_loss: 147.8315 - val_kl_loss: 3.6901 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 156.3143 - reconstruction_loss: 147.1509 - kl_loss: 3.8492 - val_loss: 150.3444 - val_reconstruction_loss: 146.7220 - val_kl_loss: 3.6223 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 147.7774 - reconstruction_loss: 145.2724 - kl_loss: 3.9099 - val_loss: 149.9865 - val_reconstruction_loss: 146.2747 - val_kl_loss: 3.7118 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 150.4007 - reconstruction_loss: 146.4171 - kl_loss: 3.9174 - val_loss: 149.9442 - val_reconstruction_loss: 146.1751 - val_kl_loss: 3.7691 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 152.8971 - reconstruction_loss: 151.4980 - kl_loss: 3.9489 - val_loss: 149.3953 - val_reconstruction_loss: 145.5639 - val_kl_loss: 3.8314 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 147.1180 - reconstruction_loss: 145.9815 - kl_loss: 3.9735 - val_loss: 151.0057 - val_reconstruction_loss: 147.1132 - val_kl_loss: 3.8925 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 148.1715 - reconstruction_loss: 141.7255 - kl_loss: 4.0507 - val_loss: 147.9626 - val_reconstruction_loss: 144.0889 - val_kl_loss: 3.8738 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 151.4256 - reconstruction_loss: 143.2542 - kl_loss: 4.0317 - val_loss: 147.7639 - val_reconstruction_loss: 143.9234 - val_kl_loss: 3.8404 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 142.2249 - reconstruction_loss: 139.9826 - kl_loss: 4.0565 - val_loss: 147.5885 - val_reconstruction_loss: 143.7363 - val_kl_loss: 3.8522 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 141.1240 - reconstruction_loss: 138.6829 - kl_loss: 4.1285 - val_loss: 147.6199 - val_reconstruction_loss: 143.7522 - val_kl_loss: 3.8678 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 155.2888 - reconstruction_loss: 144.3149 - kl_loss: 4.1139 - val_loss: 146.9204 - val_reconstruction_loss: 142.9632 - val_kl_loss: 3.9572 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 163.2141 - reconstruction_loss: 145.0156 - kl_loss: 4.1024 - val_loss: 146.0763 - val_reconstruction_loss: 142.1496 - val_kl_loss: 3.9267 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 144.8849 - reconstruction_loss: 141.8808 - kl_loss: 4.1370 - val_loss: 146.0187 - val_reconstruction_loss: 142.1089 - val_kl_loss: 3.9098 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "383/391 [============================>.] - ETA: 0s - loss: 144.3162 - reconstruction_loss: 139.9552 - kl_loss: 4.1661\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 144.3086 - reconstruction_loss: 139.7129 - kl_loss: 4.1625 - val_loss: 146.3772 - val_reconstruction_loss: 142.4148 - val_kl_loss: 3.9624 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 142.9204 - reconstruction_loss: 137.5038 - kl_loss: 4.2348 - val_loss: 145.1725 - val_reconstruction_loss: 141.2222 - val_kl_loss: 3.9503 - lr: 1.0000e-04\n",
      "Epoch 32/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 138.2380 - reconstruction_loss: 135.3374 - kl_loss: 4.2232 - val_loss: 144.8569 - val_reconstruction_loss: 140.8570 - val_kl_loss: 3.9998 - lr: 1.0000e-04\n",
      "Epoch 33/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 139.2688 - reconstruction_loss: 134.5560 - kl_loss: 4.2505 - val_loss: 144.6618 - val_reconstruction_loss: 140.6717 - val_kl_loss: 3.9902 - lr: 1.0000e-04\n",
      "Epoch 34/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 139.8405 - reconstruction_loss: 134.1385 - kl_loss: 4.2481 - val_loss: 144.5418 - val_reconstruction_loss: 140.5444 - val_kl_loss: 3.9974 - lr: 1.0000e-04\n",
      "Epoch 35/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 140.5265 - reconstruction_loss: 133.8372 - kl_loss: 4.2497 - val_loss: 144.3851 - val_reconstruction_loss: 140.3760 - val_kl_loss: 4.0091 - lr: 1.0000e-04\n",
      "Epoch 36/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 138.2880 - reconstruction_loss: 133.6357 - kl_loss: 4.2551 - val_loss: 144.2731 - val_reconstruction_loss: 140.2705 - val_kl_loss: 4.0026 - lr: 1.0000e-04\n",
      "Epoch 37/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 136.1973 - reconstruction_loss: 133.5139 - kl_loss: 4.2621 - val_loss: 144.1418 - val_reconstruction_loss: 140.1262 - val_kl_loss: 4.0156 - lr: 1.0000e-04\n",
      "Epoch 38/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 137.2643 - reconstruction_loss: 133.2766 - kl_loss: 4.2613 - val_loss: 144.0157 - val_reconstruction_loss: 139.9918 - val_kl_loss: 4.0239 - lr: 1.0000e-04\n",
      "Epoch 39/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.3459 - reconstruction_loss: 133.1451 - kl_loss: 4.2680 - val_loss: 143.9367 - val_reconstruction_loss: 139.8901 - val_kl_loss: 4.0467 - lr: 1.0000e-04\n",
      "Epoch 40/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 137.5136 - reconstruction_loss: 132.9786 - kl_loss: 4.2819 - val_loss: 143.8323 - val_reconstruction_loss: 139.7982 - val_kl_loss: 4.0341 - lr: 1.0000e-04\n",
      "Epoch 41/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.8090 - reconstruction_loss: 132.8319 - kl_loss: 4.2797 - val_loss: 143.7894 - val_reconstruction_loss: 139.7418 - val_kl_loss: 4.0476 - lr: 1.0000e-04\n",
      "Epoch 42/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.2056 - reconstruction_loss: 132.6515 - kl_loss: 4.2849 - val_loss: 143.6128 - val_reconstruction_loss: 139.5603 - val_kl_loss: 4.0526 - lr: 1.0000e-04\n",
      "Epoch 43/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.9231 - reconstruction_loss: 132.5352 - kl_loss: 4.2925 - val_loss: 143.4837 - val_reconstruction_loss: 139.4385 - val_kl_loss: 4.0451 - lr: 1.0000e-04\n",
      "Epoch 44/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 136.5989 - reconstruction_loss: 132.4120 - kl_loss: 4.2859 - val_loss: 143.4147 - val_reconstruction_loss: 139.3370 - val_kl_loss: 4.0777 - lr: 1.0000e-04\n",
      "Epoch 45/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.8731 - reconstruction_loss: 132.2925 - kl_loss: 4.2969 - val_loss: 143.3625 - val_reconstruction_loss: 139.2953 - val_kl_loss: 4.0673 - lr: 1.0000e-04\n",
      "Epoch 46/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 136.1316 - reconstruction_loss: 132.1903 - kl_loss: 4.3061 - val_loss: 143.2339 - val_reconstruction_loss: 139.1542 - val_kl_loss: 4.0798 - lr: 1.0000e-04\n",
      "Epoch 47/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.0533 - reconstruction_loss: 132.0873 - kl_loss: 4.3046 - val_loss: 143.1872 - val_reconstruction_loss: 139.1226 - val_kl_loss: 4.0647 - lr: 1.0000e-04\n",
      "Epoch 48/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 135.6133 - reconstruction_loss: 131.8925 - kl_loss: 4.3095 - val_loss: 143.0850 - val_reconstruction_loss: 139.0254 - val_kl_loss: 4.0595 - lr: 1.0000e-04\n",
      "Epoch 49/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 139.5405 - reconstruction_loss: 131.7797 - kl_loss: 4.3117 - val_loss: 143.0380 - val_reconstruction_loss: 138.9647 - val_kl_loss: 4.0734 - lr: 1.0000e-04\n",
      "Epoch 50/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 136.1596 - reconstruction_loss: 131.7053 - kl_loss: 4.3253 - val_loss: 142.9346 - val_reconstruction_loss: 138.8413 - val_kl_loss: 4.0933 - lr: 1.0000e-04\n",
      "Epoch 51/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 136.7923 - reconstruction_loss: 131.5078 - kl_loss: 4.3105 - val_loss: 142.8449 - val_reconstruction_loss: 138.7569 - val_kl_loss: 4.0880 - lr: 1.0000e-04\n",
      "Epoch 52/500\n",
      "390/391 [============================>.] - ETA: 0s - loss: 135.0050 - reconstruction_loss: 131.3385 - kl_loss: 4.3280\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.0089 - reconstruction_loss: 131.4413 - kl_loss: 4.3266 - val_loss: 142.8506 - val_reconstruction_loss: 138.7637 - val_kl_loss: 4.0869 - lr: 1.0000e-04\n",
      "Epoch 53/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.7664 - reconstruction_loss: 131.1354 - kl_loss: 4.3187 - val_loss: 142.8370 - val_reconstruction_loss: 138.7495 - val_kl_loss: 4.0875 - lr: 1.0000e-05\n",
      "Epoch 54/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 134.8702 - reconstruction_loss: 131.0943 - kl_loss: 4.3215 - val_loss: 142.8158 - val_reconstruction_loss: 138.7277 - val_kl_loss: 4.0880 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 133.8453 - reconstruction_loss: 131.0875 - kl_loss: 4.3235 - val_loss: 142.8288 - val_reconstruction_loss: 138.7346 - val_kl_loss: 4.0942 - lr: 1.0000e-05\n",
      "Epoch 56/500\n",
      "388/391 [============================>.] - ETA: 0s - loss: 136.7159 - reconstruction_loss: 131.1267 - kl_loss: 4.3305\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 136.7024 - reconstruction_loss: 131.0300 - kl_loss: 4.3268 - val_loss: 142.8035 - val_reconstruction_loss: 138.7057 - val_kl_loss: 4.0978 - lr: 1.0000e-05\n",
      "Epoch 57/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 137.9811 - reconstruction_loss: 130.9750 - kl_loss: 4.3285 - val_loss: 142.7957 - val_reconstruction_loss: 138.6980 - val_kl_loss: 4.0978 - lr: 1.0000e-06\n",
      "Epoch 58/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 136.5975 - reconstruction_loss: 130.9759 - kl_loss: 4.3293 - val_loss: 142.7913 - val_reconstruction_loss: 138.6934 - val_kl_loss: 4.0979 - lr: 1.0000e-06\n",
      "Epoch 59/500\n",
      "376/391 [===========================>..] - ETA: 0s - loss: 135.4105 - reconstruction_loss: 130.6936 - kl_loss: 4.3395\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.3998 - reconstruction_loss: 130.9763 - kl_loss: 4.3290 - val_loss: 142.8005 - val_reconstruction_loss: 138.7023 - val_kl_loss: 4.0981 - lr: 1.0000e-06\n",
      "Epoch 60/500\n",
      "378/391 [============================>.] - ETA: 0s - loss: 135.4293 - reconstruction_loss: 131.1642 - kl_loss: 4.3327Restoring model weights from the end of the best epoch: 58.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 135.4272 - reconstruction_loss: 131.0045 - kl_loss: 4.3302 - val_loss: 142.7975 - val_reconstruction_loss: 138.6986 - val_kl_loss: 4.0988 - lr: 1.0000e-06\n",
      "Epoch 00060: early stopping\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_135 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_598 (Dense)              (None, 64)           3712        ['input_135[0][0]']              \n",
      "                                                                                                  \n",
      " dense_599 (Dense)              (None, 8)            520         ['dense_598[0][0]']              \n",
      "                                                                                                  \n",
      " dense_600 (Dense)              (None, 3)            27          ['dense_599[0][0]']              \n",
      "                                                                                                  \n",
      " dense_601 (Dense)              (None, 3)            27          ['dense_599[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_67 (Sampling)         (None, 3)            0           ['dense_600[0][0]',              \n",
      "                                                                  'dense_601[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,286\n",
      "Trainable params: 4,286\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_136 (InputLayer)      [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_602 (Dense)           (None, 16)                64        \n",
      "                                                                 \n",
      " dense_603 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " dense_604 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_605 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_606 (Dense)           (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,393\n",
      "Trainable params: 18,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 182.2351 - reconstruction_loss: 191.3031 - kl_loss: 0.0112 - val_loss: 174.9879 - val_reconstruction_loss: 174.9568 - val_kl_loss: 0.0311 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 192.8846 - reconstruction_loss: 187.6574 - kl_loss: 0.9233 - val_loss: 169.6832 - val_reconstruction_loss: 167.8419 - val_kl_loss: 1.8413 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 179.7350 - reconstruction_loss: 179.0440 - kl_loss: 2.6455 - val_loss: 163.2650 - val_reconstruction_loss: 160.2992 - val_kl_loss: 2.9658 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 167.1392 - reconstruction_loss: 172.3506 - kl_loss: 3.1905 - val_loss: 161.8311 - val_reconstruction_loss: 158.6823 - val_kl_loss: 3.1489 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 168.7091 - reconstruction_loss: 169.5941 - kl_loss: 3.3129 - val_loss: 158.1961 - val_reconstruction_loss: 154.9399 - val_kl_loss: 3.2562 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 165.9244 - reconstruction_loss: 166.8297 - kl_loss: 3.4426 - val_loss: 157.5597 - val_reconstruction_loss: 154.1579 - val_kl_loss: 3.4017 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 160.9311 - reconstruction_loss: 163.4574 - kl_loss: 3.5060 - val_loss: 156.6486 - val_reconstruction_loss: 153.3137 - val_kl_loss: 3.3349 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 158.9550 - reconstruction_loss: 162.6139 - kl_loss: 3.5441 - val_loss: 159.5468 - val_reconstruction_loss: 156.2271 - val_kl_loss: 3.3197 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 162.8882 - reconstruction_loss: 161.7599 - kl_loss: 3.5648 - val_loss: 155.6292 - val_reconstruction_loss: 152.2571 - val_kl_loss: 3.3721 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 162.0087 - reconstruction_loss: 156.2004 - kl_loss: 3.5748 - val_loss: 153.4693 - val_reconstruction_loss: 150.0081 - val_kl_loss: 3.4612 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 167.0272 - reconstruction_loss: 155.0347 - kl_loss: 3.6105 - val_loss: 152.9252 - val_reconstruction_loss: 149.3566 - val_kl_loss: 3.5686 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 153.5184 - reconstruction_loss: 152.5290 - kl_loss: 3.7017 - val_loss: 152.7135 - val_reconstruction_loss: 149.1663 - val_kl_loss: 3.5472 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 153.5762 - reconstruction_loss: 151.6552 - kl_loss: 3.7480 - val_loss: 152.1003 - val_reconstruction_loss: 148.4586 - val_kl_loss: 3.6418 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 150.8500 - reconstruction_loss: 156.5163 - kl_loss: 3.8061 - val_loss: 157.6759 - val_reconstruction_loss: 154.0617 - val_kl_loss: 3.6142 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 153.2885 - reconstruction_loss: 152.6630 - kl_loss: 3.8576 - val_loss: 151.7577 - val_reconstruction_loss: 148.1931 - val_kl_loss: 3.5646 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 156.0956 - reconstruction_loss: 149.2955 - kl_loss: 3.8165 - val_loss: 148.9469 - val_reconstruction_loss: 145.2867 - val_kl_loss: 3.6601 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 147.1254 - reconstruction_loss: 146.9710 - kl_loss: 3.9082 - val_loss: 150.8207 - val_reconstruction_loss: 147.1062 - val_kl_loss: 3.7145 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "375/391 [===========================>..] - ETA: 0s - loss: 153.1078 - reconstruction_loss: 145.5690 - kl_loss: 3.8451\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 152.9779 - reconstruction_loss: 147.6361 - kl_loss: 3.9270 - val_loss: 153.0112 - val_reconstruction_loss: 149.2051 - val_kl_loss: 3.8062 - lr: 0.0010\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "from numpy.random import seed\n",
    "\n",
    "bounds = [{'name': 'latent_dim', 'type': 'discrete', 'domain':(3, 4, 5, 6, 7, 8)},\n",
    "          {'name': 'outer_layer_width', 'type': 'discrete', 'domain':(16, 32, 64)},\n",
    "          {'name': 'inner_layer_width', 'type': 'discrete', 'domain':(8, 16, 32)},\n",
    "          {'name': 'beta', 'type': 'continuous', 'domain':(0,1)}]\n",
    "          #{'name': 'batch_size', 'type': 'discrete', 'domain':(1024)}]\n",
    "\n",
    "max_iter = 1000\n",
    "myProblem = GPyOpt.methods.BayesianOptimization(main, domain=bounds)\n",
    "myProblem.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "374ecb37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation results... \n",
      "    latent_dim  outer_layer_width  inner_layer_width          beta  objective\n",
      "0          4.0               16.0               16.0  9.186001e-01   0.003849\n",
      "1          6.0               64.0                8.0  1.631360e-01   0.000246\n",
      "2          3.0               64.0                8.0  1.612322e-01   0.001230\n",
      "3          7.0               32.0                8.0  1.292525e-01   0.019508\n",
      "4          4.0               16.0               16.0  7.902077e-01   0.011854\n",
      "5          6.0               64.0                8.0  1.635160e-01   0.011713\n",
      "6          4.0               64.0                8.0  1.447487e-01   0.003471\n",
      "7          4.0               64.0                8.0  1.561783e-01   0.001911\n",
      "8          3.0               64.0                8.0  1.510058e-01   0.001932\n",
      "9          3.0               64.0               16.0  0.000000e+00   0.002862\n",
      "10         3.0               64.0               16.0  1.000000e+00   0.005901\n",
      "11         3.0               64.0                8.0  0.000000e+00   0.004122\n",
      "12         3.0               64.0                8.0  1.387779e-17   0.010634\n",
      "The value of (latent_dim, outer_layer width, inner_layer_width, beta) that minimizes the objective is: \n",
      "[ 6.         64.          8.          0.16313603]\n",
      "The the max efficiency found is: 0.0002463124381674349\n"
     ]
    }
   ],
   "source": [
    "# evaluate results of optimization\n",
    "myProblem.plot_convergence()\n",
    "print('Writing evaluation results... ')\n",
    "param1 = myProblem.get_evaluations()[0][:,0].flatten()\n",
    "param2 = myProblem.get_evaluations()[0][:,1].flatten()\n",
    "param3 = myProblem.get_evaluations()[0][:,2].flatten()\n",
    "param4 = myProblem.get_evaluations()[0][:,3].flatten()\n",
    "#param5 = myProblem.get_evaluations()[0][:,4].flatten()\n",
    "out = myProblem.get_evaluations()[1][:].flatten()\n",
    "opt_results = {'latent_dim': param1,\n",
    "               'outer_layer_width': param2,\n",
    "               'inner_layer_width': param3,\n",
    "               'beta': param4,\n",
    "               #'batch_size': param5,\n",
    "               'objective': out}\n",
    "\n",
    "evals = pd.DataFrame(opt_results)\n",
    "print(evals)\n",
    "\n",
    "print('The value of (latent_dim, outer_layer width, inner_layer_width, beta) that minimizes the objective is: \\n'+str(myProblem.x_opt))\n",
    "print('The the max efficiency found is: '+str(myProblem.fx_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f753b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
