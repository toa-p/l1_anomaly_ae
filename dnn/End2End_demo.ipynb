{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c10c6-1c56-4229-a180-33b39895bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "hep.style.use(hep.style.ROOT)\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "from model import build_AE, build_VAE\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ba512-7369-4b0a-892a-bcf0c9b4cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended/BSM_preprocessed.h5\"\n",
    "events = 10000\n",
    "load_pickle=False\n",
    "input_pickle='data-for-dnn-v2.pickle'\n",
    "output_pfile=\"data.pickle\"\n",
    "\n",
    "\n",
    "if(load_pickle):\n",
    "    if(input_pickle==''):\n",
    "        print('Please provide input pickle files')\n",
    "    with open(\"data-for-dnn-v2.pickle\", 'rb') as f:\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)\n",
    "        bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "              'VectorZPrimeToQQ__M100',\n",
    "              'VectorZPrimeToQQ__M200',\n",
    "              'VBF_HToInvisible_M125',\n",
    "              'VBF_HToInvisible_M125_private',\n",
    "              'ZprimeToZH_MZprime1000',\n",
    "              'ZprimeToZH_MZprime800',\n",
    "              'ZprimeToZH_MZprime600',\n",
    "              'GluGluToHHTo4B',\n",
    "              'HTo2LongLivedTo4mu_1000',\n",
    "              'HTo2LongLivedTo4mu_125_12',\n",
    "              'HTo2LongLivedTo4mu_125_25',\n",
    "              'HTo2LongLivedTo4mu_125_50',\n",
    "              'VBFHToTauTau',\n",
    "              'VBF_HH']\n",
    "\n",
    "else:\n",
    "    if(input_qcd==''or input_bsm==''):\n",
    "        print('Please provide input H5 files')\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, '',True)\n",
    "    \n",
    "    if(output_pfile!='' and not load_pickle):\n",
    "        with open(output_pfile, 'wb') as f:\n",
    "            pickle.dump([X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels], f)\n",
    "        print(\"Saved Pickle data to disk\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca4641-fd81-44d4-ac07-bb1db2a917c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='AE'\n",
    "latent_dim=3\n",
    "batch_size= 1024*16\n",
    "n_epochs = 20\n",
    "\n",
    "if(model_type=='AE'):\n",
    "    autoencoder = build_AE(X_train_flatten.shape[-1],latent_dim)\n",
    "    model = AE(autoencoder)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "    callbacks.append(TerminateOnNaN())\n",
    "    callbacks.append(NeptuneMonitor())\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "elif(model_type=='VAE'):\n",
    "    encoder, decoder = build_VAE(X_train_flatten.shape[-1],latent_dim)\n",
    "    model = VAE(encoder, decoder)\n",
    "    model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "    callbacks.append(TerminateOnNaN())\n",
    "    callbacks.append(NeptuneMonitor())\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "print(\"Training the model\")\n",
    "\n",
    "history = model.fit(X_train_scaled, X_train_scaled,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test_scaled, X_test_scaled),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e76ad-4e27-48a0-bdcb-ab341966d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "\n",
    "if(output_model_h5!=''):\n",
    "    model_json = autoencoder.to_json()\n",
    "    with open(output_model_json, 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    autoencoder.save_weights(output_model_h5)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "if(output_history!=''):\n",
    "    with open(output_history, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(\"Saved history to disk\")\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "print(hist.tail())\n",
    "plt.plot(hist.index.to_numpy(),hist['loss'],label='Loss')\n",
    "plt.plot(hist.index.to_numpy(),hist['val_loss'],label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1e5fd-bfc1-41f7-8ea2-d6b5f61d532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the model\")\n",
    "loss = make_mse_loss_numpy\n",
    "qcd_prediction = model.autoencoder(X_test_scaled).numpy()\n",
    "results={}\n",
    "min_loss,max_loss=1e5,0\n",
    "for i, label in enumerate(bsm_labels):\n",
    "    results[label] = {}\n",
    "    bsm_pred = model.autoencoder(bsm_data[i]).numpy()\n",
    "    results[label]['target'] = bsm_target[i]\n",
    "    results[label]['prediction'] = bsm_pred\n",
    "    total_loss = return_total_loss(loss, bsm_target[i], bsm_pred)\n",
    "    if(np.min(total_loss)<min_loss): min_loss = np.min(total_loss)\n",
    "    if(np.max(total_loss)>max_loss): max_loss = np.max(total_loss)\n",
    "    results[label]['loss'] = total_loss\n",
    "\n",
    "results['QCD'] = {}\n",
    "results['QCD']['target'] = X_test_scaled\n",
    "results['QCD']['prediction'] = qcd_prediction\n",
    "results['QCD']['loss'] = return_total_loss(loss, X_test_scaled, qcd_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d9ee2-a72d-4e6a-9286-c53edb7e3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(output_result!=''):\n",
    "    h5f = h5py.File(output_result, 'w')\n",
    "    h5f.create_dataset('loss', data=hist['loss'].to_numpy())\n",
    "    h5f.create_dataset('val_loss', data=hist['val_loss'].to_numpy())\n",
    "    h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "    h5f.create_dataset('QCD_target', data=X_test_scaled)\n",
    "    h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "    for i, key in enumerate(results):\n",
    "        if(key=='QCD'): continue\n",
    "        h5f.create_dataset('%s_scaled' %key, data=results[key]['target'])\n",
    "        h5f.create_dataset('%s_input' %key, data=bsm_data[i])\n",
    "        h5f.create_dataset('predicted_%s' %key, data=results[key]['prediction'])\n",
    "    print(\"*** OutputFile Created\")\n",
    "    h5f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e656581-64d7-4361-a342-c5e66fae1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting the results\")\n",
    "tag='test'\n",
    "if(max_loss>10000): max_loss=10000\n",
    "bins_=np.linspace(min_loss,max_loss,100)\n",
    "plt.figure(figsize=(10,10))\n",
    "for key in results.keys():\n",
    "    if(key=='QCD'): plt.hist(results[key]['loss'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "    else: plt.hist(results[key]['loss'],label=key,histtype='step',bins=bins_,density=True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Loss distribution')\n",
    "plt.savefig('loss_hist_'+model_type+'_'+tag+'.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for key in results.keys():\n",
    "    if key=='QCD': continue\n",
    "\n",
    "    true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "    pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "    fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "\n",
    "    auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    plt.plot(fpr_loss, tpr_loss, label='%s (AUC = %0.2f)' %(key,auc_loss))\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.title('ROC curve '+model_type)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig('roc_curve_'+model_type+'_'+tag+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742eb672-106f-439e-8119-b6e091ecf174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
