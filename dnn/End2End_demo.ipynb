{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c10c6-1c56-4229-a180-33b39895bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "from model import build_AE, build_VAE\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ba512-7369-4b0a-892a-bcf0c9b4cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended/BSM_preprocessed.h5\"\n",
    "events = 50000\n",
    "load_pickle=False\n",
    "input_pickle='data-for-dnn-v2.pickle'\n",
    "output_pfile=\"data.pickle\"\n",
    "\n",
    "\n",
    "if(load_pickle):\n",
    "    if(input_pickle==''):\n",
    "        print('Please provide input pickle files')\n",
    "    with open(input_pickle, 'rb') as f:\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler = pickle.load(f)\n",
    "        bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "              'VectorZPrimeToQQ__M100',\n",
    "              'VectorZPrimeToQQ__M200',\n",
    "              'VBF_HToInvisible_M125',\n",
    "              'VBF_HToInvisible_M125_private',\n",
    "              'ZprimeToZH_MZprime1000',\n",
    "              'ZprimeToZH_MZprime800',\n",
    "              'ZprimeToZH_MZprime600',\n",
    "              'GluGluToHHTo4B',\n",
    "              'HTo2LongLivedTo4mu_1000',\n",
    "              'HTo2LongLivedTo4mu_125_12',\n",
    "              'HTo2LongLivedTo4mu_125_25',\n",
    "              'HTo2LongLivedTo4mu_125_50',\n",
    "              'VBFHToTauTau',\n",
    "              'VBF_HH']\n",
    "else:\n",
    "    if(input_qcd==''or input_bsm==''):\n",
    "        print('Please provide input H5 files')\n",
    "    X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, '',True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca4641-fd81-44d4-ac07-bb1db2a917c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='VAE'\n",
    "latent_dim=3\n",
    "batch_size= 1024\n",
    "n_epochs = 150\n",
    "\n",
    "if(model_type=='AE'):\n",
    "    autoencoder = build_AE(X_train_flatten.shape[-1],latent_dim)\n",
    "    model = AE(autoencoder)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "    callbacks.append(TerminateOnNaN())\n",
    "    callbacks.append(NeptuneMonitor())\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "elif(model_type=='VAE'):\n",
    "    encoder, decoder = build_VAE(X_train_flatten.shape[-1],latent_dim)\n",
    "    model = VAE(encoder, decoder)\n",
    "    model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    callbacks=[]\n",
    "    callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "    callbacks.append(TerminateOnNaN())\n",
    "    callbacks.append(NeptuneMonitor())\n",
    "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "print(\"Training the model\")\n",
    "\n",
    "history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e76ad-4e27-48a0-bdcb-ab341966d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "\n",
    "if(output_model_h5!=''):\n",
    "    if(model_type=='VAE'):\n",
    "        model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "    else:\n",
    "        model_json = autoencoder.to_json()\n",
    "        with open(output_model_json, 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "        autoencoder.save_weights(output_model_h5)\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "if(output_history!=''):\n",
    "    with open(output_history, 'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(\"Saved history to disk\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot training & validation loss values\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "print(hist.tail())\n",
    "plt.plot(hist.index.to_numpy(),hist['loss'],label='Loss')\n",
    "plt.plot(hist.index.to_numpy(),hist['val_loss'],label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('loss.pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1e5fd-bfc1-41f7-8ea2-d6b5f61d532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating the model\")\n",
    "loss = make_mse_loss_numpy\n",
    "if(model_type=='AE'): \n",
    "    qcd_prediction = model.autoencoder(X_test_flatten).numpy()\n",
    "elif(model_type=='VAE'):\n",
    "    qcd_mean, qcd_logvar, qcd_z = model.encoder(X_test_scaled)\n",
    "    qcd_prediction = model.decoder(qcd_z).numpy()\n",
    "\n",
    "\n",
    "results={}\n",
    "min_loss,max_loss=1e5,0\n",
    "for i, label in enumerate(bsm_labels):\n",
    "    results[label] = {}\n",
    "    if(model_type=='AE'): \n",
    "        bsm_pred = model.autoencoder(bsm_data[i]).numpy()\n",
    "    elif(model_type=='VAE'): \n",
    "        mean_pred, logvar_pred, z_pred = model.encoder(bsm_data[i])\n",
    "        bsm_pred = decoder(z_pred).numpy()\n",
    "    results[label]['target'] = bsm_target[i]\n",
    "    results[label]['prediction'] = bsm_pred\n",
    "    if(model_type=='VAE'):\n",
    "        results[label]['mean_prediction'] = mean_pred.numpy()\n",
    "        results[label]['logvar_prediction'] = logvar_pred.numpy()\n",
    "        results[label]['z_prediction'] = z_pred.numpy()\n",
    "        results[label]['kl_loss'] = kl_loss(mean_pred.numpy(),logvar_pred.numpy())\n",
    "\n",
    "    total_loss = return_total_loss(loss, bsm_target[i], bsm_pred)\n",
    "    if(np.min(total_loss)<min_loss): min_loss = np.min(total_loss)\n",
    "    if(np.max(total_loss)>max_loss): max_loss = np.max(total_loss)\n",
    "    results[label]['loss'] = total_loss\n",
    "    if(model_type=='VAE'):\n",
    "        results[label]['total_loss'] = kl_loss(mean_pred.numpy(),z_pred.numpy())+total_loss\n",
    "        if(np.min(results[label]['total_loss'])<min_loss): min_loss = np.min(total_loss)\n",
    "        if(np.max(results[label]['total_loss'])>max_loss): max_loss = np.max(total_loss)\n",
    "        results[label]['radius'] = radius(mean_pred.numpy(),z_pred.numpy())\n",
    "        \n",
    "\n",
    "results['QCD'] = {}\n",
    "results['QCD']['target'] = X_test_scaled\n",
    "results['QCD']['prediction'] = qcd_prediction\n",
    "qcd_loss = return_total_loss(loss, X_test_scaled, qcd_prediction)\n",
    "results['QCD']['loss'] = qcd_loss\n",
    "if(model_type=='VAE'):\n",
    "    results['QCD']['mean_prediction'] = qcd_mean.numpy()\n",
    "    results['QCD']['logvar_prediction'] = qcd_logvar.numpy()\n",
    "    results['QCD']['z_prediction'] = qcd_z.numpy()\n",
    "    results['QCD']['kl_loss'] = kl_loss(qcd_mean.numpy(),qcd_logvar.numpy())\n",
    "    results['QCD']['total_loss']=kl_loss(qcd_mean.numpy(),qcd_logvar.numpy())+qcd_loss\n",
    "    results['QCD']['radius']=radius(qcd_mean.numpy(),qcd_logvar.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d9ee2-a72d-4e6a-9286-c53edb7e3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(output_result!=''):\n",
    "    h5f = h5py.File(output_result, 'w')\n",
    "    h5f.create_dataset('loss', data=hist['loss'].to_numpy())\n",
    "    h5f.create_dataset('val_loss', data=hist['val_loss'].to_numpy())\n",
    "    h5f.create_dataset('QCD_input', data=X_test_flatten)\n",
    "    h5f.create_dataset('QCD_target', data=X_test_scaled)\n",
    "    h5f.create_dataset('predicted_QCD', data = qcd_prediction)\n",
    "    if(model_type=='VAE'):\n",
    "        h5f.create_dataset('encoded_mean_QCD', data=qcd_mean.numpy())\n",
    "        h5f.create_dataset('encoded_logvar_QCD', data=qcd_logvar.numpy())\n",
    "        h5f.create_dataset('encoded_z_QCD', data=qcd_z.numpy())\n",
    "    for i, key in enumerate(results):\n",
    "        if(key=='QCD'): continue\n",
    "        h5f.create_dataset('%s_scaled' %key, data=results[key]['target'])\n",
    "        h5f.create_dataset('%s_input' %key, data=bsm_data[i])\n",
    "        h5f.create_dataset('predicted_%s' %key, data=results[key]['prediction'])\n",
    "        if(model_type=='VAE'):\n",
    "            h5f.create_dataset('encoded_mean_%s' %key, data=results[key]['mean_prediction'])\n",
    "            h5f.create_dataset('encoded_logvar_%s' %key, data=results[key]['logvar_prediction'])\n",
    "            h5f.create_dataset('encoded_z_%s' %key, data=results[key]['z_prediction'])\n",
    "    print(\"*** OutputFile Created\")\n",
    "    h5f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d322d21-5e62-49a1-8351-33b16cd521ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss,max_loss=1e5,0\n",
    "if(model_type=='VAE'):\n",
    "    min_tloss,max_tloss=1e5,0\n",
    "    min_r,max_r=1e5,0\n",
    "for key in results.keys():\n",
    "    # if(key=='QCD'): continue\n",
    "    if(np.min(results[key]['loss'])<min_loss): min_loss = np.min(results[key]['loss'])\n",
    "    if(np.max(results[key]['loss'])>max_loss): max_loss = np.max(results[key]['loss'])\n",
    "    if(model_type=='VAE'):\n",
    "        if(np.min(results[key]['total_loss'])<min_tloss): min_tloss = np.min(results[key]['total_loss'])\n",
    "        if(np.max(results[key]['total_loss'])>max_tloss): max_tloss = np.max(results[key]['total_loss'])\n",
    "        # if(max_tloss>np.mean(results[key]['total_loss'])+10*np.std(results[key]['total_loss'])): max_tloss = np.mean(results[key]['total_loss'])+10*np.std(results[key]['total_loss'])\n",
    "        print(key,\"Total_loss\",np.mean(results[key]['total_loss'])+10*np.std(results[key]['total_loss']))\n",
    "        if(np.min(results[key]['radius'])<min_r): min_r = np.min(results[key]['radius'])\n",
    "        if(np.max(results[key]['radius'])>max_r): max_r = np.max(results[key]['radius'])\n",
    "        # if(max_r>np.mean(results[key]['radius'])+10*np.std(results[key]['radius'])): max_r = np.mean(results[key]['radius'])+10*np.std(results[key]['radius'])\n",
    "        print(key,\"radius\",np.mean(results[key]['radius'])+10*np.std(results[key]['radius']))\n",
    "print(min_loss,max_loss)\n",
    "print(min_tloss,max_tloss)\n",
    "print(min_r,max_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e656581-64d7-4361-a342-c5e66fae1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag='vtest'\n",
    "print(\"Plotting the results\")\n",
    "bins_=np.linspace(min_loss,max_loss,100)\n",
    "plt.figure(figsize=(10,10))\n",
    "for key in results.keys():\n",
    "    if(key=='QCD'): plt.hist(results[key]['loss'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "    else: plt.hist(results[key]['loss'],label=key,histtype='step',bins=bins_,density=True)\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Loss distribution')\n",
    "plt.savefig('loss_hist_'+model_type+'_'+tag+'.pdf')\n",
    "# plt.show()\n",
    "\n",
    "if(model_type=='VAE'):\n",
    "\n",
    "    bins_=np.linspace(min_tloss,32673,100)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if(key=='QCD'): plt.hist(results[key]['total_loss'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "        else: plt.hist(results[key]['total_loss'],label=key,histtype='step',bins=bins_,density=True)\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Total Loss')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Total Loss distribution')\n",
    "    plt.savefig('total_loss_hist_'+model_type+'_'+tag+'.pdf')\n",
    "\n",
    "    bins_=np.linspace(min_r,10,100)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if(key=='QCD'): plt.hist(results[key]['radius'],label=key,histtype='step',bins=bins_,color='black',linewidth=2,density=True)\n",
    "        else: plt.hist(results[key]['radius'],label=key,histtype='step',bins=bins_,density=True)\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Radius')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Radius distribution')\n",
    "    plt.savefig('radius_hist_'+model_type+'_'+tag+'.pdf')\n",
    "    \n",
    "#     for key in results.keys():\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for i in range(latent_dim):\n",
    "#             plt.hist(results[key]['mean_prediction'][:,i],bins=100,label='mean '+str(i),histtype='step', density=True,range=[-5,5])\n",
    "#         plt.legend(fontsize='x-small')\n",
    "#         plt.xlabel('Loss')\n",
    "#         plt.ylabel('z')\n",
    "#         plt.title(key+' mean Z distribution')\n",
    "#         plt.savefig('mean_z_'+model_type+'_'+key+'_'+tag+'.pdf')\n",
    "#         # plt.show()\n",
    "\n",
    "#     for key in results.keys():\n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         for i in range(latent_dim):\n",
    "#             plt.hist(results[key]['logvar_prediction'][:,i],bins=100,label='logvar '+str(i),histtype='step', density=True,range=[-20,20])\n",
    "#         plt.legend(fontsize='x-small')\n",
    "#         plt.xlabel('Loss')\n",
    "#         plt.ylabel('z')\n",
    "#         plt.title(key+' logvar Z distribution')\n",
    "#         plt.savefig('logvar_z_'+model_type+'_'+key+'_'+tag+'.pdf')\n",
    "#         # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601e25c-267f-46a9-bbb8-a398aa61f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_eff={}\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for key in results.keys():\n",
    "    if key=='QCD': continue\n",
    "    signal_eff[key]={}\n",
    "    true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "    pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "    fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "    signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "    auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    plt.plot(fpr_loss, tpr_loss, label='%s (AUC = %0.2f)' %(key,auc_loss))\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "plt.title('ROC curve '+model_type)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.savefig('roc_curve_'+model_type+'_'+tag+'.pdf')\n",
    "# plt.show()\n",
    "\n",
    "if(model_type=='VAE'):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "\n",
    "        true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        plt.plot(fpr_loss, tpr_loss, label='%s (AUC = %0.2f)' %(key,auc_loss))\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "    plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "    plt.title('Total Loss ROC curve '+model_type)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('roc_curve_'+model_type+'_'+tag+'.pdf')\n",
    "    # plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "\n",
    "        true_label = np.concatenate(( np.ones(results[key]['target'].shape[0]), np.zeros(results['QCD']['prediction'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "        plt.plot(fpr_loss, tpr_loss, label='%s (AUC = %0.2f)' %(key,auc_loss))\n",
    "    plt.legend(fontsize='x-small')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.plot(np.linspace(0, 1),np.linspace(0, 1), '--', color='0.75')\n",
    "    plt.axvline(0.00001, color='red', linestyle='dashed', linewidth=1)\n",
    "    plt.title('Radius ROC curve '+model_type)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.savefig('roc_curve_'+model_type+'_'+tag+'.pdf')\n",
    "    # plt.show()\n",
    "    \n",
    "signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0ef4d-174a-4b0d-8dc9-c2a7653624b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_eff_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fed5c7-8bfd-4916-896b-1b8d12d92c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
