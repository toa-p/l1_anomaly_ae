{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2767e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Tesla P100-PCIE-12GB, compute capability 6.0\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Instal MPL HEP for style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "#from model import build_AE, build_VAE, Sampling\n",
    "from model import Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614e009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/BSM_preprocessed.h5\"\n",
    "events = 500000\n",
    "load_pickle=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "batch_size= 1024\n",
    "n_epochs = 150\n",
    "inner_width = 16\n",
    "outer_width = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9658010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_AE(input_shape,latent_dim):\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = BatchNormalization()(inputArray)\n",
    "    x = Dense(outer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(inner_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # ecoder = LeakyReLU(alpha=0.3)(x)\n",
    "    #decoder\n",
    "    x = Dense(inner_width, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(outer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "    #create autoencoder\n",
    "    autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "    autoencoder.summary()\n",
    "    # ae = AE(autoencoder)\n",
    "    # ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))\n",
    "\n",
    "    return autoencoder\n",
    "    \n",
    "def build_VAE(input_shape, latent_dim):\n",
    "    \n",
    "    #encoder\n",
    "    inputArray = Input(shape=(input_shape))\n",
    "    x = BatchNormalization()(inputArray)\n",
    "    x = Dense(outer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(inner_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "    # Use reparameterization trick to ensure correct gradient\n",
    "    z = Sampling()([mu, logvar])\n",
    "\n",
    "    # Create encoder\n",
    "    encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    #decoder\n",
    "    d_input = Input(shape=(int(latent_dim),), name='decoder_input')\n",
    "    x = Dense(inner_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(outer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.3)(x)\n",
    "    dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "    # Create decoder \n",
    "    decoder = Model(d_input, dec, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    # vae = VAE(encoder, decoder)\n",
    "    # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "    return encoder,decoder\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf4aa0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 12:58:25.485628: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 12:58:26.296150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11303 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           3712        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           1040        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16)          64          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,436\n",
      "Trainable params: 5,162\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,193\n",
      "Trainable params: 5,033\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 8:26 - loss: 4.4478 - reconstruction_loss: 0.6587 - kl_loss: 3.7890Batch 3: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6263 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           3712        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           1040        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16)          64          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,436\n",
      "Trainable params: 5,162\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,193\n",
      "Trainable params: 5,033\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Instal MPL HEP for style formating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           3712        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64)          256         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           1040        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16)          64          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,436\n",
      "Trainable params: 5,162\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,193\n",
      "Trainable params: 5,033\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           3712        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 64)           0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           1040        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16)          64          ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,504\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "196/196 [==============================] - 4s 9ms/step - loss: 0.8576 - reconstruction_loss: 0.3597 - kl_loss: 0.2309 - val_loss: 0.3792 - val_reconstruction_loss: 0.2995 - val_kl_loss: 0.0797 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 1s 8ms/step - loss: 0.3409 - reconstruction_loss: 0.2930 - kl_loss: 0.0364 - val_loss: 0.3175 - val_reconstruction_loss: 0.2930 - val_kl_loss: 0.0245 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.3077 - reconstruction_loss: 0.2890 - kl_loss: 0.0158 - val_loss: 0.3033 - val_reconstruction_loss: 0.2899 - val_kl_loss: 0.0135 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2988 - reconstruction_loss: 0.2872 - kl_loss: 0.0095 - val_loss: 0.2973 - val_reconstruction_loss: 0.2884 - val_kl_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2939 - reconstruction_loss: 0.2860 - kl_loss: 0.0066 - val_loss: 0.2939 - val_reconstruction_loss: 0.2876 - val_kl_loss: 0.0063 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2907 - reconstruction_loss: 0.2855 - kl_loss: 0.0048 - val_loss: 0.2917 - val_reconstruction_loss: 0.2870 - val_kl_loss: 0.0047 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2887 - reconstruction_loss: 0.2849 - kl_loss: 0.0037 - val_loss: 0.2902 - val_reconstruction_loss: 0.2865 - val_kl_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2873 - reconstruction_loss: 0.2846 - kl_loss: 0.0030 - val_loss: 0.2895 - val_reconstruction_loss: 0.2864 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2875 - reconstruction_loss: 0.2843 - kl_loss: 0.0024 - val_loss: 0.2888 - val_reconstruction_loss: 0.2862 - val_kl_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2853 - reconstruction_loss: 0.2842 - kl_loss: 0.0020 - val_loss: 0.2884 - val_reconstruction_loss: 0.2863 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2848 - reconstruction_loss: 0.2841 - kl_loss: 0.0017 - val_loss: 0.2880 - val_reconstruction_loss: 0.2857 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2848 - reconstruction_loss: 0.2838 - kl_loss: 0.0016 - val_loss: 0.2881 - val_reconstruction_loss: 0.2861 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2852 - reconstruction_loss: 0.2838 - kl_loss: 0.0014 - val_loss: 0.2877 - val_reconstruction_loss: 0.2861 - val_kl_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2846 - reconstruction_loss: 0.2837 - kl_loss: 0.0013 - val_loss: 0.2874 - val_reconstruction_loss: 0.2860 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2841 - reconstruction_loss: 0.2837 - kl_loss: 0.0011 - val_loss: 0.2872 - val_reconstruction_loss: 0.2858 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2853 - reconstruction_loss: 0.2834 - kl_loss: 0.0011 - val_loss: 0.2868 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2847 - reconstruction_loss: 0.2835 - kl_loss: 9.1925e-04 - val_loss: 0.2869 - val_reconstruction_loss: 0.2857 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2847 - reconstruction_loss: 0.2834 - kl_loss: 8.7792e-04 - val_loss: 0.2859 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2836 - reconstruction_loss: 0.2833 - kl_loss: 8.6532e-04 - val_loss: 0.2868 - val_reconstruction_loss: 0.2856 - val_kl_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2842 - reconstruction_loss: 0.2833 - kl_loss: 8.6525e-04\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2842 - reconstruction_loss: 0.2833 - kl_loss: 8.6469e-04 - val_loss: 0.2866 - val_reconstruction_loss: 0.2856 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2858 - reconstruction_loss: 0.2831 - kl_loss: 7.4025e-04 - val_loss: 0.2862 - val_reconstruction_loss: 0.2851 - val_kl_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2840 - reconstruction_loss: 0.2831 - kl_loss: 7.8591e-04 - val_loss: 0.2863 - val_reconstruction_loss: 0.2851 - val_kl_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2834 - reconstruction_loss: 0.2830 - kl_loss: 7.8105e-04\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2834 - reconstruction_loss: 0.2830 - kl_loss: 7.8088e-04 - val_loss: 0.2860 - val_reconstruction_loss: 0.2849 - val_kl_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2836 - reconstruction_loss: 0.2830 - kl_loss: 8.0702e-04 - val_loss: 0.2864 - val_reconstruction_loss: 0.2852 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2877 - reconstruction_loss: 0.2830 - kl_loss: 8.0644e-04 - val_loss: 0.2864 - val_reconstruction_loss: 0.2852 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "190/196 [============================>.] - ETA: 0s - loss: 0.2828 - reconstruction_loss: 0.2832 - kl_loss: 7.9835e-04\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2828 - reconstruction_loss: 0.2831 - kl_loss: 7.9892e-04 - val_loss: 0.2864 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0011 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2834 - reconstruction_loss: 0.2829 - kl_loss: 8.0279e-04 - val_loss: 0.2864 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2844 - reconstruction_loss: 0.2829 - kl_loss: 8.0360e-04Restoring model weights from the end of the best epoch: 18.\n",
      "196/196 [==============================] - 1s 7ms/step - loss: 0.2844 - reconstruction_loss: 0.2829 - kl_loss: 8.0360e-04 - val_loss: 0.2862 - val_reconstruction_loss: 0.2849 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 00028: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           3712        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 64)           0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           1040        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16)          64          ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,504\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           3712        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 64)           0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           1040        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16)          64          ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,504\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           3712        ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64)          256         ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 64)           0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 16)           1040        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16)          64          ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 16)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,572\n",
      "Trainable params: 5,298\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,257\n",
      "Trainable params: 5,097\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 2ms/step - loss: inf - reconstruction_loss: 0.3766 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           3712        ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64)          256         ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 64)           0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 16)           1040        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16)          64          ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 16)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,572\n",
      "Trainable params: 5,298\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,257\n",
      "Trainable params: 5,097\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           3712        ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64)          256         ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 64)           0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 16)           1040        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16)          64          ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 16)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,572\n",
      "Trainable params: 5,298\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,257\n",
      "Trainable params: 5,097\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n"
     ]
    }
   ],
   "source": [
    "test_latent_dim = [4,6,8]\n",
    "result_auc = []\n",
    "result_eff = []\n",
    "\n",
    "for latent_dim in test_latent_dim:\n",
    "    \n",
    "    if(load_pickle):\n",
    "        if(input_pickle==''):\n",
    "            print('Please provide input pickle files')\n",
    "        with open(input_pickle, 'rb') as f:\n",
    "            X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "            bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "                  'VectorZPrimeToQQ__M100',\n",
    "                  'VectorZPrimeToQQ__M200',\n",
    "                  'VBF_HToInvisible_M125',\n",
    "                  'VBF_HToInvisible_M125_private',\n",
    "                  'ZprimeToZH_MZprime1000',\n",
    "                  'ZprimeToZH_MZprime800',\n",
    "                  'ZprimeToZH_MZprime600',\n",
    "                  'GluGluToHHTo4B',\n",
    "                  'HTo2LongLivedTo4mu_1000',\n",
    "                  'HTo2LongLivedTo4mu_125_12',\n",
    "                  'HTo2LongLivedTo4mu_125_25',\n",
    "                  'HTo2LongLivedTo4mu_125_50',\n",
    "                  'VBFHToTauTau',\n",
    "                  'VBF_HH']\n",
    "    else:\n",
    "        if(input_qcd==''or input_bsm==''):\n",
    "            print('Please provide input H5 files')\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, output_pfile,True)\n",
    "        \n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1], latent_dim)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1], latent_dim)\n",
    "        model = VAE(encoder, decoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    \n",
    "    #load model\n",
    "    model_dir = output_model_h5.split('.')[0]\n",
    "    if(model_type=='AE'):\n",
    "        with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "        ae = tf.keras.models.model_from_json(config)    \n",
    "        ae.load_weights(model_dir+\"/model.h5\")\n",
    "        ae.summary()\n",
    "        model = AE(ae)\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "        encoder.summary()\n",
    "        decoder.summary()\n",
    "        model = VAE(encoder, decoder)\n",
    "    \n",
    "    from end2end import get_results\n",
    "    data_file = input_pickle\n",
    "    outdir = output_model_h5.split('.')[0]\n",
    "    if not load_pickle: data_file = output_pfile\n",
    "    results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim)   \n",
    "    \n",
    "    for key in results.keys():\n",
    "        results[key]['loss'] = results[key]['loss'][np.isfinite(results[key]['loss'])]\n",
    "        results[key]['total_loss'] = results[key]['total_loss'][np.isfinite(results[key]['total_loss'])]\n",
    "        results[key]['radius'] = results[key]['radius'][np.isfinite(results[key]['radius'])]\n",
    "\n",
    "    signal_eff={}\n",
    "\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        signal_eff[key]={}\n",
    "        true_label = np.concatenate(( np.ones(results[key]['loss'].shape[0]), np.zeros(results['QCD']['loss'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "\n",
    "    if(model_type=='VAE'):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['total_loss'].shape[0]), np.zeros(results['QCD']['total_loss'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "  \n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['radius'].shape[0]), np.zeros(results['QCD']['radius'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()\n",
    "\n",
    "    result_auc.append(auc_loss)\n",
    "    result_eff.append(tpr_loss[fpr_loss<0.000125][-1])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bb97ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scan results for outer_width = 64 and inner_width = 16 are: \n",
      "\n",
      "   latent_dim  auc_loss  efficiency\n",
      "0           4  0.437736     0.38624\n",
      "1           6  0.621581     0.00000\n",
      "2           8  0.956039     0.92298\n"
     ]
    }
   ],
   "source": [
    "scan_results = {'latent_dim': test_latent_dim,\n",
    "               'auc_loss': result_auc,\n",
    "               'efficiency': result_eff\n",
    "               }\n",
    "\n",
    "scan_evals = pd.DataFrame(scan_results)\n",
    "print('The scan results for outer_width = %d and inner_width = %d are: \\n' %(outer_width, inner_width))\n",
    "print(scan_evals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d673f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
