{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c69881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Tesla P100-PCIE-12GB, compute capability 6.0\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Instal MPL HEP for style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "#from model import build_AE, build_VAE, Sampling\n",
    "from model import Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03179001",
   "metadata": {},
   "outputs": [],
   "source": [
    "####configuration####\n",
    "#global input_qcd, input_bsm, events, load_pickle, input_pickle, output_pfile, \\\n",
    "        #output_model_h5, output_model_json, output_history, output_result, \\\n",
    "        #model_type, batch_size, n_epochs\n",
    "\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/BSM_preprocessed.h5\"\n",
    "events = 500000\n",
    "load_pickle=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "batch_size= 1024\n",
    "n_epochs = 150\n",
    "result_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32339027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hyperparameters):\n",
    "    \n",
    "    latent_dim = hyperparameters[:,0]\n",
    "    outer_layer_width = hyperparameters[:,1]\n",
    "    inner_layer_width = hyperparameters[:,2]\n",
    "    \n",
    "    def build_AE(input_shape,latent_dim, outer_layer_width, inner_layer_width):\n",
    "        inputArray = Input(shape=(input_shape))\n",
    "        x = BatchNormalization()(inputArray)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        # x = BatchNormalization()(x)\n",
    "        # encoder = LeakyReLU(alpha=0.3)(x)\n",
    "        #decoder\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "        #create autoencoder\n",
    "        autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "        autoencoder.summary()\n",
    "        # ae = AE(autoencoder)\n",
    "        # ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "    def build_VAE(input_shape, latent_dim, outer_layer_width, inner_layer_width):\n",
    "    \n",
    "        #encoder\n",
    "        inputArray = Input(shape=(input_shape))\n",
    "        x = BatchNormalization()(inputArray)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Use reparameterization trick to ensure correct gradient\n",
    "        z = Sampling()([mu, logvar])\n",
    "\n",
    "        # Create encoder\n",
    "        encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "        encoder.summary()\n",
    "\n",
    "        #decoder\n",
    "        d_input = Input(shape=(int(latent_dim),), name='decoder_input')\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Create decoder \n",
    "        decoder = Model(d_input, dec, name='decoder')\n",
    "        decoder.summary()\n",
    "    \n",
    "        # vae = VAE(encoder, decoder)\n",
    "        # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        return encoder,decoder\n",
    "    \n",
    "    if(load_pickle):\n",
    "        if(input_pickle==''):\n",
    "            print('Please provide input pickle files')\n",
    "        with open(input_pickle, 'rb') as f:\n",
    "            X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "            bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "                  'VectorZPrimeToQQ__M100',\n",
    "                  'VectorZPrimeToQQ__M200',\n",
    "                  'VBF_HToInvisible_M125',\n",
    "                  'VBF_HToInvisible_M125_private',\n",
    "                  'ZprimeToZH_MZprime1000',\n",
    "                  'ZprimeToZH_MZprime800',\n",
    "                  'ZprimeToZH_MZprime600',\n",
    "                  'GluGluToHHTo4B',\n",
    "                  'HTo2LongLivedTo4mu_1000',\n",
    "                  'HTo2LongLivedTo4mu_125_12',\n",
    "                  'HTo2LongLivedTo4mu_125_25',\n",
    "                  'HTo2LongLivedTo4mu_125_50',\n",
    "                  'VBFHToTauTau',\n",
    "                  'VBF_HH']\n",
    "    else:\n",
    "        if(input_qcd==''or input_bsm==''):\n",
    "            print('Please provide input H5 files')\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, output_pfile,True)\n",
    "        \n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = VAE(encoder, decoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    \n",
    "    #load model\n",
    "    model_dir = output_model_h5.split('.')[0]\n",
    "    if(model_type=='AE'):\n",
    "        with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "        ae = tf.keras.models.model_from_json(config)    \n",
    "        ae.load_weights(model_dir+\"/model.h5\")\n",
    "        ae.summary()\n",
    "        model = AE(ae)\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "        encoder.summary()\n",
    "        decoder.summary()\n",
    "        model = VAE(encoder, decoder)\n",
    "    \n",
    "    from end2end import get_results\n",
    "    data_file = input_pickle\n",
    "    outdir = output_model_h5.split('.')[0]\n",
    "    if not load_pickle: data_file = output_pfile\n",
    "    results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim)   \n",
    "    \n",
    "    for key in results.keys():\n",
    "        results[key]['loss'] = results[key]['loss'][np.isfinite(results[key]['loss'])]\n",
    "        results[key]['total_loss'] = results[key]['total_loss'][np.isfinite(results[key]['total_loss'])]\n",
    "        results[key]['radius'] = results[key]['radius'][np.isfinite(results[key]['radius'])]\n",
    "\n",
    "    signal_eff={}\n",
    "\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        signal_eff[key]={}\n",
    "        true_label = np.concatenate(( np.ones(results[key]['loss'].shape[0]), np.zeros(results['QCD']['loss'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "\n",
    "    if(model_type=='VAE'):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['total_loss'].shape[0]), np.zeros(results['QCD']['total_loss'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "  \n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['radius'].shape[0]), np.zeros(results['QCD']['radius'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()\n",
    "\n",
    "    #print('The auc is, ', auc_loss)\n",
    "    #print('The efficiency is, ', -(tpr_loss[fpr_loss<0.000125][-1]))\n",
    "    result_auc.append(auc_loss)\n",
    "    return -(tpr_loss[fpr_loss<0.000125][-1])\n",
    "    #return auc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb94325c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 57)          228         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 64)           3712        ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 64)          256         ['dense_40[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 64)           0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 64)           4160        ['leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 64)          256         ['dense_41[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 64)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_8 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6459 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 57)          228         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 64)           3712        ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 64)          256         ['dense_40[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 64)           0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 64)           4160        ['leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 64)          256         ['dense_41[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 64)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_8 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 57)          228         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 64)           3712        ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 64)          256         ['dense_40[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 64)           0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 64)           4160        ['leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 64)          256         ['dense_41[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 64)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_8 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 57)          228         ['input_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           928         ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16)          64          ['dense_45[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 16)           0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 16)           272         ['leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16)          64          ['dense_46[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 16)           0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_9 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,692\n",
      "Trainable params: 1,514\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,449\n",
      "Trainable params: 1,385\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:5 out of the last 12746 calls to <function Model.make_train_function.<locals>.train_function at 0x7f812eaecee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "196/196 [==============================] - 4s 12ms/step - loss: 1.0580 - reconstruction_loss: 0.4234 - kl_loss: 0.3266 - val_loss: 0.4303 - val_reconstruction_loss: 0.3157 - val_kl_loss: 0.1146 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3886 - reconstruction_loss: 0.3029 - kl_loss: 0.0608 - val_loss: 0.3302 - val_reconstruction_loss: 0.2915 - val_kl_loss: 0.0388 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3340 - reconstruction_loss: 0.2932 - kl_loss: 0.0276 - val_loss: 0.3088 - val_reconstruction_loss: 0.2870 - val_kl_loss: 0.0218 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3080 - reconstruction_loss: 0.2907 - kl_loss: 0.0164 - val_loss: 0.2991 - val_reconstruction_loss: 0.2852 - val_kl_loss: 0.0139 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3004 - reconstruction_loss: 0.2895 - kl_loss: 0.0109 - val_loss: 0.2939 - val_reconstruction_loss: 0.2844 - val_kl_loss: 0.0095 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2936 - reconstruction_loss: 0.2889 - kl_loss: 0.0076 - val_loss: 0.2905 - val_reconstruction_loss: 0.2838 - val_kl_loss: 0.0067 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2984 - reconstruction_loss: 0.2884 - kl_loss: 0.0055 - val_loss: 0.2883 - val_reconstruction_loss: 0.2834 - val_kl_loss: 0.0049 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2898 - reconstruction_loss: 0.2881 - kl_loss: 0.0041 - val_loss: 0.2869 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0037 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2949 - reconstruction_loss: 0.2878 - kl_loss: 0.0032 - val_loss: 0.2857 - val_reconstruction_loss: 0.2828 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2949 - reconstruction_loss: 0.2876 - kl_loss: 0.0025 - val_loss: 0.2850 - val_reconstruction_loss: 0.2827 - val_kl_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2890 - reconstruction_loss: 0.2873 - kl_loss: 0.0020 - val_loss: 0.2844 - val_reconstruction_loss: 0.2825 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2932 - reconstruction_loss: 0.2872 - kl_loss: 0.0017 - val_loss: 0.2839 - val_reconstruction_loss: 0.2824 - val_kl_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2907 - reconstruction_loss: 0.2871 - kl_loss: 0.0014 - val_loss: 0.2836 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2864 - reconstruction_loss: 0.2869 - kl_loss: 0.0012 - val_loss: 0.2833 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2930 - reconstruction_loss: 0.2868 - kl_loss: 9.8426e-04 - val_loss: 0.2830 - val_reconstruction_loss: 0.2821 - val_kl_loss: 9.1974e-04 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2878 - reconstruction_loss: 0.2867 - kl_loss: 8.4120e-04 - val_loss: 0.2827 - val_reconstruction_loss: 0.2819 - val_kl_loss: 7.8341e-04 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2878 - reconstruction_loss: 0.2865 - kl_loss: 7.2184e-04 - val_loss: 0.2825 - val_reconstruction_loss: 0.2818 - val_kl_loss: 6.7803e-04 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2883 - reconstruction_loss: 0.2866 - kl_loss: 6.4020e-04 - val_loss: 0.2823 - val_reconstruction_loss: 0.2818 - val_kl_loss: 5.8995e-04 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2858 - reconstruction_loss: 0.2863 - kl_loss: 5.6118e-04 - val_loss: 0.2823 - val_reconstruction_loss: 0.2818 - val_kl_loss: 5.2217e-04 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2904 - reconstruction_loss: 0.2863 - kl_loss: 5.1072e-04 - val_loss: 0.2821 - val_reconstruction_loss: 0.2816 - val_kl_loss: 4.5941e-04 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2851 - reconstruction_loss: 0.2862 - kl_loss: 4.4812e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2816 - val_kl_loss: 4.1683e-04 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2899 - reconstruction_loss: 0.2860 - kl_loss: 4.2772e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2815 - val_kl_loss: 3.8029e-04 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2852 - reconstruction_loss: 0.2860 - kl_loss: 3.9276e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2815 - val_kl_loss: 3.3934e-04 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2842 - reconstruction_loss: 0.2860 - kl_loss: 3.6075e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2815 - val_kl_loss: 3.1705e-04 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2949 - reconstruction_loss: 0.2858 - kl_loss: 3.5795e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2813 - val_kl_loss: 2.9204e-04 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2819 - reconstruction_loss: 0.2925 - kl_loss: 2.7929e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2814 - val_kl_loss: 2.4578e-04 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2833 - reconstruction_loss: 0.2857 - kl_loss: 3.6323e-04 - val_loss: 0.2815 - val_reconstruction_loss: 0.2813 - val_kl_loss: 2.5439e-04 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2900 - reconstruction_loss: 0.2857 - kl_loss: 3.5091e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2813 - val_kl_loss: 2.4320e-04 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2893 - reconstruction_loss: 0.2856 - kl_loss: 3.9059e-04\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2893 - reconstruction_loss: 0.2856 - kl_loss: 3.9059e-04 - val_loss: 0.2815 - val_reconstruction_loss: 0.2812 - val_kl_loss: 2.5491e-04 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2826 - reconstruction_loss: 0.2854 - kl_loss: 2.8408e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2812 - val_kl_loss: 2.1435e-04 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2926 - reconstruction_loss: 0.2862 - kl_loss: 3.0506e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.1878e-04 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "192/196 [============================>.] - ETA: 0s - loss: 0.2820 - reconstruction_loss: 0.2857 - kl_loss: 2.9629e-04\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2821 - reconstruction_loss: 0.2855 - kl_loss: 2.9737e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.0217e-04 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2849 - reconstruction_loss: 0.2855 - kl_loss: 2.9582e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2812 - val_kl_loss: 2.2308e-04 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2865 - reconstruction_loss: 0.2854 - kl_loss: 2.9998e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.3690e-04 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2855 - reconstruction_loss: 0.2855 - kl_loss: 2.9668e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.3920e-04 - lr: 1.0000e-05\n",
      "Epoch 36/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2834 - reconstruction_loss: 0.2856 - kl_loss: 3.0108e-04\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2834 - reconstruction_loss: 0.2857 - kl_loss: 3.0080e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.1595e-04 - lr: 1.0000e-05\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2845 - reconstruction_loss: 0.2854 - kl_loss: 3.0110e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.3048e-04 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2895 - reconstruction_loss: 0.2855 - kl_loss: 2.9827e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.5426e-04 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "192/196 [============================>.] - ETA: 0s - loss: 0.2854 - reconstruction_loss: 0.2855 - kl_loss: 2.9712e-04\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2854 - reconstruction_loss: 0.2855 - kl_loss: 2.9638e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.4885e-04 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2854 - kl_loss: 2.9850e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.2738e-04 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2894 - reconstruction_loss: 0.2854 - kl_loss: 3.0140e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.5501e-04 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2867 - reconstruction_loss: 0.2854 - kl_loss: 2.9800e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.4229e-04 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2848 - reconstruction_loss: 0.2855 - kl_loss: 2.9841e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.2744e-04 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2884 - reconstruction_loss: 0.2854 - kl_loss: 2.9716e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.4189e-04 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2906 - reconstruction_loss: 0.2854 - kl_loss: 3.0539e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.5468e-04 - lr: 1.0000e-06\n",
      "Epoch 46/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2884 - reconstruction_loss: 0.2855 - kl_loss: 2.9936e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.4564e-04 - lr: 1.0000e-06\n",
      "Epoch 47/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2863 - reconstruction_loss: 0.2854 - kl_loss: 3.0482e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.3895e-04 - lr: 1.0000e-06\n",
      "Epoch 48/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2855 - kl_loss: 3.0168e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.1161e-04 - lr: 1.0000e-06\n",
      "Epoch 49/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2855 - kl_loss: 2.9696e-04 - val_loss: 0.2812 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.2173e-04 - lr: 1.0000e-06\n",
      "Epoch 50/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2833 - reconstruction_loss: 0.2855 - kl_loss: 2.9321e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.1095e-04 - lr: 1.0000e-06\n",
      "Epoch 51/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2859 - reconstruction_loss: 0.2855 - kl_loss: 2.9755e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.4027e-04 - lr: 1.0000e-06\n",
      "Epoch 52/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2856 - kl_loss: 2.9714e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.3686e-04 - lr: 1.0000e-06\n",
      "Epoch 53/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2866 - reconstruction_loss: 0.2855 - kl_loss: 3.0126e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.4088e-04 - lr: 1.0000e-06\n",
      "Epoch 54/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2850 - reconstruction_loss: 0.2854 - kl_loss: 2.9961e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.3429e-04 - lr: 1.0000e-06\n",
      "Epoch 55/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2856 - reconstruction_loss: 0.2854 - kl_loss: 3.0265e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.3629e-04 - lr: 1.0000e-06\n",
      "Epoch 56/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2855 - kl_loss: 2.9472e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.1616e-04 - lr: 1.0000e-06\n",
      "Epoch 57/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2918 - reconstruction_loss: 0.2854 - kl_loss: 2.8772e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.4000e-04 - lr: 1.0000e-06\n",
      "Epoch 58/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2837 - reconstruction_loss: 0.2853 - kl_loss: 2.9898e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2811 - val_kl_loss: 2.2747e-04 - lr: 1.0000e-06\n",
      "Epoch 59/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2884 - reconstruction_loss: 0.2855 - kl_loss: 2.9569e-04Restoring model weights from the end of the best epoch: 49.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2883 - reconstruction_loss: 0.2855 - kl_loss: 2.9545e-04 - val_loss: 0.2812 - val_reconstruction_loss: 0.2810 - val_kl_loss: 2.4623e-04 - lr: 1.0000e-06\n",
      "Epoch 00059: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 57)          228         ['input_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           928         ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16)          64          ['dense_45[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 16)           0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 16)           272         ['leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16)          64          ['dense_46[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 16)           0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_9 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,692\n",
      "Trainable params: 1,514\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,449\n",
      "Trainable params: 1,385\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 57)          228         ['input_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           928         ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16)          64          ['dense_45[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 16)           0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 16)           272         ['leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16)          64          ['dense_46[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 16)           0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_9 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,692\n",
      "Trainable params: 1,514\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,449\n",
      "Trainable params: 1,385\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 57)          228         ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 64)           3712        ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 64)          256         ['dense_50[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 64)           0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 16)           1040        ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 16)          64          ['dense_51[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 16)           0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,504\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:24 - loss: 1.3285 - reconstruction_loss: 0.6517 - kl_loss: 0.6768Batch 1: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6037 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 57)          228         ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 64)           3712        ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 64)          256         ['dense_50[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 64)           0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 16)           1040        ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 16)          64          ['dense_51[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 16)           0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,504\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 57)          228         ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 64)           3712        ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 64)          256         ['dense_50[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 64)           0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 16)           1040        ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 16)          64          ['dense_51[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 16)           0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,504\n",
      "Trainable params: 5,230\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,065\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 57)          228         ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 64)           3712        ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 64)          256         ['dense_55[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 64)           0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 16)           1040        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 16)          64          ['dense_56[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 16)           0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,572\n",
      "Trainable params: 5,298\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,257\n",
      "Trainable params: 5,097\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 4s 13ms/step - loss: 0.7892 - reconstruction_loss: 0.3636 - kl_loss: 0.1781 - val_loss: 0.3537 - val_reconstruction_loss: 0.2982 - val_kl_loss: 0.0555 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3274 - reconstruction_loss: 0.2958 - kl_loss: 0.0218 - val_loss: 0.3035 - val_reconstruction_loss: 0.2882 - val_kl_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3031 - reconstruction_loss: 0.2904 - kl_loss: 0.0099 - val_loss: 0.2936 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0083 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2964 - reconstruction_loss: 0.2886 - kl_loss: 0.0062 - val_loss: 0.2896 - val_reconstruction_loss: 0.2844 - val_kl_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2903 - reconstruction_loss: 0.2876 - kl_loss: 0.0042 - val_loss: 0.2876 - val_reconstruction_loss: 0.2834 - val_kl_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2927 - reconstruction_loss: 0.2867 - kl_loss: 0.0035 - val_loss: 0.2862 - val_reconstruction_loss: 0.2834 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2893 - reconstruction_loss: 0.2866 - kl_loss: 0.0028 - val_loss: 0.2855 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2870 - reconstruction_loss: 0.2861 - kl_loss: 0.0024 - val_loss: 0.2851 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2891 - reconstruction_loss: 0.2857 - kl_loss: 0.0020 - val_loss: 0.2848 - val_reconstruction_loss: 0.2826 - val_kl_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2850 - reconstruction_loss: 0.2856 - kl_loss: 0.0019 - val_loss: 0.2847 - val_reconstruction_loss: 0.2826 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2862 - reconstruction_loss: 0.2853 - kl_loss: 0.0018 - val_loss: 0.2841 - val_reconstruction_loss: 0.2826 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2976 - reconstruction_loss: 0.2855 - kl_loss: 0.0015 - val_loss: 0.2837 - val_reconstruction_loss: 0.2824 - val_kl_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2849 - reconstruction_loss: 0.2853 - kl_loss: 0.0014 - val_loss: 0.2839 - val_reconstruction_loss: 0.2824 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2867 - reconstruction_loss: 0.2850 - kl_loss: 0.0016\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2867 - reconstruction_loss: 0.2849 - kl_loss: 0.0016 - val_loss: 0.2837 - val_reconstruction_loss: 0.2826 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2850 - kl_loss: 0.0012 - val_loss: 0.2833 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2883 - reconstruction_loss: 0.2846 - kl_loss: 0.0014 - val_loss: 0.2833 - val_reconstruction_loss: 0.2820 - val_kl_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2880 - reconstruction_loss: 0.2846 - kl_loss: 0.0013\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2879 - reconstruction_loss: 0.2845 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2857 - reconstruction_loss: 0.2846 - kl_loss: 0.0013 - val_loss: 0.2832 - val_reconstruction_loss: 0.2820 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2845 - kl_loss: 0.0013 - val_loss: 0.2831 - val_reconstruction_loss: 0.2819 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2847 - reconstruction_loss: 0.2846 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2855 - reconstruction_loss: 0.2842 - kl_loss: 0.0013\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2855 - reconstruction_loss: 0.2842 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2888 - reconstruction_loss: 0.2847 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2845 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 24/150\n",
      "194/196 [============================>.] - ETA: 0s - loss: 0.2839 - reconstruction_loss: 0.2845 - kl_loss: 0.0013\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2839 - reconstruction_loss: 0.2845 - kl_loss: 0.0013 - val_loss: 0.2832 - val_reconstruction_loss: 0.2820 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2844 - reconstruction_loss: 0.2844 - kl_loss: 0.0013 - val_loss: 0.2833 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2856 - reconstruction_loss: 0.2846 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2864 - reconstruction_loss: 0.2847 - kl_loss: 0.0013 - val_loss: 0.2831 - val_reconstruction_loss: 0.2819 - val_kl_loss: 0.0013 - lr: 1.0000e-06\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2844 - kl_loss: 0.0013 - val_loss: 0.2834 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2846 - reconstruction_loss: 0.2848 - kl_loss: 0.0013Restoring model weights from the end of the best epoch: 19.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2846 - reconstruction_loss: 0.2848 - kl_loss: 0.0013 - val_loss: 0.2835 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 00029: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 57)          228         ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 64)           3712        ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 64)          256         ['dense_55[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 64)           0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 16)           1040        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 16)          64          ['dense_56[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 16)           0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,572\n",
      "Trainable params: 5,298\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,257\n",
      "Trainable params: 5,097\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 57)          228         ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 64)           3712        ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 64)          256         ['dense_55[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 64)           0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 16)           1040        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 16)          64          ['dense_56[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 16)           0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,572\n",
      "Trainable params: 5,298\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,257\n",
      "Trainable params: 5,097\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 57)          228         ['input_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 16)           928         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16)          64          ['dense_60[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 16)           0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 32)           544         ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32)          128         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 32)           0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_12 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,288\n",
      "Trainable params: 2,078\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,913\n",
      "Trainable params: 1,817\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:27 - loss: 3.7119 - reconstruction_loss: 0.6336 - kl_loss: 3.0783Batch 1: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 3ms/step - loss: inf - reconstruction_loss: 0.5992 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 57)          228         ['input_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 16)           928         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16)          64          ['dense_60[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 16)           0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 32)           544         ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32)          128         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 32)           0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_12 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,288\n",
      "Trainable params: 2,078\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,913\n",
      "Trainable params: 1,817\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 57)          228         ['input_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 16)           928         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16)          64          ['dense_60[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 16)           0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 32)           544         ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32)          128         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 32)           0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_12 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,288\n",
      "Trainable params: 2,078\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,913\n",
      "Trainable params: 1,817\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 57)          228         ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 64)           3712        ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 64)          256         ['dense_65[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 64)           0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           4160        ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 64)           0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_13 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,392\n",
      "Trainable params: 9,022\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,825\n",
      "Trainable params: 8,569\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 5s 13ms/step - loss: 0.9209 - reconstruction_loss: 0.3529 - kl_loss: 0.2465 - val_loss: 0.3823 - val_reconstruction_loss: 0.2953 - val_kl_loss: 0.0870 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3586 - reconstruction_loss: 0.2937 - kl_loss: 0.0530 - val_loss: 0.3257 - val_reconstruction_loss: 0.2888 - val_kl_loss: 0.0369 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3189 - reconstruction_loss: 0.2905 - kl_loss: 0.0288 - val_loss: 0.3102 - val_reconstruction_loss: 0.2860 - val_kl_loss: 0.0242 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3101 - reconstruction_loss: 0.2884 - kl_loss: 0.0196 - val_loss: 0.3026 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0174 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3101 - reconstruction_loss: 0.2877 - kl_loss: 0.0145 - val_loss: 0.2976 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0131 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3047 - reconstruction_loss: 0.2869 - kl_loss: 0.0115 - val_loss: 0.2943 - val_reconstruction_loss: 0.2837 - val_kl_loss: 0.0105 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2961 - reconstruction_loss: 0.2866 - kl_loss: 0.0094 - val_loss: 0.2920 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2960 - reconstruction_loss: 0.2863 - kl_loss: 0.0080 - val_loss: 0.2904 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0073 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2977 - reconstruction_loss: 0.2860 - kl_loss: 0.0069 - val_loss: 0.2890 - val_reconstruction_loss: 0.2827 - val_kl_loss: 0.0062 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2892 - reconstruction_loss: 0.2857 - kl_loss: 0.0059 - val_loss: 0.2884 - val_reconstruction_loss: 0.2829 - val_kl_loss: 0.0055 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.2915 - reconstruction_loss: 0.2857 - kl_loss: 0.0053 - val_loss: 0.2875 - val_reconstruction_loss: 0.2827 - val_kl_loss: 0.0048 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2883 - reconstruction_loss: 0.2855 - kl_loss: 0.0047 - val_loss: 0.2866 - val_reconstruction_loss: 0.2824 - val_kl_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2882 - reconstruction_loss: 0.2854 - kl_loss: 0.0042 - val_loss: 0.2864 - val_reconstruction_loss: 0.2824 - val_kl_loss: 0.0040 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.2862 - reconstruction_loss: 0.2855 - kl_loss: 0.0037 - val_loss: 0.2864 - val_reconstruction_loss: 0.2825 - val_kl_loss: 0.0039 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2888 - reconstruction_loss: 0.2854 - kl_loss: 0.0036 - val_loss: 0.2854 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.2864 - reconstruction_loss: 0.2853 - kl_loss: 0.0033 - val_loss: 0.2853 - val_reconstruction_loss: 0.2825 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2863 - reconstruction_loss: 0.2854 - kl_loss: 0.0027 - val_loss: 0.2858 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.2872 - reconstruction_loss: 0.2851 - kl_loss: 0.0030 - val_loss: 0.2847 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2890 - reconstruction_loss: 0.2851 - kl_loss: 0.0027 - val_loss: 0.2843 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2887 - reconstruction_loss: 0.2850 - kl_loss: 0.0024 - val_loss: 0.2844 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2868 - reconstruction_loss: 0.2852 - kl_loss: 0.0023 - val_loss: 0.2842 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 12ms/step - loss: 0.2851 - reconstruction_loss: 0.2850 - kl_loss: 0.0022 - val_loss: 0.2840 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2880 - reconstruction_loss: 0.2848 - kl_loss: 0.0020 - val_loss: 0.2840 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2869 - reconstruction_loss: 0.2849 - kl_loss: 0.0019 - val_loss: 0.2838 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2849 - kl_loss: 0.0017 - val_loss: 0.2853 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2889 - reconstruction_loss: 0.2846 - kl_loss: 0.0020 - val_loss: 0.2836 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2850 - reconstruction_loss: 0.2845 - kl_loss: 0.0017 - val_loss: 0.2836 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2833 - reconstruction_loss: 0.2848 - kl_loss: 0.0014\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2834 - reconstruction_loss: 0.2848 - kl_loss: 0.0014 - val_loss: 0.2846 - val_reconstruction_loss: 0.2820 - val_kl_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2854 - reconstruction_loss: 0.2839 - kl_loss: 0.0022 - val_loss: 0.2831 - val_reconstruction_loss: 0.2814 - val_kl_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2885 - reconstruction_loss: 0.2839 - kl_loss: 0.0018 - val_loss: 0.2834 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2857 - reconstruction_loss: 0.2841 - kl_loss: 0.0016\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2857 - reconstruction_loss: 0.2841 - kl_loss: 0.0016 - val_loss: 0.2832 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2855 - reconstruction_loss: 0.2838 - kl_loss: 0.0016 - val_loss: 0.2833 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2856 - reconstruction_loss: 0.2837 - kl_loss: 0.0016 - val_loss: 0.2833 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2850 - reconstruction_loss: 0.2841 - kl_loss: 0.0016\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2850 - reconstruction_loss: 0.2839 - kl_loss: 0.0016 - val_loss: 0.2831 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2840 - reconstruction_loss: 0.2837 - kl_loss: 0.0016 - val_loss: 0.2830 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2841 - reconstruction_loss: 0.2837 - kl_loss: 0.0016 - val_loss: 0.2831 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2908 - reconstruction_loss: 0.2840 - kl_loss: 0.0016\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2907 - reconstruction_loss: 0.2839 - kl_loss: 0.0016 - val_loss: 0.2832 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0016 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2840 - kl_loss: 0.0016 - val_loss: 0.2832 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2838 - kl_loss: 0.0016 - val_loss: 0.2832 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2838 - kl_loss: 0.0016 - val_loss: 0.2831 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2853 - reconstruction_loss: 0.2838 - kl_loss: 0.0016 - val_loss: 0.2831 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2841 - reconstruction_loss: 0.2838 - kl_loss: 0.0016 - val_loss: 0.2831 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2932 - reconstruction_loss: 0.2837 - kl_loss: 0.0016 - val_loss: 0.2832 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0016 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2868 - reconstruction_loss: 0.2837 - kl_loss: 0.0015 - val_loss: 0.2831 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "191/196 [============================>.] - ETA: 0s - loss: 0.2842 - reconstruction_loss: 0.2841 - kl_loss: 0.0016Restoring model weights from the end of the best epoch: 35.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2842 - reconstruction_loss: 0.2840 - kl_loss: 0.0016 - val_loss: 0.2832 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00045: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 57)          228         ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 64)           3712        ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 64)          256         ['dense_65[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 64)           0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           4160        ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 64)           0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_13 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,392\n",
      "Trainable params: 9,022\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,825\n",
      "Trainable params: 8,569\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 57)          228         ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 64)           3712        ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 64)          256         ['dense_65[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 64)           0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           4160        ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 64)           0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_13 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,392\n",
      "Trainable params: 9,022\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,825\n",
      "Trainable params: 8,569\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 57)          228         ['input_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 16)           928         ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16)          64          ['dense_70[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16)           0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 32)           544         ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 32)          128         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 32)           0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_14 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:36 - loss: 1.5094 - reconstruction_loss: 0.6296 - kl_loss: 0.8798Batch 2: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.5379 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 57)          228         ['input_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 16)           928         ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16)          64          ['dense_70[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16)           0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 32)           544         ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 32)          128         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 32)           0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_14 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 57)          228         ['input_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 16)           928         ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16)          64          ['dense_70[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16)           0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 32)           544         ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 32)          128         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 32)           0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_14 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 57)          228         ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 32)           1856        ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32)          128         ['dense_75[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 32)           0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 16)           528         ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 16)          64          ['dense_76[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 16)           0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,008\n",
      "Trainable params: 2,798\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,729\n",
      "Trainable params: 2,633\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 3ms/step - loss: inf - reconstruction_loss: 0.5491 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 57)          228         ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 32)           1856        ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32)          128         ['dense_75[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 32)           0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 16)           528         ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 16)          64          ['dense_76[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 16)           0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,008\n",
      "Trainable params: 2,798\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,729\n",
      "Trainable params: 2,633\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 57)          228         ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 32)           1856        ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32)          128         ['dense_75[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 32)           0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 16)           528         ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 16)          64          ['dense_76[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 16)           0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,008\n",
      "Trainable params: 2,798\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,729\n",
      "Trainable params: 2,633\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 57)          228         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 16)           928         ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16)          64          ['dense_80[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 16)           0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 64)           1088        ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64)          256         ['dense_81[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 64)           0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_16 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 2ms/step - loss: inf - reconstruction_loss: 0.5728 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 57)          228         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 16)           928         ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16)          64          ['dense_80[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 16)           0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 64)           1088        ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64)          256         ['dense_81[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 64)           0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_16 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 57)          228         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 16)           928         ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16)          64          ['dense_80[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 16)           0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 64)           1088        ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64)          256         ['dense_81[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 64)           0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_16 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 57)          228         ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 64)           3712        ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 64)          256         ['dense_85[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 64)           0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 16)           1040        ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 16)          64          ['dense_86[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 16)           0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_17 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,436\n",
      "Trainable params: 5,162\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,193\n",
      "Trainable params: 5,033\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:5 out of the last 8826 calls to <function Model.make_train_function.<locals>.train_function at 0x7f812eaec280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  1/196 [..............................] - ETA: 6:25 - loss: 1.5429 - reconstruction_loss: 0.6259 - kl_loss: 0.9169Batch 1: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 2ms/step - loss: inf - reconstruction_loss: 0.5625 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 57)          228         ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 64)           3712        ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 64)          256         ['dense_85[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 64)           0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 16)           1040        ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 16)          64          ['dense_86[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 16)           0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_17 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,436\n",
      "Trainable params: 5,162\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,193\n",
      "Trainable params: 5,033\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 57)          228         ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 64)           3712        ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 64)          256         ['dense_85[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 64)           0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 16)           1040        ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 16)          64          ['dense_86[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 16)           0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_17 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,436\n",
      "Trainable params: 5,162\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,193\n",
      "Trainable params: 5,033\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 57)          228         ['input_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 32)           1856        ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 32)          128         ['dense_90[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)     (None, 32)           0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 16)           528         ['leaky_re_lu_72[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 16)          64          ['dense_91[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)     (None, 16)           0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,940\n",
      "Trainable params: 2,730\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,697\n",
      "Trainable params: 2,601\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:35 - loss: 7.5777 - reconstruction_loss: 0.5707 - kl_loss: 7.0069Batch 1: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.5539 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 57)          228         ['input_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 32)           1856        ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 32)          128         ['dense_90[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)     (None, 32)           0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 16)           528         ['leaky_re_lu_72[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 16)          64          ['dense_91[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)     (None, 16)           0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,940\n",
      "Trainable params: 2,730\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,697\n",
      "Trainable params: 2,601\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 57)          228         ['input_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 32)           1856        ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 32)          128         ['dense_90[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)     (None, 32)           0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 16)           528         ['leaky_re_lu_72[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 16)          64          ['dense_91[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)     (None, 16)           0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,940\n",
      "Trainable params: 2,730\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,697\n",
      "Trainable params: 2,601\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 57)          228         ['input_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 16)           928         ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16)          64          ['dense_95[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)     (None, 16)           0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 64)           1088        ['leaky_re_lu_76[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 64)          256         ['dense_96[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)     (None, 64)           0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_19 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,344\n",
      "Trainable params: 3,070\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,777\n",
      "Trainable params: 2,617\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:20 - loss: 2.1378 - reconstruction_loss: 0.6409 - kl_loss: 1.4969Batch 1: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 3ms/step - loss: inf - reconstruction_loss: 0.5537 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 57)          228         ['input_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 16)           928         ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16)          64          ['dense_95[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)     (None, 16)           0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 64)           1088        ['leaky_re_lu_76[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 64)          256         ['dense_96[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)     (None, 64)           0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_19 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,344\n",
      "Trainable params: 3,070\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,777\n",
      "Trainable params: 2,617\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 57)          228         ['input_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 16)           928         ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16)          64          ['dense_95[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)     (None, 16)           0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 64)           1088        ['leaky_re_lu_76[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 64)          256         ['dense_96[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)     (None, 64)           0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_19 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,344\n",
      "Trainable params: 3,070\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,777\n",
      "Trainable params: 2,617\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 57)          228         ['input_21[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 32)           1856        ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 32)          128         ['dense_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)     (None, 32)           0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 32)           1056        ['leaky_re_lu_80[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 32)          128         ['dense_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_81 (LeakyReLU)     (None, 32)           0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_20 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,792\n",
      "Trainable params: 3,550\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,417\n",
      "Trainable params: 3,289\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  6/196 [..............................] - ETA: 1s - loss: 2.8013 - reconstruction_loss: 0.6282 - kl_loss: 1.6348  Batch 8: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6119 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 57)          228         ['input_21[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 32)           1856        ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 32)          128         ['dense_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)     (None, 32)           0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 32)           1056        ['leaky_re_lu_80[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 32)          128         ['dense_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_81 (LeakyReLU)     (None, 32)           0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_20 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,792\n",
      "Trainable params: 3,550\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,417\n",
      "Trainable params: 3,289\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 57)          228         ['input_21[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 32)           1856        ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 32)          128         ['dense_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)     (None, 32)           0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 32)           1056        ['leaky_re_lu_80[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 32)          128         ['dense_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_81 (LeakyReLU)     (None, 32)           0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_20 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,792\n",
      "Trainable params: 3,550\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,417\n",
      "Trainable params: 3,289\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 57)          228         ['input_22[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 32)           1856        ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 32)          128         ['dense_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_84 (LeakyReLU)     (None, 32)           0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 16)           528         ['leaky_re_lu_84[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 16)          64          ['dense_106[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_85 (LeakyReLU)     (None, 16)           0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_21 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,076\n",
      "Trainable params: 2,866\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_86 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_87 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,761\n",
      "Trainable params: 2,665\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 2ms/step - loss: inf - reconstruction_loss: 0.4908 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 57)          228         ['input_22[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 32)           1856        ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 32)          128         ['dense_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_84 (LeakyReLU)     (None, 32)           0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 16)           528         ['leaky_re_lu_84[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 16)          64          ['dense_106[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_85 (LeakyReLU)     (None, 16)           0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_21 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,076\n",
      "Trainable params: 2,866\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_86 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_87 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,761\n",
      "Trainable params: 2,665\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 57)          228         ['input_22[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 32)           1856        ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 32)          128         ['dense_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_84 (LeakyReLU)     (None, 32)           0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 16)           528         ['leaky_re_lu_84[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 16)          64          ['dense_106[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_85 (LeakyReLU)     (None, 16)           0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_21 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,076\n",
      "Trainable params: 2,866\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_86 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 32)                544       \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_87 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,761\n",
      "Trainable params: 2,665\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 57)          228         ['input_23[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 64)           3712        ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 64)          256         ['dense_110[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_88 (LeakyReLU)     (None, 64)           0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " dense_111 (Dense)              (None, 32)           2080        ['leaky_re_lu_88[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 32)          128         ['dense_111[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_89 (LeakyReLU)     (None, 32)           0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,800\n",
      "Trainable params: 6,494\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_90 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_91 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,425\n",
      "Trainable params: 6,233\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:33 - loss: 1.9993 - reconstruction_loss: 0.5839 - kl_loss: 1.4154Batch 4: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6059 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 57)          228         ['input_23[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 64)           3712        ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 64)          256         ['dense_110[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_88 (LeakyReLU)     (None, 64)           0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " dense_111 (Dense)              (None, 32)           2080        ['leaky_re_lu_88[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 32)          128         ['dense_111[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_89 (LeakyReLU)     (None, 32)           0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,800\n",
      "Trainable params: 6,494\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_90 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_91 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,425\n",
      "Trainable params: 6,233\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 57)          228         ['input_23[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 64)           3712        ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 64)          256         ['dense_110[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_88 (LeakyReLU)     (None, 64)           0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " dense_111 (Dense)              (None, 32)           2080        ['leaky_re_lu_88[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 32)          128         ['dense_111[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_89 (LeakyReLU)     (None, 32)           0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            198         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            198         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,800\n",
      "Trainable params: 6,494\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 32)                224       \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_90 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_91 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,425\n",
      "Trainable params: 6,233\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 57)          228         ['input_24[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 64)           3712        ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 64)          256         ['dense_115[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_92 (LeakyReLU)     (None, 64)           0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " dense_116 (Dense)              (None, 32)           2080        ['leaky_re_lu_92[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 32)          128         ['dense_116[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_93 (LeakyReLU)     (None, 32)           0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_23 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,932\n",
      "Trainable params: 6,626\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_118 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_94 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_119 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_95 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,489\n",
      "Trainable params: 6,297\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:51 - loss: 5.2215 - reconstruction_loss: 0.5770 - kl_loss: 4.6445Batch 2: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6145 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 57)          228         ['input_24[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 64)           3712        ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 64)          256         ['dense_115[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_92 (LeakyReLU)     (None, 64)           0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " dense_116 (Dense)              (None, 32)           2080        ['leaky_re_lu_92[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 32)          128         ['dense_116[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_93 (LeakyReLU)     (None, 32)           0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_23 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,932\n",
      "Trainable params: 6,626\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_118 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_94 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_119 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_95 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,489\n",
      "Trainable params: 6,297\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 57)          228         ['input_24[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 64)           3712        ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 64)          256         ['dense_115[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_92 (LeakyReLU)     (None, 64)           0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " dense_116 (Dense)              (None, 32)           2080        ['leaky_re_lu_92[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 32)          128         ['dense_116[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_93 (LeakyReLU)     (None, 32)           0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_23 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,932\n",
      "Trainable params: 6,626\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_118 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_94 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_119 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_95 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,489\n",
      "Trainable params: 6,297\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 57)          228         ['input_25[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 32)           1856        ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 32)          128         ['dense_120[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_96 (LeakyReLU)     (None, 32)           0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 64)           2112        ['leaky_re_lu_96[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 64)          256         ['dense_121[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_97 (LeakyReLU)     (None, 64)           0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,100\n",
      "Trainable params: 4,794\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_98 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_99 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,665\n",
      "Trainable params: 4,473\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 2ms/step - loss: inf - reconstruction_loss: 0.4940 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 57)          228         ['input_25[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 32)           1856        ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 32)          128         ['dense_120[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_96 (LeakyReLU)     (None, 32)           0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 64)           2112        ['leaky_re_lu_96[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 64)          256         ['dense_121[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_97 (LeakyReLU)     (None, 64)           0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,100\n",
      "Trainable params: 4,794\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_98 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_99 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,665\n",
      "Trainable params: 4,473\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 57)          228         ['input_25[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 32)           1856        ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 32)          128         ['dense_120[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_96 (LeakyReLU)     (None, 32)           0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 64)           2112        ['leaky_re_lu_96[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 64)          256         ['dense_121[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_97 (LeakyReLU)     (None, 64)           0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,100\n",
      "Trainable params: 4,794\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_98 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_99 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,665\n",
      "Trainable params: 4,473\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 57)          228         ['input_26[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 32)           1856        ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 32)          128         ['dense_125[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_100 (LeakyReLU)    (None, 32)           0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 32)           1056        ['leaky_re_lu_100[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 32)          128         ['dense_126[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_101 (LeakyReLU)    (None, 32)           0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_25 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,924\n",
      "Trainable params: 3,682\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_102 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_103 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,481\n",
      "Trainable params: 3,353\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 2ms/step - loss: inf - reconstruction_loss: 0.5564 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 57)          228         ['input_26[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 32)           1856        ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 32)          128         ['dense_125[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_100 (LeakyReLU)    (None, 32)           0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 32)           1056        ['leaky_re_lu_100[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 32)          128         ['dense_126[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_101 (LeakyReLU)    (None, 32)           0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_25 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,924\n",
      "Trainable params: 3,682\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_102 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_103 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,481\n",
      "Trainable params: 3,353\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 57)          228         ['input_26[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 32)           1856        ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 32)          128         ['dense_125[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_100 (LeakyReLU)    (None, 32)           0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 32)           1056        ['leaky_re_lu_100[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 32)          128         ['dense_126[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_101 (LeakyReLU)    (None, 32)           0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_25 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,924\n",
      "Trainable params: 3,682\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_102 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_103 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,481\n",
      "Trainable params: 3,353\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 57)          228         ['input_27[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 32)           1856        ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 32)          128         ['dense_130[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_104 (LeakyReLU)    (None, 32)           0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 64)           2112        ['leaky_re_lu_104[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 64)          256         ['dense_131[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_105 (LeakyReLU)    (None, 64)           0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_26 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,360\n",
      "Trainable params: 5,054\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_106 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_107 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,793\n",
      "Trainable params: 4,601\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:29 - loss: 1.4025 - reconstruction_loss: 0.6544 - kl_loss: 0.7481Batch 2: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 3ms/step - loss: inf - reconstruction_loss: 0.8607 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 57)          228         ['input_27[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 32)           1856        ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 32)          128         ['dense_130[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_104 (LeakyReLU)    (None, 32)           0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 64)           2112        ['leaky_re_lu_104[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 64)          256         ['dense_131[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_105 (LeakyReLU)    (None, 64)           0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_26 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,360\n",
      "Trainable params: 5,054\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_106 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_107 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,793\n",
      "Trainable params: 4,601\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 57)          228         ['input_27[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 32)           1856        ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 32)          128         ['dense_130[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_104 (LeakyReLU)    (None, 32)           0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 64)           2112        ['leaky_re_lu_104[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 64)          256         ['dense_131[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_105 (LeakyReLU)    (None, 64)           0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            390         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            390         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_26 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,360\n",
      "Trainable params: 5,054\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 64)                448       \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_106 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_107 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,793\n",
      "Trainable params: 4,601\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 57)          228         ['input_28[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 32)           1856        ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 32)          128         ['dense_135[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_108 (LeakyReLU)    (None, 32)           0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 64)           2112        ['leaky_re_lu_108[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 64)          256         ['dense_136[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_109 (LeakyReLU)    (None, 64)           0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_27 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,620\n",
      "Trainable params: 5,314\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,921\n",
      "Trainable params: 4,729\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 3ms/step - loss: inf - reconstruction_loss: 0.3516 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 57)          228         ['input_28[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 32)           1856        ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 32)          128         ['dense_135[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_108 (LeakyReLU)    (None, 32)           0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 64)           2112        ['leaky_re_lu_108[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 64)          256         ['dense_136[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_109 (LeakyReLU)    (None, 64)           0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_27 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,620\n",
      "Trainable params: 5,314\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,921\n",
      "Trainable params: 4,729\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 57)          228         ['input_28[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 32)           1856        ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 32)          128         ['dense_135[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_108 (LeakyReLU)    (None, 32)           0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 64)           2112        ['leaky_re_lu_108[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 64)          256         ['dense_136[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_109 (LeakyReLU)    (None, 64)           0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_27 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,620\n",
      "Trainable params: 5,314\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,921\n",
      "Trainable params: 4,729\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 57)          228         ['input_29[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 16)           928         ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 16)          64          ['dense_140[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_112 (LeakyReLU)    (None, 16)           0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 32)           544         ['leaky_re_lu_112[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 32)          128         ['dense_141[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_113 (LeakyReLU)    (None, 32)           0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_28 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,156\n",
      "Trainable params: 1,946\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_143 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_114 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_144 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_115 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,849\n",
      "Trainable params: 1,753\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 4s 12ms/step - loss: 1.0103 - reconstruction_loss: 0.4005 - kl_loss: 0.2841 - val_loss: 0.4028 - val_reconstruction_loss: 0.3033 - val_kl_loss: 0.0995 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3691 - reconstruction_loss: 0.2935 - kl_loss: 0.0610 - val_loss: 0.3289 - val_reconstruction_loss: 0.2881 - val_kl_loss: 0.0408 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3261 - reconstruction_loss: 0.2883 - kl_loss: 0.0320 - val_loss: 0.3107 - val_reconstruction_loss: 0.2854 - val_kl_loss: 0.0254 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3092 - reconstruction_loss: 0.2865 - kl_loss: 0.0206 - val_loss: 0.3017 - val_reconstruction_loss: 0.2842 - val_kl_loss: 0.0175 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3026 - reconstruction_loss: 0.2855 - kl_loss: 0.0147 - val_loss: 0.2964 - val_reconstruction_loss: 0.2834 - val_kl_loss: 0.0130 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2967 - reconstruction_loss: 0.2850 - kl_loss: 0.0111 - val_loss: 0.2928 - val_reconstruction_loss: 0.2828 - val_kl_loss: 0.0100 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2936 - reconstruction_loss: 0.2846 - kl_loss: 0.0088 - val_loss: 0.2904 - val_reconstruction_loss: 0.2825 - val_kl_loss: 0.0079 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2927 - reconstruction_loss: 0.2843 - kl_loss: 0.0070 - val_loss: 0.2885 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0065 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2898 - reconstruction_loss: 0.2841 - kl_loss: 0.0058 - val_loss: 0.2874 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0053 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2894 - reconstruction_loss: 0.2839 - kl_loss: 0.0049 - val_loss: 0.2864 - val_reconstruction_loss: 0.2819 - val_kl_loss: 0.0045 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2884 - reconstruction_loss: 0.2838 - kl_loss: 0.0042 - val_loss: 0.2855 - val_reconstruction_loss: 0.2817 - val_kl_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2893 - reconstruction_loss: 0.2837 - kl_loss: 0.0036 - val_loss: 0.2849 - val_reconstruction_loss: 0.2815 - val_kl_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2859 - reconstruction_loss: 0.2835 - kl_loss: 0.0031 - val_loss: 0.2844 - val_reconstruction_loss: 0.2816 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2865 - reconstruction_loss: 0.2834 - kl_loss: 0.0027 - val_loss: 0.2840 - val_reconstruction_loss: 0.2815 - val_kl_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2834 - kl_loss: 0.0024 - val_loss: 0.2835 - val_reconstruction_loss: 0.2813 - val_kl_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2832 - kl_loss: 0.0021 - val_loss: 0.2832 - val_reconstruction_loss: 0.2812 - val_kl_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2859 - reconstruction_loss: 0.2832 - kl_loss: 0.0019 - val_loss: 0.2830 - val_reconstruction_loss: 0.2811 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2866 - reconstruction_loss: 0.2831 - kl_loss: 0.0017 - val_loss: 0.2828 - val_reconstruction_loss: 0.2813 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2841 - reconstruction_loss: 0.2831 - kl_loss: 0.0015 - val_loss: 0.2825 - val_reconstruction_loss: 0.2811 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2830 - kl_loss: 0.0013 - val_loss: 0.2824 - val_reconstruction_loss: 0.2812 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2857 - reconstruction_loss: 0.2831 - kl_loss: 0.0012 - val_loss: 0.2823 - val_reconstruction_loss: 0.2812 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2831 - kl_loss: 0.0011 - val_loss: 0.2820 - val_reconstruction_loss: 0.2810 - val_kl_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2848 - reconstruction_loss: 0.2830 - kl_loss: 0.0010 - val_loss: 0.2821 - val_reconstruction_loss: 0.2813 - val_kl_loss: 8.7539e-04 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2831 - reconstruction_loss: 0.2830 - kl_loss: 8.6580e-04\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2830 - kl_loss: 8.6580e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2811 - val_kl_loss: 9.5069e-04 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2845 - reconstruction_loss: 0.2829 - kl_loss: 9.2254e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2809 - val_kl_loss: 8.5402e-04 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2835 - reconstruction_loss: 0.2829 - kl_loss: 8.5490e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2810 - val_kl_loss: 8.1269e-04 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2838 - reconstruction_loss: 0.2829 - kl_loss: 8.2349e-04\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2829 - kl_loss: 8.2349e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2810 - val_kl_loss: 8.0830e-04 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2828 - reconstruction_loss: 0.2828 - kl_loss: 8.2498e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2810 - val_kl_loss: 7.9564e-04 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2845 - reconstruction_loss: 0.2828 - kl_loss: 8.1773e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2808 - val_kl_loss: 8.0772e-04 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2828 - reconstruction_loss: 0.2840 - kl_loss: 8.1129e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2810 - val_kl_loss: 7.5185e-04 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2855 - reconstruction_loss: 0.2830 - kl_loss: 8.2143e-04\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2855 - reconstruction_loss: 0.2828 - kl_loss: 8.2163e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2810 - val_kl_loss: 7.9790e-04 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2823 - reconstruction_loss: 0.2829 - kl_loss: 8.1251e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2810 - val_kl_loss: 7.5962e-04 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2819 - reconstruction_loss: 0.2830 - kl_loss: 8.2001e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2809 - val_kl_loss: 7.5939e-04 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2836 - reconstruction_loss: 0.2829 - kl_loss: 8.1205e-04\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2829 - kl_loss: 8.1205e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2809 - val_kl_loss: 7.6840e-04 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2828 - kl_loss: 8.1688e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2810 - val_kl_loss: 7.8849e-04 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2827 - reconstruction_loss: 0.2828 - kl_loss: 8.1248e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2811 - val_kl_loss: 7.7646e-04 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2841 - reconstruction_loss: 0.2830 - kl_loss: 8.1336e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2809 - val_kl_loss: 8.0456e-04 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2832 - reconstruction_loss: 0.2829 - kl_loss: 8.1362e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2810 - val_kl_loss: 7.8343e-04 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2829 - reconstruction_loss: 0.2829 - kl_loss: 8.1151e-04Restoring model weights from the end of the best epoch: 29.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2829 - kl_loss: 8.1162e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2809 - val_kl_loss: 7.9220e-04 - lr: 1.0000e-06\n",
      "Epoch 00039: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 57)          228         ['input_29[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 16)           928         ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 16)          64          ['dense_140[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_112 (LeakyReLU)    (None, 16)           0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 32)           544         ['leaky_re_lu_112[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 32)          128         ['dense_141[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_113 (LeakyReLU)    (None, 32)           0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_28 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,156\n",
      "Trainable params: 1,946\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_143 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_114 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_144 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_115 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,849\n",
      "Trainable params: 1,753\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 57)          228         ['input_29[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 16)           928         ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 16)          64          ['dense_140[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_112 (LeakyReLU)    (None, 16)           0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 32)           544         ['leaky_re_lu_112[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 32)          128         ['dense_141[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_113 (LeakyReLU)    (None, 32)           0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_28 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,156\n",
      "Trainable params: 1,946\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_143 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_114 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_144 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_115 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,849\n",
      "Trainable params: 1,753\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 57)          228         ['input_30[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 32)           1856        ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 32)          128         ['dense_145[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_116 (LeakyReLU)    (None, 32)           0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 32)           1056        ['leaky_re_lu_116[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 32)          128         ['dense_146[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_117 (LeakyReLU)    (None, 32)           0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,660\n",
      "Trainable params: 3,418\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_118 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_119 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,353\n",
      "Trainable params: 3,225\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 2ms/step - loss: inf - reconstruction_loss: 0.5567 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 57)          228         ['input_30[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 32)           1856        ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 32)          128         ['dense_145[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_116 (LeakyReLU)    (None, 32)           0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 32)           1056        ['leaky_re_lu_116[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 32)          128         ['dense_146[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_117 (LeakyReLU)    (None, 32)           0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,660\n",
      "Trainable params: 3,418\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_118 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_119 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,353\n",
      "Trainable params: 3,225\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 57)          228         ['input_30[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 32)           1856        ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 32)          128         ['dense_145[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_116 (LeakyReLU)    (None, 32)           0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 32)           1056        ['leaky_re_lu_116[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 32)          128         ['dense_146[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_117 (LeakyReLU)    (None, 32)           0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,660\n",
      "Trainable params: 3,418\n",
      "Non-trainable params: 242\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_118 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_119 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,353\n",
      "Trainable params: 3,225\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 57)          228         ['input_31[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 64)           3712        ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 64)          256         ['dense_150[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_120 (LeakyReLU)    (None, 64)           0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           4160        ['leaky_re_lu_120[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 64)          256         ['dense_151[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_121 (LeakyReLU)    (None, 64)           0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_30 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,132\n",
      "Trainable params: 8,762\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,697\n",
      "Trainable params: 8,441\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 4s 13ms/step - loss: 0.6539 - reconstruction_loss: 0.3343 - kl_loss: 0.1164 - val_loss: 0.3274 - val_reconstruction_loss: 0.2922 - val_kl_loss: 0.0352 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3155 - reconstruction_loss: 0.2900 - kl_loss: 0.0210 - val_loss: 0.3013 - val_reconstruction_loss: 0.2868 - val_kl_loss: 0.0145 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2984 - reconstruction_loss: 0.2866 - kl_loss: 0.0112 - val_loss: 0.2930 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0084 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2927 - reconstruction_loss: 0.2850 - kl_loss: 0.0070 - val_loss: 0.2887 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0056 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2887 - reconstruction_loss: 0.2844 - kl_loss: 0.0049 - val_loss: 0.2867 - val_reconstruction_loss: 0.2829 - val_kl_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2878 - reconstruction_loss: 0.2840 - kl_loss: 0.0036 - val_loss: 0.2852 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2860 - reconstruction_loss: 0.2835 - kl_loss: 0.0028 - val_loss: 0.2841 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2857 - reconstruction_loss: 0.2834 - kl_loss: 0.0022 - val_loss: 0.2839 - val_reconstruction_loss: 0.2819 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2849 - reconstruction_loss: 0.2833 - kl_loss: 0.0019 - val_loss: 0.2831 - val_reconstruction_loss: 0.2815 - val_kl_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2855 - reconstruction_loss: 0.2830 - kl_loss: 0.0016 - val_loss: 0.2828 - val_reconstruction_loss: 0.2815 - val_kl_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2839 - reconstruction_loss: 0.2829 - kl_loss: 0.0014 - val_loss: 0.2826 - val_reconstruction_loss: 0.2814 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2828 - kl_loss: 0.0012 - val_loss: 0.2824 - val_reconstruction_loss: 0.2812 - val_kl_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2856 - reconstruction_loss: 0.2827 - kl_loss: 0.0012 - val_loss: 0.2824 - val_reconstruction_loss: 0.2816 - val_kl_loss: 8.0562e-04 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2830 - reconstruction_loss: 0.2828 - kl_loss: 0.0011 - val_loss: 0.2822 - val_reconstruction_loss: 0.2814 - val_kl_loss: 8.2154e-04 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2834 - reconstruction_loss: 0.2825 - kl_loss: 9.8009e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2813 - val_kl_loss: 6.8582e-04 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2827 - reconstruction_loss: 0.2826 - kl_loss: 8.8140e-04 - val_loss: 0.2822 - val_reconstruction_loss: 0.2814 - val_kl_loss: 7.7795e-04 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "194/196 [============================>.] - ETA: 0s - loss: 0.2840 - reconstruction_loss: 0.2825 - kl_loss: 8.1470e-04\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2840 - reconstruction_loss: 0.2825 - kl_loss: 8.1511e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.0150e-04 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2822 - kl_loss: 7.6617e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 7.0095e-04 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2835 - reconstruction_loss: 0.2821 - kl_loss: 7.6538e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 7.0170e-04 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2851 - reconstruction_loss: 0.2821 - kl_loss: 7.4460e-04\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2851 - reconstruction_loss: 0.2821 - kl_loss: 7.4460e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2806 - val_kl_loss: 7.1959e-04 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2819 - kl_loss: 7.1163e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.6352e-04 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2828 - reconstruction_loss: 0.2819 - kl_loss: 6.9497e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.6423e-04 - lr: 1.0000e-05\n",
      "Epoch 23/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2839 - reconstruction_loss: 0.2820 - kl_loss: 7.0005e-04\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2839 - reconstruction_loss: 0.2820 - kl_loss: 7.0132e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.7669e-04 - lr: 1.0000e-05\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2816 - reconstruction_loss: 0.2820 - kl_loss: 7.0488e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.2641e-04 - lr: 1.0000e-06\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2826 - reconstruction_loss: 0.2820 - kl_loss: 7.1767e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.6041e-04 - lr: 1.0000e-06\n",
      "Epoch 26/150\n",
      "192/196 [============================>.] - ETA: 0s - loss: 0.2816 - reconstruction_loss: 0.2820 - kl_loss: 7.0029e-04\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2817 - reconstruction_loss: 0.2820 - kl_loss: 7.0100e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.3656e-04 - lr: 1.0000e-06\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2820 - kl_loss: 7.1619e-04 - val_loss: 0.2812 - val_reconstruction_loss: 0.2806 - val_kl_loss: 6.9174e-04 - lr: 1.0000e-06\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2832 - reconstruction_loss: 0.2820 - kl_loss: 7.1597e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.9352e-04 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2834 - reconstruction_loss: 0.2820 - kl_loss: 7.3019e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2806 - val_kl_loss: 6.9669e-04 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2820 - kl_loss: 7.1472e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.6479e-04 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2821 - kl_loss: 7.1970e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2806 - val_kl_loss: 6.8574e-04 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2816 - reconstruction_loss: 0.2820 - kl_loss: 7.1929e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2808 - val_kl_loss: 6.4978e-04 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2839 - reconstruction_loss: 0.2820 - kl_loss: 7.1626e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.8218e-04 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2837 - reconstruction_loss: 0.2819 - kl_loss: 7.3036e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2807 - val_kl_loss: 7.0378e-04 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2835 - reconstruction_loss: 0.2820 - kl_loss: 7.1271e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.6260e-04 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2821 - kl_loss: 7.1659e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.8214e-04 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2824 - reconstruction_loss: 0.2820 - kl_loss: 7.2561e-04Restoring model weights from the end of the best epoch: 27.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2825 - reconstruction_loss: 0.2819 - kl_loss: 7.2548e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2807 - val_kl_loss: 6.4985e-04 - lr: 1.0000e-06\n",
      "Epoch 00037: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 57)          228         ['input_31[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 64)           3712        ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 64)          256         ['dense_150[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_120 (LeakyReLU)    (None, 64)           0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           4160        ['leaky_re_lu_120[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 64)          256         ['dense_151[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_121 (LeakyReLU)    (None, 64)           0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_30 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,132\n",
      "Trainable params: 8,762\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,697\n",
      "Trainable params: 8,441\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 57)          228         ['input_31[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 64)           3712        ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 64)          256         ['dense_150[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_120 (LeakyReLU)    (None, 64)           0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           4160        ['leaky_re_lu_120[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 64)          256         ['dense_151[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_121 (LeakyReLU)    (None, 64)           0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_30 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,132\n",
      "Trainable params: 8,762\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,697\n",
      "Trainable params: 8,441\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 57)          228         ['input_32[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 64)           3712        ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 64)          256         ['dense_155[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_124 (LeakyReLU)    (None, 64)           0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 32)           2080        ['leaky_re_lu_124[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 32)          128         ['dense_156[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_125 (LeakyReLU)    (None, 32)           0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_31 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,668\n",
      "Trainable params: 6,362\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_158 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_159 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,361\n",
      "Trainable params: 6,169\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "196/196 [==============================] - 4s 12ms/step - loss: 0.9680 - reconstruction_loss: 0.3431 - kl_loss: 0.2738 - val_loss: 0.3802 - val_reconstruction_loss: 0.2909 - val_kl_loss: 0.0894 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3542 - reconstruction_loss: 0.2903 - kl_loss: 0.0525 - val_loss: 0.3206 - val_reconstruction_loss: 0.2864 - val_kl_loss: 0.0343 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3175 - reconstruction_loss: 0.2881 - kl_loss: 0.0261 - val_loss: 0.3046 - val_reconstruction_loss: 0.2848 - val_kl_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3040 - reconstruction_loss: 0.2870 - kl_loss: 0.0161 - val_loss: 0.2974 - val_reconstruction_loss: 0.2842 - val_kl_loss: 0.0132 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3005 - reconstruction_loss: 0.2863 - kl_loss: 0.0111 - val_loss: 0.2926 - val_reconstruction_loss: 0.2833 - val_kl_loss: 0.0094 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2960 - reconstruction_loss: 0.2857 - kl_loss: 0.0082 - val_loss: 0.2901 - val_reconstruction_loss: 0.2830 - val_kl_loss: 0.0071 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2915 - reconstruction_loss: 0.2854 - kl_loss: 0.0064 - val_loss: 0.2882 - val_reconstruction_loss: 0.2826 - val_kl_loss: 0.0056 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2897 - reconstruction_loss: 0.2851 - kl_loss: 0.0052 - val_loss: 0.2869 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0046 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2896 - reconstruction_loss: 0.2849 - kl_loss: 0.0043 - val_loss: 0.2860 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0037 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2879 - reconstruction_loss: 0.2849 - kl_loss: 0.0035 - val_loss: 0.2853 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0032 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2881 - reconstruction_loss: 0.2847 - kl_loss: 0.0030 - val_loss: 0.2848 - val_reconstruction_loss: 0.2822 - val_kl_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2865 - reconstruction_loss: 0.2847 - kl_loss: 0.0026 - val_loss: 0.2843 - val_reconstruction_loss: 0.2820 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2885 - reconstruction_loss: 0.2847 - kl_loss: 0.0022 - val_loss: 0.2839 - val_reconstruction_loss: 0.2820 - val_kl_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2863 - reconstruction_loss: 0.2847 - kl_loss: 0.0019 - val_loss: 0.2838 - val_reconstruction_loss: 0.2821 - val_kl_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2854 - reconstruction_loss: 0.2844 - kl_loss: 0.0017 - val_loss: 0.2834 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2866 - reconstruction_loss: 0.2844 - kl_loss: 0.0015 - val_loss: 0.2832 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2844 - reconstruction_loss: 0.2844 - kl_loss: 0.0014 - val_loss: 0.2829 - val_reconstruction_loss: 0.2818 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2853 - reconstruction_loss: 0.2842 - kl_loss: 0.0013 - val_loss: 0.2829 - val_reconstruction_loss: 0.2819 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2842 - kl_loss: 0.0011 - val_loss: 0.2825 - val_reconstruction_loss: 0.2815 - val_kl_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2849 - reconstruction_loss: 0.2842 - kl_loss: 9.9058e-04 - val_loss: 0.2825 - val_reconstruction_loss: 0.2817 - val_kl_loss: 8.4241e-04 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2850 - reconstruction_loss: 0.2843 - kl_loss: 8.7974e-04\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2850 - reconstruction_loss: 0.2843 - kl_loss: 8.7974e-04 - val_loss: 0.2826 - val_reconstruction_loss: 0.2818 - val_kl_loss: 7.9847e-04 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2848 - reconstruction_loss: 0.2840 - kl_loss: 8.3333e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.5948e-04 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2839 - kl_loss: 7.8364e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.3747e-04 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2833 - reconstruction_loss: 0.2839 - kl_loss: 7.9411e-04\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2833 - reconstruction_loss: 0.2839 - kl_loss: 7.9391e-04 - val_loss: 0.2821 - val_reconstruction_loss: 0.2814 - val_kl_loss: 7.4334e-04 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2850 - reconstruction_loss: 0.2840 - kl_loss: 7.9489e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.5368e-04 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2838 - kl_loss: 7.9142e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.3826e-04 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2841 - reconstruction_loss: 0.2839 - kl_loss: 7.8361e-04\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2841 - reconstruction_loss: 0.2838 - kl_loss: 7.8358e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.3897e-04 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2873 - reconstruction_loss: 0.2839 - kl_loss: 7.9068e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.5786e-04 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2859 - reconstruction_loss: 0.2838 - kl_loss: 7.8703e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.5204e-04 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "194/196 [============================>.] - ETA: 0s - loss: 0.2840 - reconstruction_loss: 0.2839 - kl_loss: 7.8649e-04\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2840 - reconstruction_loss: 0.2838 - kl_loss: 7.8582e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.3727e-04 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2851 - reconstruction_loss: 0.2838 - kl_loss: 7.8530e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.3093e-04 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2845 - reconstruction_loss: 0.2837 - kl_loss: 7.8637e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.3346e-04 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2846 - reconstruction_loss: 0.2838 - kl_loss: 7.8086e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.4285e-04 - lr: 1.0000e-06\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2842 - reconstruction_loss: 0.2838 - kl_loss: 7.8834e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2812 - val_kl_loss: 7.3729e-04 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2827 - reconstruction_loss: 0.2838 - kl_loss: 7.8417e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.2809e-04 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2839 - reconstruction_loss: 0.2838 - kl_loss: 7.8877e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.3471e-04 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2851 - reconstruction_loss: 0.2838 - kl_loss: 7.8180e-04Restoring model weights from the end of the best epoch: 27.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2851 - reconstruction_loss: 0.2838 - kl_loss: 7.8180e-04 - val_loss: 0.2820 - val_reconstruction_loss: 0.2813 - val_kl_loss: 7.4272e-04 - lr: 1.0000e-06\n",
      "Epoch 00037: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 57)          228         ['input_32[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 64)           3712        ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 64)          256         ['dense_155[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_124 (LeakyReLU)    (None, 64)           0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 32)           2080        ['leaky_re_lu_124[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 32)          128         ['dense_156[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_125 (LeakyReLU)    (None, 32)           0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_31 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,668\n",
      "Trainable params: 6,362\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_158 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_159 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,361\n",
      "Trainable params: 6,169\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 57)          228         ['input_32[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 64)           3712        ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 64)          256         ['dense_155[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_124 (LeakyReLU)    (None, 64)           0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 32)           2080        ['leaky_re_lu_124[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 32)          128         ['dense_156[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_125 (LeakyReLU)    (None, 32)           0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            132         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            132         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_31 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,668\n",
      "Trainable params: 6,362\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 32)                160       \n",
      "                                                                 \n",
      " batch_normalization_158 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_159 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,361\n",
      "Trainable params: 6,169\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 57)          228         ['input_33[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 16)           928         ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 16)          64          ['dense_160[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_128 (LeakyReLU)    (None, 16)           0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dense_161 (Dense)              (None, 16)           272         ['leaky_re_lu_128[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 16)          64          ['dense_161[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_129 (LeakyReLU)    (None, 16)           0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_32 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_163 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_130 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_164 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_131 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  6/196 [..............................] - ETA: 2s - loss: 1.6359 - reconstruction_loss: 0.6682 - kl_loss: 1.1551  Batch 10: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6597 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 57)          228         ['input_33[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 16)           928         ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 16)          64          ['dense_160[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_128 (LeakyReLU)    (None, 16)           0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dense_161 (Dense)              (None, 16)           272         ['leaky_re_lu_128[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 16)          64          ['dense_161[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_129 (LeakyReLU)    (None, 16)           0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_32 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_163 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_130 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_164 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_131 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 57)          228         ['input_33[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 16)           928         ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 16)          64          ['dense_160[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_128 (LeakyReLU)    (None, 16)           0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dense_161 (Dense)              (None, 16)           272         ['leaky_re_lu_128[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 16)          64          ['dense_161[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_129 (LeakyReLU)    (None, 16)           0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_32 (Sampling)         (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_163 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_130 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_164 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_131 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 57)          228         ['input_34[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 16)           928         ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 16)          64          ['dense_165[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_132 (LeakyReLU)    (None, 16)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 64)           1088        ['leaky_re_lu_132[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 64)          256         ['dense_166[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_133 (LeakyReLU)    (None, 64)           0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_33 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,084\n",
      "Trainable params: 2,810\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_168 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_134 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,649\n",
      "Trainable params: 2,489\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "196/196 [==============================] - 4s 12ms/step - loss: 0.8516 - reconstruction_loss: 0.3978 - kl_loss: 0.1706 - val_loss: 0.3583 - val_reconstruction_loss: 0.3130 - val_kl_loss: 0.0453 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3328 - reconstruction_loss: 0.2958 - kl_loss: 0.0295 - val_loss: 0.3160 - val_reconstruction_loss: 0.2947 - val_kl_loss: 0.0213 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.3084 - reconstruction_loss: 0.2901 - kl_loss: 0.0168 - val_loss: 0.3045 - val_reconstruction_loss: 0.2910 - val_kl_loss: 0.0134 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2996 - reconstruction_loss: 0.2880 - kl_loss: 0.0110 - val_loss: 0.2988 - val_reconstruction_loss: 0.2895 - val_kl_loss: 0.0093 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2951 - reconstruction_loss: 0.2869 - kl_loss: 0.0078 - val_loss: 0.2953 - val_reconstruction_loss: 0.2884 - val_kl_loss: 0.0069 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2948 - reconstruction_loss: 0.2863 - kl_loss: 0.0059 - val_loss: 0.2931 - val_reconstruction_loss: 0.2877 - val_kl_loss: 0.0053 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2911 - reconstruction_loss: 0.2860 - kl_loss: 0.0045 - val_loss: 0.2915 - val_reconstruction_loss: 0.2873 - val_kl_loss: 0.0041 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2896 - reconstruction_loss: 0.2856 - kl_loss: 0.0036 - val_loss: 0.2903 - val_reconstruction_loss: 0.2869 - val_kl_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2870 - reconstruction_loss: 0.2852 - kl_loss: 0.0029 - val_loss: 0.2892 - val_reconstruction_loss: 0.2861 - val_kl_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2891 - reconstruction_loss: 0.2850 - kl_loss: 0.0025 - val_loss: 0.2886 - val_reconstruction_loss: 0.2861 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2848 - kl_loss: 0.0021 - val_loss: 0.2877 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2859 - reconstruction_loss: 0.2845 - kl_loss: 0.0018 - val_loss: 0.2870 - val_reconstruction_loss: 0.2851 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2852 - reconstruction_loss: 0.2846 - kl_loss: 0.0016 - val_loss: 0.2867 - val_reconstruction_loss: 0.2849 - val_kl_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2862 - reconstruction_loss: 0.2843 - kl_loss: 0.0015 - val_loss: 0.2862 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2845 - reconstruction_loss: 0.2841 - kl_loss: 0.0013 - val_loss: 0.2858 - val_reconstruction_loss: 0.2843 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2865 - reconstruction_loss: 0.2840 - kl_loss: 0.0012 - val_loss: 0.2853 - val_reconstruction_loss: 0.2838 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2844 - reconstruction_loss: 0.2840 - kl_loss: 0.0011 - val_loss: 0.2852 - val_reconstruction_loss: 0.2837 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2861 - reconstruction_loss: 0.2838 - kl_loss: 0.0010 - val_loss: 0.2851 - val_reconstruction_loss: 0.2839 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2854 - reconstruction_loss: 0.2838 - kl_loss: 8.9570e-04 - val_loss: 0.2850 - val_reconstruction_loss: 0.2836 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2848 - reconstruction_loss: 0.2838 - kl_loss: 8.9211e-04 - val_loss: 0.2849 - val_reconstruction_loss: 0.2838 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2846 - reconstruction_loss: 0.2838 - kl_loss: 8.3829e-04 - val_loss: 0.2847 - val_reconstruction_loss: 0.2837 - val_kl_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2833 - reconstruction_loss: 0.2837 - kl_loss: 7.5264e-04 - val_loss: 0.2845 - val_reconstruction_loss: 0.2830 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2836 - kl_loss: 7.5127e-04 - val_loss: 0.2845 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2836 - reconstruction_loss: 0.2836 - kl_loss: 7.3733e-04\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2835 - kl_loss: 7.3509e-04 - val_loss: 0.2845 - val_reconstruction_loss: 0.2836 - val_kl_loss: 8.8152e-04 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2853 - reconstruction_loss: 0.2833 - kl_loss: 6.1579e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2833 - val_kl_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2834 - kl_loss: 6.2355e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2824 - reconstruction_loss: 0.2835 - kl_loss: 6.4610e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2848 - reconstruction_loss: 0.2834 - kl_loss: 6.2440e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "192/196 [============================>.] - ETA: 0s - loss: 0.2835 - reconstruction_loss: 0.2835 - kl_loss: 6.4199e-04\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2835 - reconstruction_loss: 0.2833 - kl_loss: 6.3977e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2832 - reconstruction_loss: 0.2834 - kl_loss: 6.2090e-04 - val_loss: 0.2844 - val_reconstruction_loss: 0.2834 - val_kl_loss: 9.5196e-04 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2838 - reconstruction_loss: 0.2833 - kl_loss: 6.1595e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0010 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "191/196 [============================>.] - ETA: 0s - loss: 0.2831 - reconstruction_loss: 0.2834 - kl_loss: 6.3651e-04\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2833 - kl_loss: 6.3747e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2833 - val_kl_loss: 9.8176e-04 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2831 - reconstruction_loss: 0.2834 - kl_loss: 6.2509e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 9.9971e-04 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2841 - reconstruction_loss: 0.2835 - kl_loss: 6.1411e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2831 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/196 [============================>.] - ETA: 0s - loss: 0.2832 - reconstruction_loss: 0.2835 - kl_loss: 6.1466e-04\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2832 - reconstruction_loss: 0.2834 - kl_loss: 6.1564e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2833 - val_kl_loss: 9.2016e-04 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2835 - kl_loss: 6.2964e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2836 - kl_loss: 6.1979e-04 - val_loss: 0.2841 - val_reconstruction_loss: 0.2831 - val_kl_loss: 9.8502e-04 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2828 - reconstruction_loss: 0.2835 - kl_loss: 6.1063e-04 - val_loss: 0.2840 - val_reconstruction_loss: 0.2831 - val_kl_loss: 9.4849e-04 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2843 - reconstruction_loss: 0.2833 - kl_loss: 6.2481e-04 - val_loss: 0.2841 - val_reconstruction_loss: 0.2830 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2835 - kl_loss: 6.1029e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2834 - val_kl_loss: 9.2960e-04 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2874 - reconstruction_loss: 0.2833 - kl_loss: 6.2498e-04 - val_loss: 0.2841 - val_reconstruction_loss: 0.2830 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2835 - reconstruction_loss: 0.2834 - kl_loss: 6.1925e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0010 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2842 - reconstruction_loss: 0.2833 - kl_loss: 6.1918e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0010 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2857 - reconstruction_loss: 0.2834 - kl_loss: 6.2872e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 9.8685e-04 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2839 - reconstruction_loss: 0.2833 - kl_loss: 6.2124e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 9.7876e-04 - lr: 1.0000e-06\n",
      "Epoch 46/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2829 - reconstruction_loss: 0.2834 - kl_loss: 6.2919e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2833 - val_kl_loss: 9.3097e-04 - lr: 1.0000e-06\n",
      "Epoch 47/150\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2828 - reconstruction_loss: 0.2834 - kl_loss: 6.1112e-04 - val_loss: 0.2843 - val_reconstruction_loss: 0.2834 - val_kl_loss: 8.4854e-04 - lr: 1.0000e-06\n",
      "Epoch 48/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2836 - reconstruction_loss: 0.2834 - kl_loss: 6.1860e-04Restoring model weights from the end of the best epoch: 38.\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.2836 - reconstruction_loss: 0.2834 - kl_loss: 6.1860e-04 - val_loss: 0.2842 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0010 - lr: 1.0000e-06\n",
      "Epoch 00048: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 57)          228         ['input_34[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 16)           928         ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 16)          64          ['dense_165[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_132 (LeakyReLU)    (None, 16)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 64)           1088        ['leaky_re_lu_132[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 64)          256         ['dense_166[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_133 (LeakyReLU)    (None, 64)           0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_33 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,084\n",
      "Trainable params: 2,810\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_168 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_134 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,649\n",
      "Trainable params: 2,489\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 57)          228         ['input_34[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 16)           928         ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 16)          64          ['dense_165[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_132 (LeakyReLU)    (None, 16)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 64)           1088        ['leaky_re_lu_132[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 64)          256         ['dense_166[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_133 (LeakyReLU)    (None, 64)           0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_33 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,084\n",
      "Trainable params: 2,810\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_168 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_134 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,649\n",
      "Trainable params: 2,489\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 57)          228         ['input_35[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 16)           928         ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 16)          64          ['dense_170[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 16)           0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 16)           272         ['leaky_re_lu_136[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 16)          64          ['dense_171[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 16)           0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_34 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,828\n",
      "Trainable params: 1,650\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_173 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_174 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513\n",
      "Trainable params: 1,449\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:44 - loss: 1.6236 - reconstruction_loss: 0.6930 - kl_loss: 0.9306Batch 4: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6600 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 57)          228         ['input_35[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 16)           928         ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 16)          64          ['dense_170[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 16)           0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 16)           272         ['leaky_re_lu_136[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 16)          64          ['dense_171[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 16)           0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_34 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,828\n",
      "Trainable params: 1,650\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_173 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_174 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513\n",
      "Trainable params: 1,449\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 57)          228         ['input_35[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 16)           928         ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 16)          64          ['dense_170[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 16)           0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 16)           272         ['leaky_re_lu_136[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 16)          64          ['dense_171[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 16)           0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            136         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            136         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_34 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,828\n",
      "Trainable params: 1,650\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " batch_normalization_173 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_174 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513\n",
      "Trainable params: 1,449\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 57)          228         ['input_36[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_175 (Dense)              (None, 64)           3712        ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 64)          256         ['dense_175[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_140 (LeakyReLU)    (None, 64)           0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dense_176 (Dense)              (None, 64)           4160        ['leaky_re_lu_140[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 64)          256         ['dense_176[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_141 (LeakyReLU)    (None, 64)           0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_35 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_178 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_142 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_179 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_143 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 2s 2ms/step - loss: inf - reconstruction_loss: 0.3962 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 57)          228         ['input_36[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_175 (Dense)              (None, 64)           3712        ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 64)          256         ['dense_175[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_140 (LeakyReLU)    (None, 64)           0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dense_176 (Dense)              (None, 64)           4160        ['leaky_re_lu_140[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 64)          256         ['dense_176[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_141 (LeakyReLU)    (None, 64)           0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_35 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_178 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_142 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_179 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_143 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 57)          228         ['input_36[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_175 (Dense)              (None, 64)           3712        ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 64)          256         ['dense_175[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_140 (LeakyReLU)    (None, 64)           0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dense_176 (Dense)              (None, 64)           4160        ['leaky_re_lu_140[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 64)          256         ['dense_176[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_141 (LeakyReLU)    (None, 64)           0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_35 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_178 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_142 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_179 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_143 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 57)          228         ['input_37[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_180 (Dense)              (None, 64)           3712        ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 64)          256         ['dense_180[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_144 (LeakyReLU)    (None, 64)           0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dense_181 (Dense)              (None, 64)           4160        ['leaky_re_lu_144[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 64)          256         ['dense_181[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_145 (LeakyReLU)    (None, 64)           0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_36 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_183 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_146 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_184 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_147 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 6:43 - loss: 1.7298 - reconstruction_loss: 0.6444 - kl_loss: 1.0854Batch 2: Invalid loss, terminating training\n",
      "196/196 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 0.6449 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 57)          228         ['input_37[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_180 (Dense)              (None, 64)           3712        ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 64)          256         ['dense_180[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_144 (LeakyReLU)    (None, 64)           0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dense_181 (Dense)              (None, 64)           4160        ['leaky_re_lu_144[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 64)          256         ['dense_181[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_145 (LeakyReLU)    (None, 64)           0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_36 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_183 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_146 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_184 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_147 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 57)          228         ['input_37[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_180 (Dense)              (None, 64)           3712        ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 64)          256         ['dense_180[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_144 (LeakyReLU)    (None, 64)           0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dense_181 (Dense)              (None, 64)           4160        ['leaky_re_lu_144[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 64)          256         ['dense_181[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_145 (LeakyReLU)    (None, 64)           0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_36 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,652\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 370\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_183 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_146 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_184 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_147 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,953\n",
      "Trainable params: 8,697\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "from numpy.random import seed\n",
    "\n",
    "bounds = [{'name': 'latent_dim', 'type': 'discrete', 'domain':(4,6,8)},\n",
    "          {'name': 'outer_layer_width', 'type': 'discrete', 'domain':(16, 32, 64)},\n",
    "          {'name': 'inner_layer_width', 'type': 'discrete', 'domain':(16, 32, 64)}]\n",
    "\n",
    "max_iter = 1000\n",
    "myProblem = GPyOpt.methods.BayesianOptimization(main, domain=bounds)\n",
    "myProblem.run_optimization(max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e453ce66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation results... \n",
      "    latent_dim  outer_layer_width  inner_layer_width  efficiency\n",
      "0          8.0               64.0               64.0     0.98642\n",
      "1          4.0               16.0               16.0     0.00008\n",
      "2          6.0               64.0               16.0     0.77410\n",
      "3          8.0               64.0               16.0     0.00008\n",
      "4          6.0               16.0               32.0     0.94172\n",
      "5          6.0               64.0               64.0     0.00010\n",
      "6          8.0               16.0               32.0     0.53698\n",
      "7          6.0               32.0               16.0     0.89792\n",
      "8          8.0               16.0               64.0     0.65592\n",
      "9          4.0               64.0               16.0     0.45354\n",
      "10         4.0               32.0               16.0     0.66556\n",
      "11         6.0               16.0               64.0     0.47484\n",
      "12         6.0               32.0               32.0     0.52626\n",
      "13         8.0               32.0               16.0     0.78568\n",
      "14         6.0               64.0               32.0     0.42550\n",
      "15         8.0               64.0               32.0     0.74078\n",
      "16         4.0               32.0               64.0     0.68576\n",
      "17         8.0               32.0               32.0     0.93222\n",
      "18         6.0               32.0               64.0     0.46644\n",
      "19         8.0               32.0               64.0     0.93518\n",
      "20         4.0               16.0               32.0     0.00006\n",
      "21         4.0               32.0               32.0     0.43954\n",
      "22         4.0               64.0               64.0     0.00000\n",
      "23         4.0               64.0               32.0     0.00006\n",
      "24         6.0               16.0               16.0     0.48924\n",
      "25         4.0               16.0               64.0     0.00006\n",
      "26         8.0               16.0               16.0     0.17240\n",
      "27         8.0               64.0               64.0     0.93258\n",
      "28         8.0               64.0               64.0     0.62322\n",
      "The value of (latent_dim, outer_layer width, inner_layer_width) that maximizes efficiency is:[ 8. 64. 64.]\n",
      "The the max efficiency found is: 0.98642\n"
     ]
    }
   ],
   "source": [
    "# evaluate results of optimization\n",
    "myProblem.plot_convergence()\n",
    "print('Writing evaluation results... ')\n",
    "param1 = myProblem.get_evaluations()[0][:,0].flatten()\n",
    "param2 = myProblem.get_evaluations()[0][:,1].flatten()\n",
    "param3 = myProblem.get_evaluations()[0][:,2].flatten()\n",
    "out = -1*myProblem.get_evaluations()[1][:].flatten()\n",
    "opt_results = {'latent_dim': param1,\n",
    "               'outer_layer_width': param2,\n",
    "               'inner_layer_width': param3,\n",
    "               'efficiency': out}\n",
    "\n",
    "evals = pd.DataFrame(opt_results)\n",
    "print(evals)\n",
    "\n",
    "print('The value of (latent_dim, outer_layer width, inner_layer_width) that maximizes efficiency is:'+str(myProblem.x_opt))\n",
    "print('The the max efficiency found is: '+str(-1*myProblem.fx_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a97bc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63943710064, 0.79221735708, 0.7945423632799998, 0.60699372368, 0.6891909890000001, 0.58573127104, 0.9335930666799999, 0.50742573496, 0.9929650016, 0.59408587704, 0.88573496032, 0.67252521572, 0.96800662576, 0.62302010072, 0.7683099225600001, 0.93026688004, 0.8305036036799999, 0.65289643344, 0.85312834476, 0.5990349908399999, 0.91303784776, 0.8540216689200001, 0.83105917364, 0.918152828, 0.8296060536800001, 0.95558725168, 0.7519221694, 0.9527656625600002, 0.5682080222, 0.53054660092, 0.65388695444, 0.58090283512, 0.8310504938800001, 0.6156742244800001, 0.8566961126, 0.9569680246400001, 0.7713848444]\n"
     ]
    }
   ],
   "source": [
    "print(result_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7edf30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
