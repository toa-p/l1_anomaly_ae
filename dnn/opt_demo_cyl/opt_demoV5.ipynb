{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9939e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "#from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from functions import make_mse_loss\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "#from model import build_AE, build_VAE, Sampling\n",
    "from model import Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####configuration####\n",
    "global input_qcd, input_bsm, events, load_pickle, input_pickle, output_pfile, \\\n",
    "        output_model_h5, output_model_json, output_history, output_result, \\\n",
    "        model_type, n_epochs\n",
    "\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/BSM_preprocessed.h5\"\n",
    "events = 1000000\n",
    "load_pickle=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "#batch_size= 1024\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hyperparameters):\n",
    "    \n",
    "    latent_dim = hyperparameters[:,0]\n",
    "    outer_layer_width = hyperparameters[:,1]\n",
    "    inner_layer_width = hyperparameters[:,2]\n",
    "    beta = hyperparameters[:,3]\n",
    "    batch_size = int(hyperparameters[:,4])\n",
    "    \n",
    "    class VAE(Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "            self.total_val_loss_tracker = keras.metrics.Mean(name=\"total_val_loss\")\n",
    "            self.reconstruction_val_loss_tracker = keras.metrics.Mean(name=\"reconstruction_val_loss\")\n",
    "            self.kl_val_loss_tracker = keras.metrics.Mean(name=\"kl_val_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "                self.total_val_loss_tracker,\n",
    "                self.reconstruction_val_loss_tracker,\n",
    "                self.kl_val_loss_tracker\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            print('Beta is ', beta)\n",
    "            data_in, target = data\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data_in, training=True)\n",
    "                reconstruction = self.decoder(z, training=True)\n",
    "            \n",
    "                reconstruction_loss = make_mse_loss(target, reconstruction) #one value\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                kl_loss = tf.reduce_mean(kl_loss, axis=-1)\n",
    "                kl_loss = tf.cast(kl_loss, tf.float32)\n",
    "                total_loss = (1-beta)*reconstruction_loss + beta*kl_loss\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state((1-beta)*reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(beta*kl_loss)\n",
    "        \n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "\n",
    "        def test_step(self, data):\n",
    "            #validation\n",
    "            data_in, target = data\n",
    "            z_mean, z_log_var, z = self.encoder(data_in)\n",
    "            reconstruction = self.decoder(z)\n",
    "        \n",
    "            reconstruction_loss = make_mse_loss(target, reconstruction) \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss, axis=1)\n",
    "            kl_loss = tf.cast(kl_loss, tf.float32)\n",
    "            total_loss = (1-beta)*reconstruction_loss + beta*kl_loss\n",
    "            self.total_val_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_val_loss_tracker.update_state((1-beta)*reconstruction_loss)\n",
    "            self.kl_val_loss_tracker.update_state(beta*kl_loss)\n",
    "\n",
    "            return {\n",
    "                \"loss\": self.total_val_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_val_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_val_loss_tracker.result()\n",
    "            }\n",
    "    \n",
    "        def save(self, path):\n",
    "            pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "            print('saving model to {}'.format(path))\n",
    "            self.encoder.save(os.path.join(path, 'encoder.h5'))\n",
    "            self.decoder.save(os.path.join(path,'decoder.h5'))\n",
    "\n",
    "        @classmethod\n",
    "        def load(cls, path, custom_objects={}):\n",
    "            ''' loading only for inference -> passing compile=False '''\n",
    "            encoder = tf.keras.models.load_model(os.path.join(path,'encoder.h5'), custom_objects=custom_objects, compile=False)\n",
    "            decoder = tf.keras.models.load_model(os.path.join(path,'decoder.h5'), custom_objects=custom_objects, compile=False)\n",
    "            return encoder, decoder\n",
    "    \n",
    "        def build_AE(input_shape,latent_dim, outer_layer_width, inner_layer_width):\n",
    "            inputArray = Input(shape=(input_shape))\n",
    "            x = BatchNormalization()(inputArray)\n",
    "            x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            # x = BatchNormalization()(x)\n",
    "            # encoder = LeakyReLU(alpha=0.3)(x)\n",
    "            #decoder\n",
    "            x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "            #create autoencoder\n",
    "            autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "            autoencoder.summary()\n",
    "            # ae = AE(autoencoder)\n",
    "            # ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))\n",
    "\n",
    "            return autoencoder\n",
    "    \n",
    "    def build_VAE(input_shape, latent_dim, outer_layer_width, inner_layer_width):\n",
    "    \n",
    "        #encoder\n",
    "        inputArray = Input(shape=(input_shape))\n",
    "        x = BatchNormalization()(inputArray)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Use reparameterization trick to ensure correct gradient\n",
    "        z = Sampling()([mu, logvar])\n",
    "\n",
    "        # Create encoder\n",
    "        encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "        encoder.summary()\n",
    "\n",
    "        #decoder\n",
    "        d_input = Input(shape=(int(latent_dim),), name='decoder_input')\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Create decoder \n",
    "        decoder = Model(d_input, dec, name='decoder')\n",
    "        decoder.summary()\n",
    "    \n",
    "        # vae = VAE(encoder, decoder)\n",
    "        # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        return encoder,decoder\n",
    "    \n",
    "    if(load_pickle):\n",
    "        if(input_pickle==''):\n",
    "            print('Please provide input pickle files')\n",
    "        with open(input_pickle, 'rb') as f:\n",
    "            X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "            bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "                  'VectorZPrimeToQQ__M100',\n",
    "                  'VectorZPrimeToQQ__M200',\n",
    "                  'VBF_HToInvisible_M125',\n",
    "                  'VBF_HToInvisible_M125_private',\n",
    "                  'ZprimeToZH_MZprime1000',\n",
    "                  'ZprimeToZH_MZprime800',\n",
    "                  'ZprimeToZH_MZprime600',\n",
    "                  'GluGluToHHTo4B',\n",
    "                  'HTo2LongLivedTo4mu_1000',\n",
    "                  'HTo2LongLivedTo4mu_125_12',\n",
    "                  'HTo2LongLivedTo4mu_125_25',\n",
    "                  'HTo2LongLivedTo4mu_125_50',\n",
    "                  'VBFHToTauTau',\n",
    "                  'VBF_HH']\n",
    "    else:\n",
    "        if(input_qcd==''or input_bsm==''):\n",
    "            print('Please provide input H5 files')\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, output_pfile,True)\n",
    "        \n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = VAE(encoder, decoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    \n",
    "    #load model\n",
    "    model_dir = output_model_h5.split('.')[0]\n",
    "    if(model_type=='AE'):\n",
    "        with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "        ae = tf.keras.models.model_from_json(config)    \n",
    "        ae.load_weights(model_dir+\"/model.h5\")\n",
    "        ae.summary()\n",
    "        model = AE(ae)\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "        encoder.summary()\n",
    "        decoder.summary()\n",
    "        model = VAE(encoder, decoder)\n",
    "    \n",
    "    from end2end import get_results\n",
    "    data_file = input_pickle\n",
    "    outdir = output_model_h5.split('.')[0]\n",
    "    if not load_pickle: data_file = output_pfile\n",
    "    results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim)   \n",
    "    \n",
    "    for key in results.keys():\n",
    "        results[key]['loss'] = results[key]['loss'][np.isfinite(results[key]['loss'])]\n",
    "        results[key]['total_loss'] = results[key]['total_loss'][np.isfinite(results[key]['total_loss'])]\n",
    "        results[key]['radius'] = results[key]['radius'][np.isfinite(results[key]['radius'])]\n",
    "\n",
    "    signal_eff={}\n",
    "\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        signal_eff[key]={}\n",
    "        true_label = np.concatenate(( np.ones(results[key]['loss'].shape[0]), np.zeros(results['QCD']['loss'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "\n",
    "    if(model_type=='VAE'):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['total_loss'].shape[0]), np.zeros(results['QCD']['total_loss'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "  \n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['radius'].shape[0]), np.zeros(results['QCD']['radius'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()\n",
    "\n",
    "    #return auc_loss\n",
    "    return -(tpr_loss[fpr_loss<0.000125][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "from numpy.random import seed\n",
    "\n",
    "bounds = [{'name': 'latent_dim', 'type': 'discrete', 'domain':(3, 4, 5, 6, 7, 8)},\n",
    "          {'name': 'outer_layer_width', 'type': 'discrete', 'domain':(16, 32, 64)},\n",
    "          {'name': 'inner_layer_width', 'type': 'discrete', 'domain':(16, 32, 64)},\n",
    "          {'name': 'beta', 'type': 'continuous', 'domain':(0,1)},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain':(1024)}]\n",
    "\n",
    "max_iter = 1000\n",
    "myProblem = GPyOpt.methods.BayesianOptimization(main, domain=bounds)\n",
    "myProblem.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate results of optimization\n",
    "myProblem.plot_convergence()http://127.0.0.1:3244/?token=a81ca675177d942935a0d3476cf95d0923ca4264598f5f93\n",
    "print('Writing evaluation results... ')\n",
    "param1 = myProblem.get_evaluations()[0][:,0].flatten()\n",
    "param2 = myProblem.get_evaluations()[0][:,1].flatten()\n",
    "param3 = myProblem.get_evaluations()[0][:,2].flatten()\n",
    "param4 = myProblem.get_evaluations()[0][:,3].flatten()\n",
    "param5 = myProblem.get_evaluations()[0][:,4].flatten()\n",
    "out = -1*myProblem.get_evaluations()[1][:].flatten()\n",
    "opt_results = {'latent_dim': param1,\n",
    "               'outer_layer_width': param2,\n",
    "               'inner_layer_width': param3,\n",
    "               'beta': param4,\n",
    "               'batch_size': param5\n",
    "               'efficiency': out}\n",
    "\n",
    "evals = pd.DataFrame(opt_results)\n",
    "print(evals)\n",
    "\n",
    "print('The value of (latent_dim, outer_layer width, inner_layer_width, beta) that maximizes efficiency is: \\n'+str(myProblem.x_opt))\n",
    "print('The the max efficiency found is: '+str(-1*myProblem.fx_opt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
