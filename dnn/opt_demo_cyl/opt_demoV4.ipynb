{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7f4107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Tesla P100-PCIE-12GB, compute capability 6.0\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Instal MPL HEP for style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "#from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from functions import make_mse_loss\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "#from model import build_AE, build_VAE, Sampling\n",
    "from model import Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf44f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####configuration####\n",
    "#global input_qcd, input_bsm, events, load_pickle, input_pickle, output_pfile, \\\n",
    "        #output_model_h5, output_model_json, output_history, output_result, \\\n",
    "        #model_type, n_epochs\n",
    "\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/BSM_preprocessed.h5\"\n",
    "events = 1000000\n",
    "load_pickle=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "batch_size= 1024\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d16f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hyperparameters):\n",
    "    \n",
    "    latent_dim = hyperparameters[:,0]\n",
    "    outer_layer_width = hyperparameters[:,1]\n",
    "    inner_layer_width = hyperparameters[:,2]\n",
    "    beta = hyperparameters[:,3]\n",
    "    \n",
    "    class VAE(Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "            self.total_val_loss_tracker = keras.metrics.Mean(name=\"total_val_loss\")\n",
    "            self.reconstruction_val_loss_tracker = keras.metrics.Mean(name=\"reconstruction_val_loss\")\n",
    "            self.kl_val_loss_tracker = keras.metrics.Mean(name=\"kl_val_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "                self.total_val_loss_tracker,\n",
    "                self.reconstruction_val_loss_tracker,\n",
    "                self.kl_val_loss_tracker\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            print('Beta is ', beta)\n",
    "            data_in, target = data\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data_in, training=True)\n",
    "                reconstruction = self.decoder(z, training=True)\n",
    "            \n",
    "                reconstruction_loss = make_mse_loss(target, reconstruction) #one value\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                kl_loss = tf.reduce_mean(kl_loss, axis=-1)\n",
    "                kl_loss = tf.cast(kl_loss, tf.float32)\n",
    "                total_loss = (1-beta)*reconstruction_loss + beta*kl_loss\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state((1-beta)*reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(beta*kl_loss)\n",
    "        \n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "\n",
    "        def test_step(self, data):\n",
    "            #validation\n",
    "            data_in, target = data\n",
    "            z_mean, z_log_var, z = self.encoder(data_in)\n",
    "            reconstruction = self.decoder(z)\n",
    "        \n",
    "            reconstruction_loss = make_mse_loss(target, reconstruction) \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss, axis=1)\n",
    "            kl_loss = tf.cast(kl_loss, tf.float32)\n",
    "            total_loss = (1-beta)*reconstruction_loss + beta*kl_loss\n",
    "            self.total_val_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_val_loss_tracker.update_state((1-beta)*reconstruction_loss)\n",
    "            self.kl_val_loss_tracker.update_state(beta*kl_loss)\n",
    "\n",
    "            return {\n",
    "                \"loss\": self.total_val_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_val_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_val_loss_tracker.result()\n",
    "            }\n",
    "    \n",
    "        def save(self, path):\n",
    "            pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "            print('saving model to {}'.format(path))\n",
    "            self.encoder.save(os.path.join(path, 'encoder.h5'))\n",
    "            self.decoder.save(os.path.join(path,'decoder.h5'))\n",
    "\n",
    "        @classmethod\n",
    "        def load(cls, path, custom_objects={}):\n",
    "            ''' loading only for inference -> passing compile=False '''\n",
    "            encoder = tf.keras.models.load_model(os.path.join(path,'encoder.h5'), custom_objects=custom_objects, compile=False)\n",
    "            decoder = tf.keras.models.load_model(os.path.join(path,'decoder.h5'), custom_objects=custom_objects, compile=False)\n",
    "            return encoder, decoder\n",
    "    \n",
    "        def build_AE(input_shape,latent_dim, outer_layer_width, inner_layer_width):\n",
    "            inputArray = Input(shape=(input_shape))\n",
    "            x = BatchNormalization()(inputArray)\n",
    "            x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            # x = BatchNormalization()(x)\n",
    "            # encoder = LeakyReLU(alpha=0.3)(x)\n",
    "            #decoder\n",
    "            x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = LeakyReLU(alpha=0.3)(x)\n",
    "            decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "            #create autoencoder\n",
    "            autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "            autoencoder.summary()\n",
    "            # ae = AE(autoencoder)\n",
    "            # ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))\n",
    "\n",
    "            return autoencoder\n",
    "    \n",
    "    def build_VAE(input_shape, latent_dim, outer_layer_width, inner_layer_width):\n",
    "    \n",
    "        #encoder\n",
    "        inputArray = Input(shape=(input_shape))\n",
    "        x = BatchNormalization()(inputArray)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Use reparameterization trick to ensure correct gradient\n",
    "        z = Sampling()([mu, logvar])\n",
    "\n",
    "        # Create encoder\n",
    "        encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "        encoder.summary()\n",
    "\n",
    "        #decoder\n",
    "        d_input = Input(shape=(int(latent_dim),), name='decoder_input')\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Create decoder \n",
    "        decoder = Model(d_input, dec, name='decoder')\n",
    "        decoder.summary()\n",
    "    \n",
    "        # vae = VAE(encoder, decoder)\n",
    "        # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        return encoder,decoder\n",
    "    \n",
    "    if(load_pickle):\n",
    "        if(input_pickle==''):\n",
    "            print('Please provide input pickle files')\n",
    "        with open(input_pickle, 'rb') as f:\n",
    "            X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "            bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "                  'VectorZPrimeToQQ__M100',\n",
    "                  'VectorZPrimeToQQ__M200',\n",
    "                  'VBF_HToInvisible_M125',\n",
    "                  'VBF_HToInvisible_M125_private',\n",
    "                  'ZprimeToZH_MZprime1000',\n",
    "                  'ZprimeToZH_MZprime800',\n",
    "                  'ZprimeToZH_MZprime600',\n",
    "                  'GluGluToHHTo4B',\n",
    "                  'HTo2LongLivedTo4mu_1000',\n",
    "                  'HTo2LongLivedTo4mu_125_12',\n",
    "                  'HTo2LongLivedTo4mu_125_25',\n",
    "                  'HTo2LongLivedTo4mu_125_50',\n",
    "                  'VBFHToTauTau',\n",
    "                  'VBF_HH']\n",
    "    else:\n",
    "        if(input_qcd==''or input_bsm==''):\n",
    "            print('Please provide input H5 files')\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, output_pfile,True)\n",
    "        \n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = VAE(encoder, decoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    \n",
    "    #load model\n",
    "    model_dir = output_model_h5.split('.')[0]\n",
    "    if(model_type=='AE'):\n",
    "        with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "        ae = tf.keras.models.model_from_json(config)    \n",
    "        ae.load_weights(model_dir+\"/model.h5\")\n",
    "        ae.summary()\n",
    "        model = AE(ae)\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "        encoder.summary()\n",
    "        decoder.summary()\n",
    "        model = VAE(encoder, decoder)\n",
    "    \n",
    "    from end2end import get_results\n",
    "    data_file = input_pickle\n",
    "    outdir = output_model_h5.split('.')[0]\n",
    "    if not load_pickle: data_file = output_pfile\n",
    "    results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim)   \n",
    "    \n",
    "    for key in results.keys():\n",
    "        results[key]['loss'] = results[key]['loss'][np.isfinite(results[key]['loss'])]\n",
    "        results[key]['total_loss'] = results[key]['total_loss'][np.isfinite(results[key]['total_loss'])]\n",
    "        results[key]['radius'] = results[key]['radius'][np.isfinite(results[key]['radius'])]\n",
    "\n",
    "    signal_eff={}\n",
    "\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        signal_eff[key]={}\n",
    "        true_label = np.concatenate(( np.ones(results[key]['loss'].shape[0]), np.zeros(results['QCD']['loss'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "\n",
    "    if(model_type=='VAE'):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['total_loss'].shape[0]), np.zeros(results['QCD']['total_loss'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "  \n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['radius'].shape[0]), np.zeros(results['QCD']['radius'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()\n",
    "\n",
    "    #return auc_loss\n",
    "    return -(tpr_loss[fpr_loss<0.000125][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c3f550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 23:24:41.199489: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-01 23:24:42.776442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8921 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           1856        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           2112        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64)          256         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,970\n",
      "Trainable params: 4,664\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,409\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.26664312]\n",
      "Beta is  [0.26664312]\n",
      "391/391 [==============================] - 19s 37ms/step - loss: 1.4810 - reconstruction_loss: 1.1453 - kl_loss: 0.0740 - val_loss: 1.0626 - val_reconstruction_loss: 1.0246 - val_kl_loss: 0.0382 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0648 - reconstruction_loss: 1.0206 - kl_loss: 0.0364 - val_loss: 1.0391 - val_reconstruction_loss: 1.0013 - val_kl_loss: 0.0380 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0418 - reconstruction_loss: 1.0031 - kl_loss: 0.0392 - val_loss: 1.0291 - val_reconstruction_loss: 0.9853 - val_kl_loss: 0.0440 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0317 - reconstruction_loss: 0.9885 - kl_loss: 0.0455 - val_loss: 1.0225 - val_reconstruction_loss: 0.9712 - val_kl_loss: 0.0514 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0329 - reconstruction_loss: 0.9784 - kl_loss: 0.0506 - val_loss: 1.0182 - val_reconstruction_loss: 0.9653 - val_kl_loss: 0.0530 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0194 - reconstruction_loss: 0.9726 - kl_loss: 0.0529 - val_loss: 1.0148 - val_reconstruction_loss: 0.9604 - val_kl_loss: 0.0545 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0393 - reconstruction_loss: 0.9679 - kl_loss: 0.0546 - val_loss: 1.0131 - val_reconstruction_loss: 0.9566 - val_kl_loss: 0.0566 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0166 - reconstruction_loss: 0.9642 - kl_loss: 0.0561 - val_loss: 1.0104 - val_reconstruction_loss: 0.9535 - val_kl_loss: 0.0570 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0212 - reconstruction_loss: 0.9612 - kl_loss: 0.0578 - val_loss: 1.0088 - val_reconstruction_loss: 0.9513 - val_kl_loss: 0.0577 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0170 - reconstruction_loss: 0.9585 - kl_loss: 0.0588 - val_loss: 1.0074 - val_reconstruction_loss: 0.9466 - val_kl_loss: 0.0609 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0159 - reconstruction_loss: 0.9554 - kl_loss: 0.0602 - val_loss: 1.0068 - val_reconstruction_loss: 0.9452 - val_kl_loss: 0.0618 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 1.0173 - reconstruction_loss: 0.9540 - kl_loss: 0.0611 - val_loss: 1.0045 - val_reconstruction_loss: 0.9422 - val_kl_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0179 - reconstruction_loss: 0.9516 - kl_loss: 0.0622 - val_loss: 1.0032 - val_reconstruction_loss: 0.9406 - val_kl_loss: 0.0627 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0075 - reconstruction_loss: 0.9502 - kl_loss: 0.0628 - val_loss: 1.0033 - val_reconstruction_loss: 0.9392 - val_kl_loss: 0.0643 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0106 - reconstruction_loss: 0.9480 - kl_loss: 0.0633 - val_loss: 1.0030 - val_reconstruction_loss: 0.9389 - val_kl_loss: 0.0642 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0121 - reconstruction_loss: 0.9469 - kl_loss: 0.0643 - val_loss: 1.0022 - val_reconstruction_loss: 0.9354 - val_kl_loss: 0.0669 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0191 - reconstruction_loss: 0.9460 - kl_loss: 0.0645 - val_loss: 1.0014 - val_reconstruction_loss: 0.9352 - val_kl_loss: 0.0662 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0072 - reconstruction_loss: 0.9447 - kl_loss: 0.0650 - val_loss: 1.0014 - val_reconstruction_loss: 0.9360 - val_kl_loss: 0.0655 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0091 - reconstruction_loss: 0.9446 - kl_loss: 0.0651\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0091 - reconstruction_loss: 0.9446 - kl_loss: 0.0651 - val_loss: 1.0014 - val_reconstruction_loss: 0.9362 - val_kl_loss: 0.0652 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0032 - reconstruction_loss: 0.9424 - kl_loss: 0.0654 - val_loss: 1.0003 - val_reconstruction_loss: 0.9344 - val_kl_loss: 0.0660 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0071 - reconstruction_loss: 0.9418 - kl_loss: 0.0657 - val_loss: 1.0005 - val_reconstruction_loss: 0.9342 - val_kl_loss: 0.0664 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0047 - reconstruction_loss: 0.9414 - kl_loss: 0.0660 - val_loss: 0.9995 - val_reconstruction_loss: 0.9331 - val_kl_loss: 0.0665 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0065 - reconstruction_loss: 0.9411 - kl_loss: 0.0658 - val_loss: 1.0001 - val_reconstruction_loss: 0.9340 - val_kl_loss: 0.0661 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.0239 - reconstruction_loss: 0.9404 - kl_loss: 0.0660\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0238 - reconstruction_loss: 0.9406 - kl_loss: 0.0660 - val_loss: 1.0000 - val_reconstruction_loss: 0.9338 - val_kl_loss: 0.0663 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0076 - reconstruction_loss: 0.9410 - kl_loss: 0.0657 - val_loss: 0.9998 - val_reconstruction_loss: 0.9335 - val_kl_loss: 0.0664 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 1.0029 - reconstruction_loss: 0.9409 - kl_loss: 0.0657 - val_loss: 1.0001 - val_reconstruction_loss: 0.9344 - val_kl_loss: 0.0659 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0028 - reconstruction_loss: 0.9408 - kl_loss: 0.0657\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0028 - reconstruction_loss: 0.9408 - kl_loss: 0.0657 - val_loss: 1.0001 - val_reconstruction_loss: 0.9343 - val_kl_loss: 0.0659 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0045 - reconstruction_loss: 0.9410 - kl_loss: 0.0658 - val_loss: 1.0003 - val_reconstruction_loss: 0.9340 - val_kl_loss: 0.0664 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0026 - reconstruction_loss: 0.9407 - kl_loss: 0.0658 - val_loss: 1.0002 - val_reconstruction_loss: 0.9341 - val_kl_loss: 0.0661 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.0082 - reconstruction_loss: 0.9408 - kl_loss: 0.0658\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0082 - reconstruction_loss: 0.9407 - kl_loss: 0.0658 - val_loss: 1.0006 - val_reconstruction_loss: 0.9345 - val_kl_loss: 0.0662 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0064 - reconstruction_loss: 0.9410 - kl_loss: 0.0658 - val_loss: 1.0003 - val_reconstruction_loss: 0.9340 - val_kl_loss: 0.0664 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.0068 - reconstruction_loss: 0.9408 - kl_loss: 0.0658Restoring model weights from the end of the best epoch: 22.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.0068 - reconstruction_loss: 0.9407 - kl_loss: 0.0658 - val_loss: 1.0001 - val_reconstruction_loss: 0.9339 - val_kl_loss: 0.0662 - lr: 1.0000e-06\n",
      "Epoch 00032: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           1856        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           2112        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64)          256         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,970\n",
      "Trainable params: 4,664\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,409\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Instal MPL HEP for style formating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           1856        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           2112        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64)          256         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,970\n",
      "Trainable params: 4,664\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,409\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          7424        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 128)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           8256        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64)          256         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 64)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,196\n",
      "Trainable params: 16,698\n",
      "Non-trainable params: 498\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,761\n",
      "Trainable params: 16,377\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.84117506]\n",
      "Beta is  [0.84117506]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.4625 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          7424        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 128)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           8256        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64)          256         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 64)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,196\n",
      "Trainable params: 16,698\n",
      "Non-trainable params: 498\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,761\n",
      "Trainable params: 16,377\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          7424        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 128)          0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           8256        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64)          256         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 64)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,196\n",
      "Trainable params: 16,698\n",
      "Non-trainable params: 498\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,761\n",
      "Trainable params: 16,377\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           928         ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16)          64          ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 16)           0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           1088        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 64)          256         ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 64)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.32532155]\n",
      "Beta is  [0.32532155]\n",
      "  1/391 [..............................] - ETA: 16:56 - loss: 2.4882 - reconstruction_loss: 2.0601 - kl_loss: 0.4281Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.0960 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           928         ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16)          64          ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 16)           0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           1088        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 64)          256         ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 64)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           928         ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16)          64          ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 16)           0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 64)           1088        ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 64)          256         ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 64)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 57)          228         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 16)           928         ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16)          64          ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64)           1088        ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64)          256         ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 64)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,954\n",
      "Trainable params: 2,680\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,585\n",
      "Trainable params: 2,425\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.28492653]\n",
      "Beta is  [0.28492653]\n",
      "  1/391 [..............................] - ETA: 16:21 - loss: 2.5544 - reconstruction_loss: 1.9094 - kl_loss: 0.6451Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.8386 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 57)          228         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 16)           928         ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16)          64          ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64)           1088        ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64)          256         ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 64)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,954\n",
      "Trainable params: 2,680\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,585\n",
      "Trainable params: 2,425\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 57)          228         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 16)           928         ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16)          64          ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64)           1088        ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64)          256         ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 64)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,954\n",
      "Trainable params: 2,680\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,585\n",
      "Trainable params: 2,425\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 57)          228         ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64)           3712        ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64)          256         ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 64)           0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           2080        ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32)          128         ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 32)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,602\n",
      "Trainable params: 6,296\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,329\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.69362687]\n",
      "Beta is  [0.69362687]\n",
      "WARNING:tensorflow:5 out of the last 12518 calls to <function Model.make_train_function.<locals>.train_function at 0x7efeeeee6ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 0.7091 - reconstruction_loss: 0.4810 - kl_loss: 0.0575 - val_loss: 0.4518 - val_reconstruction_loss: 0.4403 - val_kl_loss: 0.0116 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4482 - reconstruction_loss: 0.4403 - kl_loss: 0.0070 - val_loss: 0.4410 - val_reconstruction_loss: 0.4360 - val_kl_loss: 0.0051 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4406 - reconstruction_loss: 0.4381 - kl_loss: 0.0038 - val_loss: 0.4381 - val_reconstruction_loss: 0.4349 - val_kl_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4402 - reconstruction_loss: 0.4373 - kl_loss: 0.0026 - val_loss: 0.4366 - val_reconstruction_loss: 0.4343 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4405 - reconstruction_loss: 0.4371 - kl_loss: 0.0020 - val_loss: 0.4360 - val_reconstruction_loss: 0.4341 - val_kl_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4358 - reconstruction_loss: 0.4366 - kl_loss: 0.0017 - val_loss: 0.4356 - val_reconstruction_loss: 0.4341 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4399 - reconstruction_loss: 0.4363 - kl_loss: 0.0016 - val_loss: 0.4355 - val_reconstruction_loss: 0.4342 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4368 - reconstruction_loss: 0.4363 - kl_loss: 0.0015 - val_loss: 0.4349 - val_reconstruction_loss: 0.4334 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4368 - reconstruction_loss: 0.4360 - kl_loss: 0.0014 - val_loss: 0.4348 - val_reconstruction_loss: 0.4334 - val_kl_loss: 0.0015 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4367 - reconstruction_loss: 0.4360 - kl_loss: 0.0012\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4367 - reconstruction_loss: 0.4360 - kl_loss: 0.0012 - val_loss: 0.4349 - val_reconstruction_loss: 0.4331 - val_kl_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4358 - reconstruction_loss: 0.4352 - kl_loss: 0.0014 - val_loss: 0.4340 - val_reconstruction_loss: 0.4326 - val_kl_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4354 - reconstruction_loss: 0.4352 - kl_loss: 0.0012 - val_loss: 0.4342 - val_reconstruction_loss: 0.4329 - val_kl_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4346 - reconstruction_loss: 0.4351 - kl_loss: 0.0011\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4346 - reconstruction_loss: 0.4351 - kl_loss: 0.0011 - val_loss: 0.4340 - val_reconstruction_loss: 0.4328 - val_kl_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4339 - reconstruction_loss: 0.4347 - kl_loss: 0.0012 - val_loss: 0.4341 - val_reconstruction_loss: 0.4329 - val_kl_loss: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4354 - reconstruction_loss: 0.4348 - kl_loss: 0.0012 - val_loss: 0.4341 - val_reconstruction_loss: 0.4326 - val_kl_loss: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4345 - reconstruction_loss: 0.4348 - kl_loss: 0.0012\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4345 - reconstruction_loss: 0.4348 - kl_loss: 0.0012 - val_loss: 0.4339 - val_reconstruction_loss: 0.4325 - val_kl_loss: 0.0014 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4416 - reconstruction_loss: 0.4351 - kl_loss: 0.0012 - val_loss: 0.4340 - val_reconstruction_loss: 0.4324 - val_kl_loss: 0.0016 - lr: 1.0000e-06\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4337 - reconstruction_loss: 0.4352 - kl_loss: 0.0012 - val_loss: 0.4342 - val_reconstruction_loss: 0.4331 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 19/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4374 - reconstruction_loss: 0.4348 - kl_loss: 0.0012\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4374 - reconstruction_loss: 0.4348 - kl_loss: 0.0012 - val_loss: 0.4339 - val_reconstruction_loss: 0.4325 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4362 - reconstruction_loss: 0.4348 - kl_loss: 0.0013 - val_loss: 0.4339 - val_reconstruction_loss: 0.4325 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4402 - reconstruction_loss: 0.4350 - kl_loss: 0.0012 - val_loss: 0.4340 - val_reconstruction_loss: 0.4325 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4368 - reconstruction_loss: 0.4350 - kl_loss: 0.0012 - val_loss: 0.4342 - val_reconstruction_loss: 0.4329 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4383 - reconstruction_loss: 0.4351 - kl_loss: 0.0012 - val_loss: 0.4339 - val_reconstruction_loss: 0.4325 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4366 - reconstruction_loss: 0.4349 - kl_loss: 0.0012 - val_loss: 0.4341 - val_reconstruction_loss: 0.4327 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4403 - reconstruction_loss: 0.4349 - kl_loss: 0.0012 - val_loss: 0.4339 - val_reconstruction_loss: 0.4324 - val_kl_loss: 0.0016 - lr: 1.0000e-06\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4339 - reconstruction_loss: 0.4349 - kl_loss: 0.0013Restoring model weights from the end of the best epoch: 16.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4339 - reconstruction_loss: 0.4349 - kl_loss: 0.0013 - val_loss: 0.4340 - val_reconstruction_loss: 0.4326 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 00026: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 57)          228         ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64)           3712        ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64)          256         ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 64)           0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           2080        ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32)          128         ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 32)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,602\n",
      "Trainable params: 6,296\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,329\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 57)          228         ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64)           3712        ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64)          256         ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 64)           0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           2080        ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32)          128         ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 32)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,602\n",
      "Trainable params: 6,296\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,329\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 57)          228         ['input_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 16)           928         ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16)          64          ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 16)           0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 64)           1088        ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 64)          256         ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 64)           0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.62026182]\n",
      "Beta is  [0.62026182]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 0.7985 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 57)          228         ['input_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 16)           928         ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16)          64          ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 16)           0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 64)           1088        ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 64)          256         ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 64)           0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 57)          228         ['input_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 16)           928         ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16)          64          ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 16)           0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 64)           1088        ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 64)          256         ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 64)           0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 57)          228         ['input_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 16)           928         ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16)          64          ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 16)           0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 32)           544         ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 32)          128         ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 32)           0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.8820358]\n",
      "Beta is  [0.8820358]\n",
      "  1/391 [..............................] - ETA: 18:36 - loss: 1.9770 - reconstruction_loss: 0.3723 - kl_loss: 1.6047Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.3320 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 57)          228         ['input_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 16)           928         ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16)          64          ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 16)           0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 32)           544         ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 32)          128         ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 32)           0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 57)          228         ['input_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 16)           928         ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16)          64          ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 16)           0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 32)           544         ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 32)          128         ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 32)           0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 57)          228         ['input_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           928         ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16)          64          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 16)           0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 32)           544         ['leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 32)          128         ['dense_36[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 32)           0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_7 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,090\n",
      "Trainable params: 1,880\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " leaky_re_lu_31 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,817\n",
      "Trainable params: 1,721\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.]\n",
      "Beta is  [0.]\n",
      "  9/391 [..............................] - ETA: 12s - loss: 2.8587 - reconstruction_loss: 2.8021 - kl_loss: 0.0000e+00Batch 9: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: nan - reconstruction_loss: 2.7788 - kl_loss: nan - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: nan - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 57)          228         ['input_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           928         ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16)          64          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 16)           0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 32)           544         ['leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 32)          128         ['dense_36[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 32)           0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_7 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,090\n",
      "Trainable params: 1,880\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,817\n",
      "Trainable params: 1,721\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 57)          228         ['input_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           928         ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16)          64          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 16)           0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 32)           544         ['leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 32)          128         ['dense_36[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 32)           0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_7 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,090\n",
      "Trainable params: 1,880\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,817\n",
      "Trainable params: 1,721\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 57)          228         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 16)           928         ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 16)          64          ['dense_40[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 16)           0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 32)           544         ['leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32)          128         ['dense_41[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 32)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_8 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [1.]\n",
      "Beta is  [1.]\n",
      " 15/391 [>.............................] - ETA: 12s - loss: 1.3158 - reconstruction_loss: 0.0000e+00 - kl_loss: 1.2310Batch 16: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 0.0000e+00 - kl_loss: inf - val_loss: inf - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 57)          228         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 16)           928         ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 16)          64          ['dense_40[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 16)           0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 32)           544         ['leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32)          128         ['dense_41[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 32)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_8 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 57)          228         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 16)           928         ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 16)          64          ['dense_40[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_32 (LeakyReLU)     (None, 16)           0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 32)           544         ['leaky_re_lu_32[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32)          128         ['dense_41[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_33 (LeakyReLU)     (None, 32)           0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_33[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_8 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_34 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_44 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 57)          228         ['input_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           928         ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16)          64          ['dense_45[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 16)           0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 64)           1088        ['leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 64)          256         ['dense_46[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 64)           0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_9 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.47490837]\n",
      "Beta is  [0.47490837]\n",
      "  1/391 [..............................] - ETA: 16:15 - loss: 2.5262 - reconstruction_loss: 1.7078 - kl_loss: 0.8184Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4654 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 57)          228         ['input_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           928         ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16)          64          ['dense_45[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 16)           0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 64)           1088        ['leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 64)          256         ['dense_46[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 64)           0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_9 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 57)          228         ['input_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 16)           928         ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16)          64          ['dense_45[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_36 (LeakyReLU)     (None, 16)           0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 64)           1088        ['leaky_re_lu_36[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 64)          256         ['dense_46[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_37 (LeakyReLU)     (None, 64)           0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_37[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_9 (Sampling)          (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 57)          228         ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 16)           928         ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 16)          64          ['dense_50[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 16)           0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 64)           1088        ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 64)          256         ['dense_51[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 64)           0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.47661947]\n",
      "Beta is  [0.47661947]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.2332 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 57)          228         ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 16)           928         ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 16)          64          ['dense_50[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 16)           0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 64)           1088        ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 64)          256         ['dense_51[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 64)           0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 57)          228         ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 16)           928         ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 16)          64          ['dense_50[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 16)           0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 64)           1088        ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 64)          256         ['dense_51[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 64)           0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_41[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_10 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_43 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 57)          228         ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 16)           928         ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 16)          64          ['dense_55[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 16)           0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 64)           1088        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 64)          256         ['dense_56[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 64)           0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.48148072]\n",
      "Beta is  [0.48148072]\n",
      "  1/391 [..............................] - ETA: 18:57 - loss: 2.5052 - reconstruction_loss: 1.6896 - kl_loss: 0.8157Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5952 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 57)          228         ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 16)           928         ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 16)          64          ['dense_55[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 16)           0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 64)           1088        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 64)          256         ['dense_56[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 64)           0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 57)          228         ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 16)           928         ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 16)          64          ['dense_55[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 16)           0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " dense_56 (Dense)               (None, 64)           1088        ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 64)          256         ['dense_56[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 64)           0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_46 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_47 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 57)          228         ['input_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 16)           928         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16)          64          ['dense_60[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 16)           0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 64)           1088        ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 64)          256         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 64)           0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_12 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.48867368]\n",
      "Beta is  [0.48867368]\n",
      "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_train_function.<locals>.train_function at 0x7eff4df52670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.3871 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 57)          228         ['input_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 16)           928         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16)          64          ['dense_60[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 16)           0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 64)           1088        ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 64)          256         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 64)           0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_12 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 57)          228         ['input_13[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_60 (Dense)               (None, 16)           928         ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16)          64          ['dense_60[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 16)           0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " dense_61 (Dense)               (None, 64)           1088        ['leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 64)          256         ['dense_61[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 64)           0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_12 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_50 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_51 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 57)          228         ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 16)           928         ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16)          64          ['dense_65[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 16)           0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           1088        ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 64)           0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_13 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.53740386]\n",
      "Beta is  [0.53740386]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 1.1469 - reconstruction_loss: 0.7859 - kl_loss: 0.0924 - val_loss: 0.6869 - val_reconstruction_loss: 0.6560 - val_kl_loss: 0.0309 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6809 - reconstruction_loss: 0.6497 - kl_loss: 0.0240 - val_loss: 0.6616 - val_reconstruction_loss: 0.6400 - val_kl_loss: 0.0216 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6621 - reconstruction_loss: 0.6423 - kl_loss: 0.0184 - val_loss: 0.6544 - val_reconstruction_loss: 0.6359 - val_kl_loss: 0.0185 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6558 - reconstruction_loss: 0.6391 - kl_loss: 0.0164 - val_loss: 0.6507 - val_reconstruction_loss: 0.6341 - val_kl_loss: 0.0166 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6526 - reconstruction_loss: 0.6372 - kl_loss: 0.0155 - val_loss: 0.6487 - val_reconstruction_loss: 0.6331 - val_kl_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6534 - reconstruction_loss: 0.6360 - kl_loss: 0.0153 - val_loss: 0.6473 - val_reconstruction_loss: 0.6305 - val_kl_loss: 0.0168 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6514 - reconstruction_loss: 0.6349 - kl_loss: 0.0151 - val_loss: 0.6461 - val_reconstruction_loss: 0.6288 - val_kl_loss: 0.0172 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6522 - reconstruction_loss: 0.6333 - kl_loss: 0.0151 - val_loss: 0.6448 - val_reconstruction_loss: 0.6305 - val_kl_loss: 0.0142 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6483 - reconstruction_loss: 0.6328 - kl_loss: 0.0150 - val_loss: 0.6432 - val_reconstruction_loss: 0.6288 - val_kl_loss: 0.0144 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6474 - reconstruction_loss: 0.6315 - kl_loss: 0.0150 - val_loss: 0.6425 - val_reconstruction_loss: 0.6271 - val_kl_loss: 0.0154 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6462 - reconstruction_loss: 0.6300 - kl_loss: 0.0155 - val_loss: 0.6421 - val_reconstruction_loss: 0.6262 - val_kl_loss: 0.0159 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6458 - reconstruction_loss: 0.6290 - kl_loss: 0.0157 - val_loss: 0.6404 - val_reconstruction_loss: 0.6240 - val_kl_loss: 0.0164 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6435 - reconstruction_loss: 0.6274 - kl_loss: 0.0164 - val_loss: 0.6398 - val_reconstruction_loss: 0.6238 - val_kl_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6435 - reconstruction_loss: 0.6263 - kl_loss: 0.0169 - val_loss: 0.6393 - val_reconstruction_loss: 0.6220 - val_kl_loss: 0.0172 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6436 - reconstruction_loss: 0.6256 - kl_loss: 0.0172 - val_loss: 0.6389 - val_reconstruction_loss: 0.6204 - val_kl_loss: 0.0184 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6430 - reconstruction_loss: 0.6247 - kl_loss: 0.0177 - val_loss: 0.6382 - val_reconstruction_loss: 0.6200 - val_kl_loss: 0.0181 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6424 - reconstruction_loss: 0.6240 - kl_loss: 0.0178 - val_loss: 0.6378 - val_reconstruction_loss: 0.6179 - val_kl_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6423 - reconstruction_loss: 0.6242 - kl_loss: 0.0181 - val_loss: 0.6377 - val_reconstruction_loss: 0.6190 - val_kl_loss: 0.0186 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6392 - reconstruction_loss: 0.6233 - kl_loss: 0.0182 - val_loss: 0.6379 - val_reconstruction_loss: 0.6174 - val_kl_loss: 0.0205 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6425 - reconstruction_loss: 0.6230 - kl_loss: 0.0183 - val_loss: 0.6373 - val_reconstruction_loss: 0.6189 - val_kl_loss: 0.0184 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6415 - reconstruction_loss: 0.6224 - kl_loss: 0.0184 - val_loss: 0.6371 - val_reconstruction_loss: 0.6187 - val_kl_loss: 0.0183 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6404 - reconstruction_loss: 0.6224 - kl_loss: 0.0183 - val_loss: 0.6370 - val_reconstruction_loss: 0.6175 - val_kl_loss: 0.0194 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6418 - reconstruction_loss: 0.6223 - kl_loss: 0.0185 - val_loss: 0.6369 - val_reconstruction_loss: 0.6166 - val_kl_loss: 0.0202 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6400 - reconstruction_loss: 0.6216 - kl_loss: 0.0187 - val_loss: 0.6368 - val_reconstruction_loss: 0.6168 - val_kl_loss: 0.0200 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6400 - reconstruction_loss: 0.6216 - kl_loss: 0.0185 - val_loss: 0.6361 - val_reconstruction_loss: 0.6163 - val_kl_loss: 0.0198 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6389 - reconstruction_loss: 0.6213 - kl_loss: 0.0187 - val_loss: 0.6366 - val_reconstruction_loss: 0.6170 - val_kl_loss: 0.0196 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6394 - reconstruction_loss: 0.6209 - kl_loss: 0.0187\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6394 - reconstruction_loss: 0.6209 - kl_loss: 0.0187 - val_loss: 0.6364 - val_reconstruction_loss: 0.6169 - val_kl_loss: 0.0194 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6411 - reconstruction_loss: 0.6204 - kl_loss: 0.0187 - val_loss: 0.6360 - val_reconstruction_loss: 0.6170 - val_kl_loss: 0.0189 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6390 - reconstruction_loss: 0.6201 - kl_loss: 0.0187 - val_loss: 0.6356 - val_reconstruction_loss: 0.6163 - val_kl_loss: 0.0193 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6392 - reconstruction_loss: 0.6199 - kl_loss: 0.0187 - val_loss: 0.6357 - val_reconstruction_loss: 0.6162 - val_kl_loss: 0.0195 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6388 - reconstruction_loss: 0.6202 - kl_loss: 0.0187\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6388 - reconstruction_loss: 0.6202 - kl_loss: 0.0187 - val_loss: 0.6358 - val_reconstruction_loss: 0.6166 - val_kl_loss: 0.0192 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6375 - reconstruction_loss: 0.6199 - kl_loss: 0.0188 - val_loss: 0.6362 - val_reconstruction_loss: 0.6167 - val_kl_loss: 0.0194 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6381 - reconstruction_loss: 0.6200 - kl_loss: 0.0188 - val_loss: 0.6356 - val_reconstruction_loss: 0.6163 - val_kl_loss: 0.0192 - lr: 1.0000e-05\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6380 - reconstruction_loss: 0.6199 - kl_loss: 0.0187\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6380 - reconstruction_loss: 0.6199 - kl_loss: 0.0187 - val_loss: 0.6358 - val_reconstruction_loss: 0.6165 - val_kl_loss: 0.0193 - lr: 1.0000e-05\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6401 - reconstruction_loss: 0.6199 - kl_loss: 0.0187 - val_loss: 0.6361 - val_reconstruction_loss: 0.6167 - val_kl_loss: 0.0193 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6386 - reconstruction_loss: 0.6199 - kl_loss: 0.0187 - val_loss: 0.6360 - val_reconstruction_loss: 0.6167 - val_kl_loss: 0.0192 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6392 - reconstruction_loss: 0.6201 - kl_loss: 0.0187\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6392 - reconstruction_loss: 0.6201 - kl_loss: 0.0187 - val_loss: 0.6359 - val_reconstruction_loss: 0.6165 - val_kl_loss: 0.0193 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6387 - reconstruction_loss: 0.6198 - kl_loss: 0.0187 - val_loss: 0.6357 - val_reconstruction_loss: 0.6162 - val_kl_loss: 0.0194 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6389 - reconstruction_loss: 0.6198 - kl_loss: 0.0187 - val_loss: 0.6358 - val_reconstruction_loss: 0.6166 - val_kl_loss: 0.0192 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6389 - reconstruction_loss: 0.6200 - kl_loss: 0.0187 - val_loss: 0.6360 - val_reconstruction_loss: 0.6166 - val_kl_loss: 0.0194 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6380 - reconstruction_loss: 0.6198 - kl_loss: 0.0187 - val_loss: 0.6361 - val_reconstruction_loss: 0.6166 - val_kl_loss: 0.0195 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6389 - reconstruction_loss: 0.6197 - kl_loss: 0.0187 - val_loss: 0.6358 - val_reconstruction_loss: 0.6165 - val_kl_loss: 0.0192 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "387/391 [============================>.] - ETA: 0s - loss: 0.6390 - reconstruction_loss: 0.6199 - kl_loss: 0.0187Restoring model weights from the end of the best epoch: 33.\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.6390 - reconstruction_loss: 0.6199 - kl_loss: 0.0187 - val_loss: 0.6364 - val_reconstruction_loss: 0.6170 - val_kl_loss: 0.0194 - lr: 1.0000e-06\n",
      "Epoch 00043: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 57)          228         ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 16)           928         ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16)          64          ['dense_65[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 16)           0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           1088        ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 64)           0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_13 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 57)          228         ['input_14[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 16)           928         ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16)          64          ['dense_65[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_52 (LeakyReLU)     (None, 16)           0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 64)           1088        ['leaky_re_lu_52[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 64)          256         ['dense_66[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_53 (LeakyReLU)     (None, 64)           0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_53[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_13 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_54 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 57)          228         ['input_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 16)           928         ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16)          64          ['dense_70[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16)           0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 64)           1088        ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 64)          256         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 64)           0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_14 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.48686935]\n",
      "Beta is  [0.48686935]\n",
      "  1/391 [..............................] - ETA: 15:55 - loss: 2.5224 - reconstruction_loss: 1.4442 - kl_loss: 1.0781Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4115 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 57)          228         ['input_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 16)           928         ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16)          64          ['dense_70[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16)           0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 64)           1088        ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 64)          256         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 64)           0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_14 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 57)          228         ['input_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 16)           928         ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16)          64          ['dense_70[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)     (None, 16)           0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 64)           1088        ['leaky_re_lu_56[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 64)          256         ['dense_71[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)     (None, 64)           0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_57[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_14 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 57)          228         ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 16)           928         ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 16)          64          ['dense_75[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 16)           0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 64)           1088        ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 64)          256         ['dense_76[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 64)           0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.48566978]\n",
      "Beta is  [0.48566978]\n",
      "  4/391 [..............................] - ETA: 10s - loss: 2.3161 - reconstruction_loss: 1.5692 - kl_loss: 1.1039  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5264 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 57)          228         ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 16)           928         ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 16)          64          ['dense_75[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 16)           0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 64)           1088        ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 64)          256         ['dense_76[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 64)           0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 57)          228         ['input_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 16)           928         ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 16)          64          ['dense_75[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_60 (LeakyReLU)     (None, 16)           0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 64)           1088        ['leaky_re_lu_60[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 64)          256         ['dense_76[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_61 (LeakyReLU)     (None, 64)           0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_61[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 57)          228         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 16)           928         ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16)          64          ['dense_80[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 16)           0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 64)           1088        ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64)          256         ['dense_81[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 64)           0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_16 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.47886059]\n",
      "Beta is  [0.47886059]\n",
      "  1/391 [..............................] - ETA: 17:15 - loss: 2.2030 - reconstruction_loss: 1.6422 - kl_loss: 0.5608Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6084 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 57)          228         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 16)           928         ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16)          64          ['dense_80[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 16)           0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 64)           1088        ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64)          256         ['dense_81[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 64)           0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_16 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 57)          228         ['input_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_80 (Dense)               (None, 16)           928         ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 16)          64          ['dense_80[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_64 (LeakyReLU)     (None, 16)           0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " dense_81 (Dense)               (None, 64)           1088        ['leaky_re_lu_64[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64)          256         ['dense_81[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_65 (LeakyReLU)     (None, 64)           0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_65[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_16 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_67 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 57)          228         ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 16)           928         ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 16)          64          ['dense_85[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 16)           0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 64)           1088        ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 64)          256         ['dense_86[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 64)           0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_17 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.33740436]\n",
      "Beta is  [0.33740436]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 2.4914 - reconstruction_loss: 2.1262 - kl_loss: 0.3397  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.0356 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 57)          228         ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 16)           928         ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 16)          64          ['dense_85[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 16)           0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 64)           1088        ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 64)          256         ['dense_86[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 64)           0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_17 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 57)          228         ['input_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_85 (Dense)               (None, 16)           928         ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 16)          64          ['dense_85[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_68 (LeakyReLU)     (None, 16)           0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " dense_86 (Dense)               (None, 64)           1088        ['leaky_re_lu_68[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 64)          256         ['dense_86[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)     (None, 64)           0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_69[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_17 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 57)          228         ['input_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 16)           928         ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 16)          64          ['dense_90[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)     (None, 16)           0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 64)           1088        ['leaky_re_lu_72[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 64)          256         ['dense_91[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)     (None, 64)           0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.48850401]\n",
      "Beta is  [0.48850401]\n",
      "  1/391 [..............................] - ETA: 18:51 - loss: 3.2126 - reconstruction_loss: 1.3570 - kl_loss: 1.8555Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4697 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 57)          228         ['input_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 16)           928         ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 16)          64          ['dense_90[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)     (None, 16)           0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 64)           1088        ['leaky_re_lu_72[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 64)          256         ['dense_91[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)     (None, 64)           0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_19 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 57)          228         ['input_19[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_90 (Dense)               (None, 16)           928         ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 16)          64          ['dense_90[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)     (None, 16)           0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " dense_91 (Dense)               (None, 64)           1088        ['leaky_re_lu_72[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 64)          256         ['dense_91[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)     (None, 64)           0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_73[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_94 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 57)          228         ['input_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 16)           928         ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16)          64          ['dense_95[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)     (None, 16)           0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 64)           1088        ['leaky_re_lu_76[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 64)          256         ['dense_96[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)     (None, 64)           0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_19 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.62786388]\n",
      "Beta is  [0.62786388]\n",
      "  1/391 [..............................] - ETA: 19:11 - loss: 2.1080 - reconstruction_loss: 1.2334 - kl_loss: 0.8745Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.1180 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 57)          228         ['input_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 16)           928         ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16)          64          ['dense_95[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)     (None, 16)           0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 64)           1088        ['leaky_re_lu_76[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 64)          256         ['dense_96[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)     (None, 64)           0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_19 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 57)          228         ['input_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_95 (Dense)               (None, 16)           928         ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 16)          64          ['dense_95[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)     (None, 16)           0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 64)           1088        ['leaky_re_lu_76[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 64)          256         ['dense_96[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)     (None, 64)           0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_77[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_19 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 57)          228         ['input_21[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 16)           928         ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 16)          64          ['dense_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)     (None, 16)           0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 64)           1088        ['leaky_re_lu_80[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 64)          256         ['dense_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_81 (LeakyReLU)     (None, 64)           0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_20 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.30826171]\n",
      "Beta is  [0.30826171]\n",
      "  7/391 [..............................] - ETA: 13s - loss: 3.1452 - reconstruction_loss: 2.1588 - kl_loss: 0.7106Batch 7: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.1039 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 57)          228         ['input_21[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 16)           928         ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 16)          64          ['dense_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)     (None, 16)           0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 64)           1088        ['leaky_re_lu_80[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 64)          256         ['dense_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_81 (LeakyReLU)     (None, 64)           0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_20 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 57)          228         ['input_21[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_100 (Dense)              (None, 16)           928         ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 16)          64          ['dense_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)     (None, 16)           0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 64)           1088        ['leaky_re_lu_80[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 64)          256         ['dense_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_81 (LeakyReLU)     (None, 64)           0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_81[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_20 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_104 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 57)          228         ['input_22[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 16)           928         ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 16)          64          ['dense_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_84 (LeakyReLU)     (None, 16)           0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 64)           1088        ['leaky_re_lu_84[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 64)          256         ['dense_106[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_85 (LeakyReLU)     (None, 64)           0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_21 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_86 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_87 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.4769955]\n",
      "Beta is  [0.4769955]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.2439 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 57)          228         ['input_22[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 16)           928         ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 16)          64          ['dense_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_84 (LeakyReLU)     (None, 16)           0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 64)           1088        ['leaky_re_lu_84[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 64)          256         ['dense_106[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_85 (LeakyReLU)     (None, 64)           0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_21 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_86 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_87 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 57)          228         ['input_22[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_105 (Dense)              (None, 16)           928         ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 16)          64          ['dense_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_84 (LeakyReLU)     (None, 16)           0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " dense_106 (Dense)              (None, 64)           1088        ['leaky_re_lu_84[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 64)          256         ['dense_106[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_85 (LeakyReLU)     (None, 64)           0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_85[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_21 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_108 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_86 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_87 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 57)          228         ['input_23[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 16)           928         ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 16)          64          ['dense_110[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_88 (LeakyReLU)     (None, 16)           0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " dense_111 (Dense)              (None, 64)           1088        ['leaky_re_lu_88[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 64)          256         ['dense_111[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_89 (LeakyReLU)     (None, 64)           0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_90 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_91 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.60729753]\n",
      "Beta is  [0.60729753]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8618 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 57)          228         ['input_23[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 16)           928         ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 16)          64          ['dense_110[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_88 (LeakyReLU)     (None, 16)           0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " dense_111 (Dense)              (None, 64)           1088        ['leaky_re_lu_88[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 64)          256         ['dense_111[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_89 (LeakyReLU)     (None, 64)           0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_90 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_91 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 57)          228         ['input_23[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 16)           928         ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 16)          64          ['dense_110[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_88 (LeakyReLU)     (None, 16)           0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " dense_111 (Dense)              (None, 64)           1088        ['leaky_re_lu_88[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 64)          256         ['dense_111[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_89 (LeakyReLU)     (None, 64)           0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_89[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_90 (LeakyReLU)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_91 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 57)          228         ['input_24[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 16)           928         ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 16)          64          ['dense_115[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_92 (LeakyReLU)     (None, 16)           0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " dense_116 (Dense)              (None, 32)           544         ['leaky_re_lu_92[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 32)          128         ['dense_116[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_93 (LeakyReLU)     (None, 32)           0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_23 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_118 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_94 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_119 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_95 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.69236684]\n",
      "Beta is  [0.69236684]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8587 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 57)          228         ['input_24[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 16)           928         ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 16)          64          ['dense_115[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_92 (LeakyReLU)     (None, 16)           0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " dense_116 (Dense)              (None, 32)           544         ['leaky_re_lu_92[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 32)          128         ['dense_116[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_93 (LeakyReLU)     (None, 32)           0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_23 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_118 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_94 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_119 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_95 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 57)          228         ['input_24[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 16)           928         ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 16)          64          ['dense_115[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_92 (LeakyReLU)     (None, 16)           0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " dense_116 (Dense)              (None, 32)           544         ['leaky_re_lu_92[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 32)          128         ['dense_116[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_93 (LeakyReLU)     (None, 32)           0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_93[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_23 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_118 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_94 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_119 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_95 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 57)          228         ['input_25[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 16)           928         ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 16)          64          ['dense_120[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_96 (LeakyReLU)     (None, 16)           0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 32)           544         ['leaky_re_lu_96[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 32)          128         ['dense_121[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_97 (LeakyReLU)     (None, 32)           0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_98 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_99 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.692367]\n",
      "Beta is  [0.692367]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.5821 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 57)          228         ['input_25[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 16)           928         ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 16)          64          ['dense_120[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_96 (LeakyReLU)     (None, 16)           0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 32)           544         ['leaky_re_lu_96[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 32)          128         ['dense_121[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_97 (LeakyReLU)     (None, 32)           0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_98 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_99 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 57)          228         ['input_25[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_120 (Dense)              (None, 16)           928         ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 16)          64          ['dense_120[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_96 (LeakyReLU)     (None, 16)           0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " dense_121 (Dense)              (None, 32)           544         ['leaky_re_lu_96[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 32)          128         ['dense_121[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_97 (LeakyReLU)     (None, 32)           0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_97[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_98 (LeakyReLU)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_99 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 57)          228         ['input_26[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 16)           928         ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 16)          64          ['dense_125[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_100 (LeakyReLU)    (None, 16)           0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 64)           1088        ['leaky_re_lu_100[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 64)          256         ['dense_126[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_101 (LeakyReLU)    (None, 64)           0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_25 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_102 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_103 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.60293355]\n",
      "Beta is  [0.60293355]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 2.3464 - reconstruction_loss: 1.2270 - kl_loss: 1.0607  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.2069 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 57)          228         ['input_26[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 16)           928         ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 16)          64          ['dense_125[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_100 (LeakyReLU)    (None, 16)           0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 64)           1088        ['leaky_re_lu_100[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 64)          256         ['dense_126[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_101 (LeakyReLU)    (None, 64)           0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_25 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_102 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_103 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 57)          228         ['input_26[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_125 (Dense)              (None, 16)           928         ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_126 (Batch  (None, 16)          64          ['dense_125[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_100 (LeakyReLU)    (None, 16)           0           ['batch_normalization_126[0][0]']\n",
      "                                                                                                  \n",
      " dense_126 (Dense)              (None, 64)           1088        ['leaky_re_lu_100[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_127 (Batch  (None, 64)          256         ['dense_126[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_101 (LeakyReLU)    (None, 64)           0           ['batch_normalization_127[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_101[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_25 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_102 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_103 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 57)          228         ['input_27[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 16)           928         ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 16)          64          ['dense_130[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_104 (LeakyReLU)    (None, 16)           0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 32)           544         ['leaky_re_lu_104[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 32)          128         ['dense_131[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_105 (LeakyReLU)    (None, 32)           0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_26 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_106 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_107 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44773045]\n",
      "Beta is  [0.44773045]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.1952 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 57)          228         ['input_27[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 16)           928         ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 16)          64          ['dense_130[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_104 (LeakyReLU)    (None, 16)           0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 32)           544         ['leaky_re_lu_104[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 32)          128         ['dense_131[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_105 (LeakyReLU)    (None, 32)           0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_26 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_106 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_107 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_130 (Batch  (None, 57)          228         ['input_27[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_130 (Dense)              (None, 16)           928         ['batch_normalization_130[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_131 (Batch  (None, 16)          64          ['dense_130[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_104 (LeakyReLU)    (None, 16)           0           ['batch_normalization_131[0][0]']\n",
      "                                                                                                  \n",
      " dense_131 (Dense)              (None, 32)           544         ['leaky_re_lu_104[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 32)          128         ['dense_131[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_105 (LeakyReLU)    (None, 32)           0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_105[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_26 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_133 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_106 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_134 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_107 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 57)          228         ['input_28[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 16)           928         ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 16)          64          ['dense_135[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_108 (LeakyReLU)    (None, 16)           0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 32)           544         ['leaky_re_lu_108[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 32)          128         ['dense_136[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_109 (LeakyReLU)    (None, 32)           0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_27 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.69236757]\n",
      "Beta is  [0.69236757]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8601 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 57)          228         ['input_28[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 16)           928         ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 16)          64          ['dense_135[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_108 (LeakyReLU)    (None, 16)           0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 32)           544         ['leaky_re_lu_108[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 32)          128         ['dense_136[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_109 (LeakyReLU)    (None, 32)           0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_27 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_28 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_135 (Batch  (None, 57)          228         ['input_28[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 16)           928         ['batch_normalization_135[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_136 (Batch  (None, 16)          64          ['dense_135[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_108 (LeakyReLU)    (None, 16)           0           ['batch_normalization_136[0][0]']\n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 32)           544         ['leaky_re_lu_108[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_137 (Batch  (None, 32)          128         ['dense_136[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_109 (LeakyReLU)    (None, 32)           0           ['batch_normalization_137[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_109[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_27 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_138 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_110 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_139 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_111 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 57)          228         ['input_29[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 16)           928         ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 16)          64          ['dense_140[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_112 (LeakyReLU)    (None, 16)           0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 32)           544         ['leaky_re_lu_112[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 32)          128         ['dense_141[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_113 (LeakyReLU)    (None, 32)           0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_28 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_143 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_114 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_144 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_115 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.6924131]\n",
      "Beta is  [0.6924131]\n",
      "  1/391 [..............................] - ETA: 16:17 - loss: 2.1931 - reconstruction_loss: 0.9297 - kl_loss: 1.2634Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8838 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 57)          228         ['input_29[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 16)           928         ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 16)          64          ['dense_140[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_112 (LeakyReLU)    (None, 16)           0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 32)           544         ['leaky_re_lu_112[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 32)          128         ['dense_141[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_113 (LeakyReLU)    (None, 32)           0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_28 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_143 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_114 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_144 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_115 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_29 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (Batch  (None, 57)          228         ['input_29[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_140 (Dense)              (None, 16)           928         ['batch_normalization_140[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_141 (Batch  (None, 16)          64          ['dense_140[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_112 (LeakyReLU)    (None, 16)           0           ['batch_normalization_141[0][0]']\n",
      "                                                                                                  \n",
      " dense_141 (Dense)              (None, 32)           544         ['leaky_re_lu_112[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_142 (Batch  (None, 32)          128         ['dense_141[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_113 (LeakyReLU)    (None, 32)           0           ['batch_normalization_142[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_113[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_28 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_143 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_114 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_144 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_115 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 57)          228         ['input_30[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 16)           928         ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 16)          64          ['dense_145[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_116 (LeakyReLU)    (None, 16)           0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 32)           544         ['leaky_re_lu_116[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 32)          128         ['dense_146[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_117 (LeakyReLU)    (None, 32)           0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_118 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_119 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.70169542]\n",
      "Beta is  [0.70169542]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 1.6429 - reconstruction_loss: 0.9158 - kl_loss: 0.6886  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8739 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 57)          228         ['input_30[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 16)           928         ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 16)          64          ['dense_145[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_116 (LeakyReLU)    (None, 16)           0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 32)           544         ['leaky_re_lu_116[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 32)          128         ['dense_146[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_117 (LeakyReLU)    (None, 32)           0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_118 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_119 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_30 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 57)          228         ['input_30[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_145 (Dense)              (None, 16)           928         ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 16)          64          ['dense_145[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_116 (LeakyReLU)    (None, 16)           0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dense_146 (Dense)              (None, 32)           544         ['leaky_re_lu_116[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_147 (Batch  (None, 32)          128         ['dense_146[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_117 (LeakyReLU)    (None, 32)           0           ['batch_normalization_147[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_117[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_29 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_147 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_118 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_119 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 57)          228         ['input_31[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 16)           928         ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 16)          64          ['dense_150[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_120 (LeakyReLU)    (None, 16)           0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           1088        ['leaky_re_lu_120[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 64)          256         ['dense_151[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_121 (LeakyReLU)    (None, 64)           0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_30 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.32369951]\n",
      "Beta is  [0.32369951]\n",
      "  1/391 [..............................] - ETA: 16:08 - loss: 2.5896 - reconstruction_loss: 2.0290 - kl_loss: 0.5606Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.0130 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 57)          228         ['input_31[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 16)           928         ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 16)          64          ['dense_150[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_120 (LeakyReLU)    (None, 16)           0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           1088        ['leaky_re_lu_120[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 64)          256         ['dense_151[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_121 (LeakyReLU)    (None, 64)           0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_30 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_31 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_150 (Batch  (None, 57)          228         ['input_31[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_150 (Dense)              (None, 16)           928         ['batch_normalization_150[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_151 (Batch  (None, 16)          64          ['dense_150[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_120 (LeakyReLU)    (None, 16)           0           ['batch_normalization_151[0][0]']\n",
      "                                                                                                  \n",
      " dense_151 (Dense)              (None, 64)           1088        ['leaky_re_lu_120[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_152 (Batch  (None, 64)          256         ['dense_151[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_121 (LeakyReLU)    (None, 64)           0           ['batch_normalization_152[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_121[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_30 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_152 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_153 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_153 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_154 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_154 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 57)          228         ['input_32[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 16)           928         ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 16)          64          ['dense_155[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_124 (LeakyReLU)    (None, 16)           0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 32)           544         ['leaky_re_lu_124[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 32)          128         ['dense_156[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_125 (LeakyReLU)    (None, 32)           0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_31 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_158 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_159 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.68804205]\n",
      "Beta is  [0.68804205]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.7494 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 57)          228         ['input_32[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 16)           928         ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 16)          64          ['dense_155[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_124 (LeakyReLU)    (None, 16)           0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 32)           544         ['leaky_re_lu_124[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 32)          128         ['dense_156[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_125 (LeakyReLU)    (None, 32)           0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_31 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_158 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_159 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_32 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_155 (Batch  (None, 57)          228         ['input_32[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_155 (Dense)              (None, 16)           928         ['batch_normalization_155[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_156 (Batch  (None, 16)          64          ['dense_155[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_124 (LeakyReLU)    (None, 16)           0           ['batch_normalization_156[0][0]']\n",
      "                                                                                                  \n",
      " dense_156 (Dense)              (None, 32)           544         ['leaky_re_lu_124[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_157 (Batch  (None, 32)          128         ['dense_156[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_125 (LeakyReLU)    (None, 32)           0           ['batch_normalization_157[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_125[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_31 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_157 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_158 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_158 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_159 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_159 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 57)          228         ['input_33[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 16)           928         ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 16)          64          ['dense_160[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_128 (LeakyReLU)    (None, 16)           0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dense_161 (Dense)              (None, 32)           544         ['leaky_re_lu_128[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 32)          128         ['dense_161[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_129 (LeakyReLU)    (None, 32)           0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_32 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_163 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_130 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_164 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_131 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.68991581]\n",
      "Beta is  [0.68991581]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8956 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 57)          228         ['input_33[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 16)           928         ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 16)          64          ['dense_160[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_128 (LeakyReLU)    (None, 16)           0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dense_161 (Dense)              (None, 32)           544         ['leaky_re_lu_128[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 32)          128         ['dense_161[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_129 (LeakyReLU)    (None, 32)           0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_32 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_163 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_130 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_164 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_131 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 57)          228         ['input_33[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_160 (Dense)              (None, 16)           928         ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 16)          64          ['dense_160[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_128 (LeakyReLU)    (None, 16)           0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " dense_161 (Dense)              (None, 32)           544         ['leaky_re_lu_128[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 32)          128         ['dense_161[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_129 (LeakyReLU)    (None, 32)           0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_129[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_32 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_162 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_163 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_130 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_164 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_131 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 57)          228         ['input_34[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 16)           928         ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 16)          64          ['dense_165[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_132 (LeakyReLU)    (None, 16)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 32)           544         ['leaky_re_lu_132[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 32)          128         ['dense_166[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_133 (LeakyReLU)    (None, 32)           0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_33 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_168 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_134 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.45805019]\n",
      "Beta is  [0.45805019]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6834 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 57)          228         ['input_34[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 16)           928         ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 16)          64          ['dense_165[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_132 (LeakyReLU)    (None, 16)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 32)           544         ['leaky_re_lu_132[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 32)          128         ['dense_166[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_133 (LeakyReLU)    (None, 32)           0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_33 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_168 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_134 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 57)          228         ['input_34[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_165 (Dense)              (None, 16)           928         ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 16)          64          ['dense_165[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_132 (LeakyReLU)    (None, 16)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " dense_166 (Dense)              (None, 32)           544         ['leaky_re_lu_132[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 32)          128         ['dense_166[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_133 (LeakyReLU)    (None, 32)           0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_133[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_33 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_168 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_134 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_169 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 57)          228         ['input_35[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 16)           928         ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 16)          64          ['dense_170[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 16)           0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 32)           544         ['leaky_re_lu_136[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 32)          128         ['dense_171[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 32)           0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_34 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_173 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_174 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.46005559]\n",
      "Beta is  [0.46005559]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.0287 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 57)          228         ['input_35[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 16)           928         ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 16)          64          ['dense_170[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 16)           0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 32)           544         ['leaky_re_lu_136[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 32)          128         ['dense_171[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 32)           0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_34 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_173 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_174 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_35 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 57)          228         ['input_35[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_170 (Dense)              (None, 16)           928         ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 16)          64          ['dense_170[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_136 (LeakyReLU)    (None, 16)           0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " dense_171 (Dense)              (None, 32)           544         ['leaky_re_lu_136[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 32)          128         ['dense_171[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_137 (LeakyReLU)    (None, 32)           0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_137[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_34 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_173 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_174 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 57)          228         ['input_36[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_175 (Dense)              (None, 16)           928         ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 16)          64          ['dense_175[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_140 (LeakyReLU)    (None, 16)           0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dense_176 (Dense)              (None, 32)           544         ['leaky_re_lu_140[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 32)          128         ['dense_176[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_141 (LeakyReLU)    (None, 32)           0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_35 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_178 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_142 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_179 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_143 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.45555723]\n",
      "Beta is  [0.45555723]\n",
      "  1/391 [..............................] - ETA: 16:46 - loss: 2.4113 - reconstruction_loss: 1.8768 - kl_loss: 0.5345Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.7565 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 57)          228         ['input_36[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_175 (Dense)              (None, 16)           928         ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 16)          64          ['dense_175[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_140 (LeakyReLU)    (None, 16)           0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dense_176 (Dense)              (None, 32)           544         ['leaky_re_lu_140[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 32)          128         ['dense_176[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_141 (LeakyReLU)    (None, 32)           0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_35 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_178 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_142 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_179 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_143 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_36 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_175 (Batch  (None, 57)          228         ['input_36[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_175 (Dense)              (None, 16)           928         ['batch_normalization_175[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_176 (Batch  (None, 16)          64          ['dense_175[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_140 (LeakyReLU)    (None, 16)           0           ['batch_normalization_176[0][0]']\n",
      "                                                                                                  \n",
      " dense_176 (Dense)              (None, 32)           544         ['leaky_re_lu_140[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_177 (Batch  (None, 32)          128         ['dense_176[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_141 (LeakyReLU)    (None, 32)           0           ['batch_normalization_177[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_141[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_35 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_178 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_142 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_179 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_143 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 57)          228         ['input_37[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_180 (Dense)              (None, 16)           928         ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 16)          64          ['dense_180[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_144 (LeakyReLU)    (None, 16)           0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dense_181 (Dense)              (None, 32)           544         ['leaky_re_lu_144[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 32)          128         ['dense_181[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_145 (LeakyReLU)    (None, 32)           0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_36 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_183 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_146 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_184 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_147 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.68513458]\n",
      "Beta is  [0.68513458]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 2.1937 - reconstruction_loss: 0.9944 - kl_loss: 1.4264  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.9805 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 57)          228         ['input_37[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_180 (Dense)              (None, 16)           928         ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 16)          64          ['dense_180[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_144 (LeakyReLU)    (None, 16)           0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dense_181 (Dense)              (None, 32)           544         ['leaky_re_lu_144[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 32)          128         ['dense_181[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_145 (LeakyReLU)    (None, 32)           0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_36 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_183 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_146 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_184 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_147 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_180 (Batch  (None, 57)          228         ['input_37[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_180 (Dense)              (None, 16)           928         ['batch_normalization_180[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_181 (Batch  (None, 16)          64          ['dense_180[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_144 (LeakyReLU)    (None, 16)           0           ['batch_normalization_181[0][0]']\n",
      "                                                                                                  \n",
      " dense_181 (Dense)              (None, 32)           544         ['leaky_re_lu_144[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_182 (Batch  (None, 32)          128         ['dense_181[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_145 (LeakyReLU)    (None, 32)           0           ['batch_normalization_182[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_145[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_36 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_182 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_183 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_146 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_183 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_184 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_147 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_184 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_185 (Batch  (None, 57)          228         ['input_38[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_185 (Dense)              (None, 16)           928         ['batch_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_186 (Batch  (None, 16)          64          ['dense_185[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_148 (LeakyReLU)    (None, 16)           0           ['batch_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " dense_186 (Dense)              (None, 32)           544         ['leaky_re_lu_148[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_187 (Batch  (None, 32)          128         ['dense_186[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_149 (LeakyReLU)    (None, 32)           0           ['batch_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_149[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_149[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_37 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_188 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_150 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_189 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_151 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.46646684]\n",
      "Beta is  [0.46646684]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.1124 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_185 (Batch  (None, 57)          228         ['input_38[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_185 (Dense)              (None, 16)           928         ['batch_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_186 (Batch  (None, 16)          64          ['dense_185[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_148 (LeakyReLU)    (None, 16)           0           ['batch_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " dense_186 (Dense)              (None, 32)           544         ['leaky_re_lu_148[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_187 (Batch  (None, 32)          128         ['dense_186[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_149 (LeakyReLU)    (None, 32)           0           ['batch_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_149[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_149[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_37 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_188 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_150 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_189 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_151 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_38 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_185 (Batch  (None, 57)          228         ['input_38[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_185 (Dense)              (None, 16)           928         ['batch_normalization_185[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_186 (Batch  (None, 16)          64          ['dense_185[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_148 (LeakyReLU)    (None, 16)           0           ['batch_normalization_186[0][0]']\n",
      "                                                                                                  \n",
      " dense_186 (Dense)              (None, 32)           544         ['leaky_re_lu_148[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_187 (Batch  (None, 32)          128         ['dense_186[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_149 (LeakyReLU)    (None, 32)           0           ['batch_normalization_187[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_149[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_149[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_37 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_187 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_188 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_150 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_188 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_189 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_151 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_189 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_190 (Batch  (None, 57)          228         ['input_39[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_190 (Dense)              (None, 16)           928         ['batch_normalization_190[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_191 (Batch  (None, 16)          64          ['dense_190[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_152 (LeakyReLU)    (None, 16)           0           ['batch_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dense_191 (Dense)              (None, 32)           544         ['leaky_re_lu_152[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_192 (Batch  (None, 32)          128         ['dense_191[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_153 (LeakyReLU)    (None, 32)           0           ['batch_normalization_192[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_153[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_153[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_38 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_193 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_154 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_194 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_155 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.45825472]\n",
      "Beta is  [0.45825472]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.3463 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_190 (Batch  (None, 57)          228         ['input_39[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_190 (Dense)              (None, 16)           928         ['batch_normalization_190[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_191 (Batch  (None, 16)          64          ['dense_190[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_152 (LeakyReLU)    (None, 16)           0           ['batch_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dense_191 (Dense)              (None, 32)           544         ['leaky_re_lu_152[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_192 (Batch  (None, 32)          128         ['dense_191[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_153 (LeakyReLU)    (None, 32)           0           ['batch_normalization_192[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_153[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_153[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_38 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_193 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_154 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_194 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_155 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_39 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_190 (Batch  (None, 57)          228         ['input_39[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_190 (Dense)              (None, 16)           928         ['batch_normalization_190[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_191 (Batch  (None, 16)          64          ['dense_190[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_152 (LeakyReLU)    (None, 16)           0           ['batch_normalization_191[0][0]']\n",
      "                                                                                                  \n",
      " dense_191 (Dense)              (None, 32)           544         ['leaky_re_lu_152[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_192 (Batch  (None, 32)          128         ['dense_191[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_153 (LeakyReLU)    (None, 32)           0           ['batch_normalization_192[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_153[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_153[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_38 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_193 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_154 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_194 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_155 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_194 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_40 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_195 (Batch  (None, 57)          228         ['input_40[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_195 (Dense)              (None, 16)           928         ['batch_normalization_195[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_196 (Batch  (None, 16)          64          ['dense_195[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_156 (LeakyReLU)    (None, 16)           0           ['batch_normalization_196[0][0]']\n",
      "                                                                                                  \n",
      " dense_196 (Dense)              (None, 32)           544         ['leaky_re_lu_156[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_197 (Batch  (None, 32)          128         ['dense_196[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_157 (LeakyReLU)    (None, 32)           0           ['batch_normalization_197[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_157[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_157[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_39 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_198 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_158 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_199 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_159 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.43526939]\n",
      "Beta is  [0.43526939]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 3s 3ms/step - loss: inf - reconstruction_loss: 1.2537 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_40 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_195 (Batch  (None, 57)          228         ['input_40[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_195 (Dense)              (None, 16)           928         ['batch_normalization_195[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_196 (Batch  (None, 16)          64          ['dense_195[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_156 (LeakyReLU)    (None, 16)           0           ['batch_normalization_196[0][0]']\n",
      "                                                                                                  \n",
      " dense_196 (Dense)              (None, 32)           544         ['leaky_re_lu_156[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_197 (Batch  (None, 32)          128         ['dense_196[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_157 (LeakyReLU)    (None, 32)           0           ['batch_normalization_197[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_157[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_157[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_39 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_198 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_158 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_199 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_159 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_40 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_195 (Batch  (None, 57)          228         ['input_40[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_195 (Dense)              (None, 16)           928         ['batch_normalization_195[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_196 (Batch  (None, 16)          64          ['dense_195[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_156 (LeakyReLU)    (None, 16)           0           ['batch_normalization_196[0][0]']\n",
      "                                                                                                  \n",
      " dense_196 (Dense)              (None, 32)           544         ['leaky_re_lu_156[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_197 (Batch  (None, 32)          128         ['dense_196[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_157 (LeakyReLU)    (None, 32)           0           ['batch_normalization_197[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_157[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_157[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_39 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_197 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_198 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_158 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_198 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_199 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_159 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_200 (Batch  (None, 57)          228         ['input_41[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_200 (Dense)              (None, 16)           928         ['batch_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_201 (Batch  (None, 16)          64          ['dense_200[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_160 (LeakyReLU)    (None, 16)           0           ['batch_normalization_201[0][0]']\n",
      "                                                                                                  \n",
      " dense_201 (Dense)              (None, 64)           1088        ['leaky_re_lu_160[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_202 (Batch  (None, 64)          256         ['dense_201[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_161 (LeakyReLU)    (None, 64)           0           ['batch_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_161[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_161[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_40 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_203 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_162 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_204 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_163 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.46086056]\n",
      "Beta is  [0.46086056]\n",
      "  1/391 [..............................] - ETA: 21:42 - loss: 2.1798 - reconstruction_loss: 1.6940 - kl_loss: 0.4858Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6545 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_200 (Batch  (None, 57)          228         ['input_41[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_200 (Dense)              (None, 16)           928         ['batch_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_201 (Batch  (None, 16)          64          ['dense_200[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_160 (LeakyReLU)    (None, 16)           0           ['batch_normalization_201[0][0]']\n",
      "                                                                                                  \n",
      " dense_201 (Dense)              (None, 64)           1088        ['leaky_re_lu_160[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_202 (Batch  (None, 64)          256         ['dense_201[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_161 (LeakyReLU)    (None, 64)           0           ['batch_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_161[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_161[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_40 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_203 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_162 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_204 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_163 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_41 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_200 (Batch  (None, 57)          228         ['input_41[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_200 (Dense)              (None, 16)           928         ['batch_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_201 (Batch  (None, 16)          64          ['dense_200[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_160 (LeakyReLU)    (None, 16)           0           ['batch_normalization_201[0][0]']\n",
      "                                                                                                  \n",
      " dense_201 (Dense)              (None, 64)           1088        ['leaky_re_lu_160[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_202 (Batch  (None, 64)          256         ['dense_201[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_161 (LeakyReLU)    (None, 64)           0           ['batch_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_161[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_161[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_40 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_203 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_162 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_204 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_163 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_204 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_205 (Batch  (None, 57)          228         ['input_42[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_205 (Dense)              (None, 16)           928         ['batch_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_206 (Batch  (None, 16)          64          ['dense_205[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_164 (LeakyReLU)    (None, 16)           0           ['batch_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " dense_206 (Dense)              (None, 32)           544         ['leaky_re_lu_164[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_207 (Batch  (None, 32)          128         ['dense_206[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_165 (LeakyReLU)    (None, 32)           0           ['batch_normalization_207[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_165[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_165[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_41 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_208 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_166 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_209 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_167 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.43176709]\n",
      "Beta is  [0.43176709]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 2.5700 - reconstruction_loss: 1.8510 - kl_loss: 0.6453  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.8250 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_205 (Batch  (None, 57)          228         ['input_42[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_205 (Dense)              (None, 16)           928         ['batch_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_206 (Batch  (None, 16)          64          ['dense_205[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_164 (LeakyReLU)    (None, 16)           0           ['batch_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " dense_206 (Dense)              (None, 32)           544         ['leaky_re_lu_164[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_207 (Batch  (None, 32)          128         ['dense_206[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_165 (LeakyReLU)    (None, 32)           0           ['batch_normalization_207[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_165[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_165[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_41 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_208 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_166 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_209 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_167 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_42 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_205 (Batch  (None, 57)          228         ['input_42[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_205 (Dense)              (None, 16)           928         ['batch_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_206 (Batch  (None, 16)          64          ['dense_205[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_164 (LeakyReLU)    (None, 16)           0           ['batch_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " dense_206 (Dense)              (None, 32)           544         ['leaky_re_lu_164[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_207 (Batch  (None, 32)          128         ['dense_206[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_165 (LeakyReLU)    (None, 32)           0           ['batch_normalization_207[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_165[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_165[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_41 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_207 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_208 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_166 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_208 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_209 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_167 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_209 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_43 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_210 (Batch  (None, 57)          228         ['input_43[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_210 (Dense)              (None, 16)           928         ['batch_normalization_210[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_211 (Batch  (None, 16)          64          ['dense_210[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_168 (LeakyReLU)    (None, 16)           0           ['batch_normalization_211[0][0]']\n",
      "                                                                                                  \n",
      " dense_211 (Dense)              (None, 32)           544         ['leaky_re_lu_168[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_212 (Batch  (None, 32)          128         ['dense_211[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_169 (LeakyReLU)    (None, 32)           0           ['batch_normalization_212[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_169[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_169[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_42 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_213 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_170 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_214 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_171 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44129948]\n",
      "Beta is  [0.44129948]\n",
      "  1/391 [..............................] - ETA: 17:51 - loss: 3.4964 - reconstruction_loss: 1.5759 - kl_loss: 1.9204Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5308 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_43 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_210 (Batch  (None, 57)          228         ['input_43[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_210 (Dense)              (None, 16)           928         ['batch_normalization_210[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_211 (Batch  (None, 16)          64          ['dense_210[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_168 (LeakyReLU)    (None, 16)           0           ['batch_normalization_211[0][0]']\n",
      "                                                                                                  \n",
      " dense_211 (Dense)              (None, 32)           544         ['leaky_re_lu_168[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_212 (Batch  (None, 32)          128         ['dense_211[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_169 (LeakyReLU)    (None, 32)           0           ['batch_normalization_212[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_169[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_169[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_42 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_213 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_170 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_214 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_171 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_43 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_210 (Batch  (None, 57)          228         ['input_43[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_210 (Dense)              (None, 16)           928         ['batch_normalization_210[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_211 (Batch  (None, 16)          64          ['dense_210[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_168 (LeakyReLU)    (None, 16)           0           ['batch_normalization_211[0][0]']\n",
      "                                                                                                  \n",
      " dense_211 (Dense)              (None, 32)           544         ['leaky_re_lu_168[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_212 (Batch  (None, 32)          128         ['dense_211[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_169 (LeakyReLU)    (None, 32)           0           ['batch_normalization_212[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_169[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_169[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_42 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_212 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_213 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_170 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_213 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_214 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_171 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_214 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_44 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_215 (Batch  (None, 57)          228         ['input_44[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_215 (Dense)              (None, 16)           928         ['batch_normalization_215[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_216 (Batch  (None, 16)          64          ['dense_215[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_172 (LeakyReLU)    (None, 16)           0           ['batch_normalization_216[0][0]']\n",
      "                                                                                                  \n",
      " dense_216 (Dense)              (None, 64)           1088        ['leaky_re_lu_172[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_217 (Batch  (None, 64)          256         ['dense_216[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_173 (LeakyReLU)    (None, 64)           0           ['batch_normalization_217[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_173[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_173[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_43 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_218 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_174 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_219 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_175 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.61176114]\n",
      "Beta is  [0.61176114]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8000 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_44 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_215 (Batch  (None, 57)          228         ['input_44[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_215 (Dense)              (None, 16)           928         ['batch_normalization_215[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_216 (Batch  (None, 16)          64          ['dense_215[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_172 (LeakyReLU)    (None, 16)           0           ['batch_normalization_216[0][0]']\n",
      "                                                                                                  \n",
      " dense_216 (Dense)              (None, 64)           1088        ['leaky_re_lu_172[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_217 (Batch  (None, 64)          256         ['dense_216[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_173 (LeakyReLU)    (None, 64)           0           ['batch_normalization_217[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_173[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_173[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_43 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_218 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_174 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_219 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_175 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_44 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_215 (Batch  (None, 57)          228         ['input_44[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_215 (Dense)              (None, 16)           928         ['batch_normalization_215[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_216 (Batch  (None, 16)          64          ['dense_215[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_172 (LeakyReLU)    (None, 16)           0           ['batch_normalization_216[0][0]']\n",
      "                                                                                                  \n",
      " dense_216 (Dense)              (None, 64)           1088        ['leaky_re_lu_172[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_217 (Batch  (None, 64)          256         ['dense_216[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_173 (LeakyReLU)    (None, 64)           0           ['batch_normalization_217[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_173[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_173[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_43 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_217 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_218 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_174 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_218 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_219 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_175 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_219 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_220 (Batch  (None, 57)          228         ['input_45[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_220 (Dense)              (None, 16)           928         ['batch_normalization_220[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_221 (Batch  (None, 16)          64          ['dense_220[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_176 (LeakyReLU)    (None, 16)           0           ['batch_normalization_221[0][0]']\n",
      "                                                                                                  \n",
      " dense_221 (Dense)              (None, 32)           544         ['leaky_re_lu_176[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_222 (Batch  (None, 32)          128         ['dense_221[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_177 (LeakyReLU)    (None, 32)           0           ['batch_normalization_222[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_177[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_177[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_44 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_223 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_178 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_224 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_179 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.49774976]\n",
      "Beta is  [0.49774976]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.0242 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_220 (Batch  (None, 57)          228         ['input_45[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_220 (Dense)              (None, 16)           928         ['batch_normalization_220[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_221 (Batch  (None, 16)          64          ['dense_220[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_176 (LeakyReLU)    (None, 16)           0           ['batch_normalization_221[0][0]']\n",
      "                                                                                                  \n",
      " dense_221 (Dense)              (None, 32)           544         ['leaky_re_lu_176[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_222 (Batch  (None, 32)          128         ['dense_221[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_177 (LeakyReLU)    (None, 32)           0           ['batch_normalization_222[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_177[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_177[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_44 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_223 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_178 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_224 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_179 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_45 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_220 (Batch  (None, 57)          228         ['input_45[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_220 (Dense)              (None, 16)           928         ['batch_normalization_220[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_221 (Batch  (None, 16)          64          ['dense_220[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_176 (LeakyReLU)    (None, 16)           0           ['batch_normalization_221[0][0]']\n",
      "                                                                                                  \n",
      " dense_221 (Dense)              (None, 32)           544         ['leaky_re_lu_176[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_222 (Batch  (None, 32)          128         ['dense_221[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_177 (LeakyReLU)    (None, 32)           0           ['batch_normalization_222[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_177[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_177[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_44 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_222 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_223 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_178 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_223 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_224 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_179 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_224 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_46 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_225 (Batch  (None, 57)          228         ['input_46[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_225 (Dense)              (None, 16)           928         ['batch_normalization_225[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_226 (Batch  (None, 16)          64          ['dense_225[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_180 (LeakyReLU)    (None, 16)           0           ['batch_normalization_226[0][0]']\n",
      "                                                                                                  \n",
      " dense_226 (Dense)              (None, 32)           544         ['leaky_re_lu_180[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_227 (Batch  (None, 32)          128         ['dense_226[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_181 (LeakyReLU)    (None, 32)           0           ['batch_normalization_227[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_181[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_181[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_45 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_228 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_182 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_228 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_229 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_183 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.52107787]\n",
      "Beta is  [0.52107787]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 1.3090 - reconstruction_loss: 0.8542 - kl_loss: 0.1225 - val_loss: 0.7318 - val_reconstruction_loss: 0.6928 - val_kl_loss: 0.0389 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7168 - reconstruction_loss: 0.6815 - kl_loss: 0.0279 - val_loss: 0.6928 - val_reconstruction_loss: 0.6688 - val_kl_loss: 0.0239 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6922 - reconstruction_loss: 0.6693 - kl_loss: 0.0200 - val_loss: 0.6816 - val_reconstruction_loss: 0.6624 - val_kl_loss: 0.0192 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6830 - reconstruction_loss: 0.6655 - kl_loss: 0.0172 - val_loss: 0.6773 - val_reconstruction_loss: 0.6601 - val_kl_loss: 0.0171 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6811 - reconstruction_loss: 0.6634 - kl_loss: 0.0159 - val_loss: 0.6747 - val_reconstruction_loss: 0.6580 - val_kl_loss: 0.0166 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6750 - reconstruction_loss: 0.6612 - kl_loss: 0.0159 - val_loss: 0.6724 - val_reconstruction_loss: 0.6561 - val_kl_loss: 0.0163 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6743 - reconstruction_loss: 0.6595 - kl_loss: 0.0161 - val_loss: 0.6713 - val_reconstruction_loss: 0.6557 - val_kl_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6723 - reconstruction_loss: 0.6577 - kl_loss: 0.0162 - val_loss: 0.6697 - val_reconstruction_loss: 0.6529 - val_kl_loss: 0.0166 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6741 - reconstruction_loss: 0.6557 - kl_loss: 0.0170 - val_loss: 0.6686 - val_reconstruction_loss: 0.6511 - val_kl_loss: 0.0174 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6696 - reconstruction_loss: 0.6539 - kl_loss: 0.0175 - val_loss: 0.6677 - val_reconstruction_loss: 0.6500 - val_kl_loss: 0.0176 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6708 - reconstruction_loss: 0.6522 - kl_loss: 0.0178 - val_loss: 0.6664 - val_reconstruction_loss: 0.6479 - val_kl_loss: 0.0184 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6670 - reconstruction_loss: 0.6505 - kl_loss: 0.0185 - val_loss: 0.6656 - val_reconstruction_loss: 0.6473 - val_kl_loss: 0.0183 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6708 - reconstruction_loss: 0.6492 - kl_loss: 0.0192 - val_loss: 0.6645 - val_reconstruction_loss: 0.6443 - val_kl_loss: 0.0201 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6697 - reconstruction_loss: 0.6475 - kl_loss: 0.0197 - val_loss: 0.6640 - val_reconstruction_loss: 0.6440 - val_kl_loss: 0.0199 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6677 - reconstruction_loss: 0.6460 - kl_loss: 0.0205 - val_loss: 0.6632 - val_reconstruction_loss: 0.6416 - val_kl_loss: 0.0215 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6658 - reconstruction_loss: 0.6441 - kl_loss: 0.0216 - val_loss: 0.6624 - val_reconstruction_loss: 0.6395 - val_kl_loss: 0.0228 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6641 - reconstruction_loss: 0.6418 - kl_loss: 0.0232 - val_loss: 0.6617 - val_reconstruction_loss: 0.6377 - val_kl_loss: 0.0239 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6636 - reconstruction_loss: 0.6401 - kl_loss: 0.0241 - val_loss: 0.6622 - val_reconstruction_loss: 0.6375 - val_kl_loss: 0.0246 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6623 - reconstruction_loss: 0.6391 - kl_loss: 0.0249 - val_loss: 0.6613 - val_reconstruction_loss: 0.6367 - val_kl_loss: 0.0246 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6633 - reconstruction_loss: 0.6377 - kl_loss: 0.0257 - val_loss: 0.6606 - val_reconstruction_loss: 0.6345 - val_kl_loss: 0.0261 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6633 - reconstruction_loss: 0.6363 - kl_loss: 0.0265 - val_loss: 0.6597 - val_reconstruction_loss: 0.6324 - val_kl_loss: 0.0272 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6624 - reconstruction_loss: 0.6343 - kl_loss: 0.0276 - val_loss: 0.6600 - val_reconstruction_loss: 0.6314 - val_kl_loss: 0.0285 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6634 - reconstruction_loss: 0.6328 - kl_loss: 0.0290 - val_loss: 0.6588 - val_reconstruction_loss: 0.6293 - val_kl_loss: 0.0294 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6603 - reconstruction_loss: 0.6311 - kl_loss: 0.0298 - val_loss: 0.6588 - val_reconstruction_loss: 0.6295 - val_kl_loss: 0.0293 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6603 - reconstruction_loss: 0.6304 - kl_loss: 0.0306\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6603 - reconstruction_loss: 0.6304 - kl_loss: 0.0306 - val_loss: 0.6590 - val_reconstruction_loss: 0.6269 - val_kl_loss: 0.0321 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6617 - reconstruction_loss: 0.6294 - kl_loss: 0.0310 - val_loss: 0.6583 - val_reconstruction_loss: 0.6270 - val_kl_loss: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6584 - reconstruction_loss: 0.6291 - kl_loss: 0.0311 - val_loss: 0.6579 - val_reconstruction_loss: 0.6265 - val_kl_loss: 0.0313 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6596 - reconstruction_loss: 0.6291 - kl_loss: 0.0312 - val_loss: 0.6582 - val_reconstruction_loss: 0.6269 - val_kl_loss: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6609 - reconstruction_loss: 0.6289 - kl_loss: 0.0313\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6609 - reconstruction_loss: 0.6289 - kl_loss: 0.0313 - val_loss: 0.6587 - val_reconstruction_loss: 0.6272 - val_kl_loss: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6593 - reconstruction_loss: 0.6285 - kl_loss: 0.0312 - val_loss: 0.6581 - val_reconstruction_loss: 0.6268 - val_kl_loss: 0.0312 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6593 - reconstruction_loss: 0.6289 - kl_loss: 0.0312 - val_loss: 0.6580 - val_reconstruction_loss: 0.6267 - val_kl_loss: 0.0313 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.6597 - reconstruction_loss: 0.6291 - kl_loss: 0.0313\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6597 - reconstruction_loss: 0.6291 - kl_loss: 0.0313 - val_loss: 0.6583 - val_reconstruction_loss: 0.6270 - val_kl_loss: 0.0313 - lr: 1.0000e-05\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6587 - reconstruction_loss: 0.6285 - kl_loss: 0.0313 - val_loss: 0.6589 - val_reconstruction_loss: 0.6272 - val_kl_loss: 0.0315 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6630 - reconstruction_loss: 0.6287 - kl_loss: 0.0313 - val_loss: 0.6582 - val_reconstruction_loss: 0.6267 - val_kl_loss: 0.0314 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6609 - reconstruction_loss: 0.6289 - kl_loss: 0.0313 - val_loss: 0.6576 - val_reconstruction_loss: 0.6264 - val_kl_loss: 0.0312 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6610 - reconstruction_loss: 0.6285 - kl_loss: 0.0313 - val_loss: 0.6582 - val_reconstruction_loss: 0.6268 - val_kl_loss: 0.0314 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6616 - reconstruction_loss: 0.6287 - kl_loss: 0.0313\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6616 - reconstruction_loss: 0.6287 - kl_loss: 0.0313 - val_loss: 0.6576 - val_reconstruction_loss: 0.6263 - val_kl_loss: 0.0313 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6595 - reconstruction_loss: 0.6292 - kl_loss: 0.0313 - val_loss: 0.6584 - val_reconstruction_loss: 0.6269 - val_kl_loss: 0.0314 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6605 - reconstruction_loss: 0.6286 - kl_loss: 0.0313 - val_loss: 0.6584 - val_reconstruction_loss: 0.6269 - val_kl_loss: 0.0314 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6599 - reconstruction_loss: 0.6285 - kl_loss: 0.0313 - val_loss: 0.6583 - val_reconstruction_loss: 0.6266 - val_kl_loss: 0.0316 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6583 - reconstruction_loss: 0.6286 - kl_loss: 0.0313 - val_loss: 0.6585 - val_reconstruction_loss: 0.6271 - val_kl_loss: 0.0314 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6587 - reconstruction_loss: 0.6286 - kl_loss: 0.0313 - val_loss: 0.6581 - val_reconstruction_loss: 0.6269 - val_kl_loss: 0.0311 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6610 - reconstruction_loss: 0.6290 - kl_loss: 0.0313 - val_loss: 0.6583 - val_reconstruction_loss: 0.6270 - val_kl_loss: 0.0313 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6600 - reconstruction_loss: 0.6287 - kl_loss: 0.0313 - val_loss: 0.6581 - val_reconstruction_loss: 0.6267 - val_kl_loss: 0.0313 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6591 - reconstruction_loss: 0.6286 - kl_loss: 0.0313Restoring model weights from the end of the best epoch: 35.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6591 - reconstruction_loss: 0.6286 - kl_loss: 0.0313 - val_loss: 0.6582 - val_reconstruction_loss: 0.6267 - val_kl_loss: 0.0314 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00045: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_46 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_225 (Batch  (None, 57)          228         ['input_46[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_225 (Dense)              (None, 16)           928         ['batch_normalization_225[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_226 (Batch  (None, 16)          64          ['dense_225[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_180 (LeakyReLU)    (None, 16)           0           ['batch_normalization_226[0][0]']\n",
      "                                                                                                  \n",
      " dense_226 (Dense)              (None, 32)           544         ['leaky_re_lu_180[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_227 (Batch  (None, 32)          128         ['dense_226[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_181 (LeakyReLU)    (None, 32)           0           ['batch_normalization_227[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_181[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_181[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_45 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_228 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_182 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_228 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_229 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_183 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_46 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_225 (Batch  (None, 57)          228         ['input_46[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_225 (Dense)              (None, 16)           928         ['batch_normalization_225[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_226 (Batch  (None, 16)          64          ['dense_225[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_180 (LeakyReLU)    (None, 16)           0           ['batch_normalization_226[0][0]']\n",
      "                                                                                                  \n",
      " dense_226 (Dense)              (None, 32)           544         ['leaky_re_lu_180[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_227 (Batch  (None, 32)          128         ['dense_226[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_181 (LeakyReLU)    (None, 32)           0           ['batch_normalization_227[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_181[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_181[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_45 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_227 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_228 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_182 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_228 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_229 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_183 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_229 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_47 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_230 (Batch  (None, 57)          228         ['input_47[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_230 (Dense)              (None, 16)           928         ['batch_normalization_230[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_231 (Batch  (None, 16)          64          ['dense_230[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_184 (LeakyReLU)    (None, 16)           0           ['batch_normalization_231[0][0]']\n",
      "                                                                                                  \n",
      " dense_231 (Dense)              (None, 64)           1088        ['leaky_re_lu_184[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_232 (Batch  (None, 64)          256         ['dense_231[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_185 (LeakyReLU)    (None, 64)           0           ['batch_normalization_232[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_185[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_185[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_46 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_232 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_233 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_186 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_233 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_234 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_187 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.43846735]\n",
      "Beta is  [0.43846735]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.8390 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_47 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_230 (Batch  (None, 57)          228         ['input_47[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_230 (Dense)              (None, 16)           928         ['batch_normalization_230[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_231 (Batch  (None, 16)          64          ['dense_230[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_184 (LeakyReLU)    (None, 16)           0           ['batch_normalization_231[0][0]']\n",
      "                                                                                                  \n",
      " dense_231 (Dense)              (None, 64)           1088        ['leaky_re_lu_184[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_232 (Batch  (None, 64)          256         ['dense_231[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_185 (LeakyReLU)    (None, 64)           0           ['batch_normalization_232[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_185[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_185[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_46 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_232 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_233 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_186 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_233 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_234 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_187 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_47 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_230 (Batch  (None, 57)          228         ['input_47[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_230 (Dense)              (None, 16)           928         ['batch_normalization_230[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_231 (Batch  (None, 16)          64          ['dense_230[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_184 (LeakyReLU)    (None, 16)           0           ['batch_normalization_231[0][0]']\n",
      "                                                                                                  \n",
      " dense_231 (Dense)              (None, 64)           1088        ['leaky_re_lu_184[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_232 (Batch  (None, 64)          256         ['dense_231[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_185 (LeakyReLU)    (None, 64)           0           ['batch_normalization_232[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_185[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_185[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_46 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_232 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_233 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_186 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_233 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_234 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_187 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_234 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_48 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_235 (Batch  (None, 57)          228         ['input_48[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_235 (Dense)              (None, 16)           928         ['batch_normalization_235[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_236 (Batch  (None, 16)          64          ['dense_235[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_188 (LeakyReLU)    (None, 16)           0           ['batch_normalization_236[0][0]']\n",
      "                                                                                                  \n",
      " dense_236 (Dense)              (None, 64)           1088        ['leaky_re_lu_188[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_237 (Batch  (None, 64)          256         ['dense_236[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_189 (LeakyReLU)    (None, 64)           0           ['batch_normalization_237[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_189[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_189[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_47 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_238 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_190 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_239 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_191 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.43549062]\n",
      "Beta is  [0.43549062]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 2.2640 - reconstruction_loss: 1.7584 - kl_loss: 0.5013  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.7302 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_48 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_235 (Batch  (None, 57)          228         ['input_48[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_235 (Dense)              (None, 16)           928         ['batch_normalization_235[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_236 (Batch  (None, 16)          64          ['dense_235[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_188 (LeakyReLU)    (None, 16)           0           ['batch_normalization_236[0][0]']\n",
      "                                                                                                  \n",
      " dense_236 (Dense)              (None, 64)           1088        ['leaky_re_lu_188[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_237 (Batch  (None, 64)          256         ['dense_236[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_189 (LeakyReLU)    (None, 64)           0           ['batch_normalization_237[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_189[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_189[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_47 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_238 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_190 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_239 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_191 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_48 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_235 (Batch  (None, 57)          228         ['input_48[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_235 (Dense)              (None, 16)           928         ['batch_normalization_235[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_236 (Batch  (None, 16)          64          ['dense_235[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_188 (LeakyReLU)    (None, 16)           0           ['batch_normalization_236[0][0]']\n",
      "                                                                                                  \n",
      " dense_236 (Dense)              (None, 64)           1088        ['leaky_re_lu_188[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_237 (Batch  (None, 64)          256         ['dense_236[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_189 (LeakyReLU)    (None, 64)           0           ['batch_normalization_237[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_189[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_189[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_47 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_237 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_238 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_190 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_238 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_239 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_191 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_239 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_240 (Batch  (None, 57)          228         ['input_49[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_240 (Dense)              (None, 16)           928         ['batch_normalization_240[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_241 (Batch  (None, 16)          64          ['dense_240[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_192 (LeakyReLU)    (None, 16)           0           ['batch_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " dense_241 (Dense)              (None, 32)           544         ['leaky_re_lu_192[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_242 (Batch  (None, 32)          128         ['dense_241[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_193 (LeakyReLU)    (None, 32)           0           ['batch_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_193[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_193[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_48 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_243 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_194 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_244 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_195 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.85401713]\n",
      "Beta is  [0.85401713]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.3805 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_240 (Batch  (None, 57)          228         ['input_49[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_240 (Dense)              (None, 16)           928         ['batch_normalization_240[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_241 (Batch  (None, 16)          64          ['dense_240[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_192 (LeakyReLU)    (None, 16)           0           ['batch_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " dense_241 (Dense)              (None, 32)           544         ['leaky_re_lu_192[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_242 (Batch  (None, 32)          128         ['dense_241[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_193 (LeakyReLU)    (None, 32)           0           ['batch_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_193[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_193[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_48 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_243 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_194 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_244 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_195 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_240 (Batch  (None, 57)          228         ['input_49[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_240 (Dense)              (None, 16)           928         ['batch_normalization_240[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_241 (Batch  (None, 16)          64          ['dense_240[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_192 (LeakyReLU)    (None, 16)           0           ['batch_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " dense_241 (Dense)              (None, 32)           544         ['leaky_re_lu_192[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_242 (Batch  (None, 32)          128         ['dense_241[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_193 (LeakyReLU)    (None, 32)           0           ['batch_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_193[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_193[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_48 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_243 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_194 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_244 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_195 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_244 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_50 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_245 (Batch  (None, 57)          228         ['input_50[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_245 (Dense)              (None, 16)           928         ['batch_normalization_245[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_246 (Batch  (None, 16)          64          ['dense_245[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_196 (LeakyReLU)    (None, 16)           0           ['batch_normalization_246[0][0]']\n",
      "                                                                                                  \n",
      " dense_246 (Dense)              (None, 32)           544         ['leaky_re_lu_196[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_247 (Batch  (None, 32)          128         ['dense_246[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_197 (LeakyReLU)    (None, 32)           0           ['batch_normalization_247[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_197[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_197[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_49 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_248 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_198 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_248 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_249 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_199 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44136956]\n",
      "Beta is  [0.44136956]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.1152 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_50 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_245 (Batch  (None, 57)          228         ['input_50[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_245 (Dense)              (None, 16)           928         ['batch_normalization_245[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_246 (Batch  (None, 16)          64          ['dense_245[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_196 (LeakyReLU)    (None, 16)           0           ['batch_normalization_246[0][0]']\n",
      "                                                                                                  \n",
      " dense_246 (Dense)              (None, 32)           544         ['leaky_re_lu_196[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_247 (Batch  (None, 32)          128         ['dense_246[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_197 (LeakyReLU)    (None, 32)           0           ['batch_normalization_247[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_197[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_197[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_49 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_248 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_198 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_248 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_249 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_199 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_50 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_245 (Batch  (None, 57)          228         ['input_50[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_245 (Dense)              (None, 16)           928         ['batch_normalization_245[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_246 (Batch  (None, 16)          64          ['dense_245[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_196 (LeakyReLU)    (None, 16)           0           ['batch_normalization_246[0][0]']\n",
      "                                                                                                  \n",
      " dense_246 (Dense)              (None, 32)           544         ['leaky_re_lu_196[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_247 (Batch  (None, 32)          128         ['dense_246[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_197 (LeakyReLU)    (None, 32)           0           ['batch_normalization_247[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_197[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_197[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_49 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_248 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_198 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_248 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_249 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_199 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_249 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_51 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_250 (Batch  (None, 57)          228         ['input_51[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_250 (Dense)              (None, 16)           928         ['batch_normalization_250[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_251 (Batch  (None, 16)          64          ['dense_250[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_200 (LeakyReLU)    (None, 16)           0           ['batch_normalization_251[0][0]']\n",
      "                                                                                                  \n",
      " dense_251 (Dense)              (None, 32)           544         ['leaky_re_lu_200[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_252 (Batch  (None, 32)          128         ['dense_251[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_201 (LeakyReLU)    (None, 32)           0           ['batch_normalization_252[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_201[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_201[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_50 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_253 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_202 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_254 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_203 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.84366589]\n",
      "Beta is  [0.84366589]\n",
      "391/391 [==============================] - 17s 37ms/step - loss: 0.7973 - reconstruction_loss: 0.2750 - kl_loss: 0.2230 - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2669 - reconstruction_loss: 0.2263 - kl_loss: 0.0312\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2668 - reconstruction_loss: 0.2263 - kl_loss: 0.0312 - val_loss: inf - val_reconstruction_loss: 0.2327 - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2429 - reconstruction_loss: 0.2247 - kl_loss: 0.0183 - val_loss: inf - val_reconstruction_loss: 0.2364 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2423 - reconstruction_loss: 0.2245 - kl_loss: 0.0166 - val_loss: inf - val_reconstruction_loss: 0.2557 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2395 - reconstruction_loss: 0.2243 - kl_loss: 0.0149\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2395 - reconstruction_loss: 0.2243 - kl_loss: 0.0149 - val_loss: inf - val_reconstruction_loss: 0.2320 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2393 - reconstruction_loss: 0.2242 - kl_loss: 0.0140 - val_loss: inf - val_reconstruction_loss: 0.2339 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2390 - reconstruction_loss: 0.2241 - kl_loss: 0.0138 - val_loss: inf - val_reconstruction_loss: 0.2248 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2368 - reconstruction_loss: 0.2241 - kl_loss: 0.0136\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.2368 - reconstruction_loss: 0.2241 - kl_loss: 0.0136 - val_loss: inf - val_reconstruction_loss: 0.2248 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2374 - reconstruction_loss: 0.2241 - kl_loss: 0.0134 - val_loss: inf - val_reconstruction_loss: 0.2243 - val_kl_loss: inf - lr: 1.0000e-06\n",
      "Epoch 10/150\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.2369 - reconstruction_loss: 0.2241 - kl_loss: 0.0134Restoring model weights from the end of the best epoch: 1.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.2369 - reconstruction_loss: 0.2241 - kl_loss: 0.0134 - val_loss: inf - val_reconstruction_loss: 0.2241 - val_kl_loss: inf - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_51 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_250 (Batch  (None, 57)          228         ['input_51[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_250 (Dense)              (None, 16)           928         ['batch_normalization_250[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_251 (Batch  (None, 16)          64          ['dense_250[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_200 (LeakyReLU)    (None, 16)           0           ['batch_normalization_251[0][0]']\n",
      "                                                                                                  \n",
      " dense_251 (Dense)              (None, 32)           544         ['leaky_re_lu_200[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_252 (Batch  (None, 32)          128         ['dense_251[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_201 (LeakyReLU)    (None, 32)           0           ['batch_normalization_252[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_201[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_201[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_50 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_253 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_202 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_254 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_203 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_51 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_250 (Batch  (None, 57)          228         ['input_51[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_250 (Dense)              (None, 16)           928         ['batch_normalization_250[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_251 (Batch  (None, 16)          64          ['dense_250[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_200 (LeakyReLU)    (None, 16)           0           ['batch_normalization_251[0][0]']\n",
      "                                                                                                  \n",
      " dense_251 (Dense)              (None, 32)           544         ['leaky_re_lu_200[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_252 (Batch  (None, 32)          128         ['dense_251[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_201 (LeakyReLU)    (None, 32)           0           ['batch_normalization_252[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_201[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_201[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_50 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_252 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_253 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_202 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_253 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_254 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_203 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_254 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_52 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_255 (Batch  (None, 57)          228         ['input_52[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_255 (Dense)              (None, 32)           1856        ['batch_normalization_255[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_256 (Batch  (None, 32)          128         ['dense_255[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_204 (LeakyReLU)    (None, 32)           0           ['batch_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " dense_256 (Dense)              (None, 64)           2112        ['leaky_re_lu_204[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_257 (Batch  (None, 64)          256         ['dense_256[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_205 (LeakyReLU)    (None, 64)           0           ['batch_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_205[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_205[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_51 (Sampling)         (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,970\n",
      "Trainable params: 4,664\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_258 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_206 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_259 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_207 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,409\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.10671012]\n",
      "Beta is  [0.10671012]\n",
      "  5/391 [..............................] - ETA: 11s - loss: 3.0641 - reconstruction_loss: 2.8506 - kl_loss: 0.1460Batch 5: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.7342 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_52 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_255 (Batch  (None, 57)          228         ['input_52[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_255 (Dense)              (None, 32)           1856        ['batch_normalization_255[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_256 (Batch  (None, 32)          128         ['dense_255[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_204 (LeakyReLU)    (None, 32)           0           ['batch_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " dense_256 (Dense)              (None, 64)           2112        ['leaky_re_lu_204[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_257 (Batch  (None, 64)          256         ['dense_256[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_205 (LeakyReLU)    (None, 64)           0           ['batch_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_205[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_205[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_51 (Sampling)         (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,970\n",
      "Trainable params: 4,664\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_258 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_206 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_259 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_207 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,409\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_52 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_255 (Batch  (None, 57)          228         ['input_52[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_255 (Dense)              (None, 32)           1856        ['batch_normalization_255[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_256 (Batch  (None, 32)          128         ['dense_255[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_204 (LeakyReLU)    (None, 32)           0           ['batch_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " dense_256 (Dense)              (None, 64)           2112        ['leaky_re_lu_204[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_257 (Batch  (None, 64)          256         ['dense_256[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_205 (LeakyReLU)    (None, 64)           0           ['batch_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            195         ['leaky_re_lu_205[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            195         ['leaky_re_lu_205[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_51 (Sampling)         (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,970\n",
      "Trainable params: 4,664\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_257 (Dense)           (None, 64)                256       \n",
      "                                                                 \n",
      " batch_normalization_258 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_206 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_258 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_259 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_207 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_259 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,601\n",
      "Trainable params: 4,409\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_53 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_260 (Batch  (None, 57)          228         ['input_53[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_260 (Dense)              (None, 16)           928         ['batch_normalization_260[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_261 (Batch  (None, 16)          64          ['dense_260[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_208 (LeakyReLU)    (None, 16)           0           ['batch_normalization_261[0][0]']\n",
      "                                                                                                  \n",
      " dense_261 (Dense)              (None, 32)           544         ['leaky_re_lu_208[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_262 (Batch  (None, 32)          128         ['dense_261[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_209 (LeakyReLU)    (None, 32)           0           ['batch_normalization_262[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_209[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_209[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_52 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_263 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_210 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_264 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_211 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.43974469]\n",
      "Beta is  [0.43974469]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6491 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_53 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_260 (Batch  (None, 57)          228         ['input_53[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_260 (Dense)              (None, 16)           928         ['batch_normalization_260[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_261 (Batch  (None, 16)          64          ['dense_260[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_208 (LeakyReLU)    (None, 16)           0           ['batch_normalization_261[0][0]']\n",
      "                                                                                                  \n",
      " dense_261 (Dense)              (None, 32)           544         ['leaky_re_lu_208[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_262 (Batch  (None, 32)          128         ['dense_261[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_209 (LeakyReLU)    (None, 32)           0           ['batch_normalization_262[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_209[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_209[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_52 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_263 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_210 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_264 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_211 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_53 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_260 (Batch  (None, 57)          228         ['input_53[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_260 (Dense)              (None, 16)           928         ['batch_normalization_260[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_261 (Batch  (None, 16)          64          ['dense_260[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_208 (LeakyReLU)    (None, 16)           0           ['batch_normalization_261[0][0]']\n",
      "                                                                                                  \n",
      " dense_261 (Dense)              (None, 32)           544         ['leaky_re_lu_208[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_262 (Batch  (None, 32)          128         ['dense_261[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_209 (LeakyReLU)    (None, 32)           0           ['batch_normalization_262[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_209[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_209[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_52 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_262 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_263 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_210 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_263 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_264 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_211 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_264 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_54 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_265 (Batch  (None, 57)          228         ['input_54[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_265 (Dense)              (None, 32)           1856        ['batch_normalization_265[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_266 (Batch  (None, 32)          128         ['dense_265[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_212 (LeakyReLU)    (None, 32)           0           ['batch_normalization_266[0][0]']\n",
      "                                                                                                  \n",
      " dense_266 (Dense)              (None, 64)           2112        ['leaky_re_lu_212[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_267 (Batch  (None, 64)          256         ['dense_266[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_213 (LeakyReLU)    (None, 64)           0           ['batch_normalization_267[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_213[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_213[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_53 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,100\n",
      "Trainable params: 4,794\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_268 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_214 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_269 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_215 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,665\n",
      "Trainable params: 4,473\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.56654467]\n",
      "Beta is  [0.56654467]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 0.9616 - reconstruction_loss: 0.7028 - kl_loss: 0.0471 - val_loss: 0.6324 - val_reconstruction_loss: 0.6201 - val_kl_loss: 0.0122 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6324 - reconstruction_loss: 0.6169 - kl_loss: 0.0083 - val_loss: 0.6180 - val_reconstruction_loss: 0.6112 - val_kl_loss: 0.0067 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6185 - reconstruction_loss: 0.6140 - kl_loss: 0.0050 - val_loss: 0.6143 - val_reconstruction_loss: 0.6101 - val_kl_loss: 0.0041 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6153 - reconstruction_loss: 0.6122 - kl_loss: 0.0040 - val_loss: 0.6127 - val_reconstruction_loss: 0.6075 - val_kl_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6164 - reconstruction_loss: 0.6114 - kl_loss: 0.0033 - val_loss: 0.6113 - val_reconstruction_loss: 0.6082 - val_kl_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6178 - reconstruction_loss: 0.6111 - kl_loss: 0.0030 - val_loss: 0.6108 - val_reconstruction_loss: 0.6072 - val_kl_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6151 - reconstruction_loss: 0.6107 - kl_loss: 0.0028 - val_loss: 0.6104 - val_reconstruction_loss: 0.6078 - val_kl_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6131 - reconstruction_loss: 0.6105 - kl_loss: 0.0027 - val_loss: 0.6101 - val_reconstruction_loss: 0.6073 - val_kl_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6119 - reconstruction_loss: 0.6099 - kl_loss: 0.0028 - val_loss: 0.6098 - val_reconstruction_loss: 0.6071 - val_kl_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 13s 35ms/step - loss: 0.6117 - reconstruction_loss: 0.6099 - kl_loss: 0.0026 - val_loss: 0.6099 - val_reconstruction_loss: 0.6069 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6123 - reconstruction_loss: 0.6097 - kl_loss: 0.0027 - val_loss: 0.6093 - val_reconstruction_loss: 0.6062 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6126 - reconstruction_loss: 0.6094 - kl_loss: 0.0026 - val_loss: 0.6094 - val_reconstruction_loss: 0.6063 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6115 - reconstruction_loss: 0.6093 - kl_loss: 0.0026\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6115 - reconstruction_loss: 0.6093 - kl_loss: 0.0026 - val_loss: 0.6094 - val_reconstruction_loss: 0.6066 - val_kl_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6114 - reconstruction_loss: 0.6085 - kl_loss: 0.0023 - val_loss: 0.6085 - val_reconstruction_loss: 0.6055 - val_kl_loss: 0.0029 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6114 - reconstruction_loss: 0.6084 - kl_loss: 0.0025 - val_loss: 0.6085 - val_reconstruction_loss: 0.6053 - val_kl_loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6104 - reconstruction_loss: 0.6083 - kl_loss: 0.0023\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6104 - reconstruction_loss: 0.6083 - kl_loss: 0.0023 - val_loss: 0.6084 - val_reconstruction_loss: 0.6056 - val_kl_loss: 0.0028 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6104 - reconstruction_loss: 0.6082 - kl_loss: 0.0026 - val_loss: 0.6084 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6103 - reconstruction_loss: 0.6083 - kl_loss: 0.0025 - val_loss: 0.6086 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0031 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6101 - reconstruction_loss: 0.6082 - kl_loss: 0.0025\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6101 - reconstruction_loss: 0.6082 - kl_loss: 0.0025 - val_loss: 0.6086 - val_reconstruction_loss: 0.6053 - val_kl_loss: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6117 - reconstruction_loss: 0.6080 - kl_loss: 0.0025 - val_loss: 0.6087 - val_reconstruction_loss: 0.6052 - val_kl_loss: 0.0034 - lr: 1.0000e-06\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6109 - reconstruction_loss: 0.6082 - kl_loss: 0.0025 - val_loss: 0.6083 - val_reconstruction_loss: 0.6052 - val_kl_loss: 0.0031 - lr: 1.0000e-06\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6109 - reconstruction_loss: 0.6081 - kl_loss: 0.0025 - val_loss: 0.6085 - val_reconstruction_loss: 0.6053 - val_kl_loss: 0.0031 - lr: 1.0000e-06\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6106 - reconstruction_loss: 0.6081 - kl_loss: 0.0025\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6106 - reconstruction_loss: 0.6081 - kl_loss: 0.0025 - val_loss: 0.6083 - val_reconstruction_loss: 0.6052 - val_kl_loss: 0.0031 - lr: 1.0000e-06\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6101 - reconstruction_loss: 0.6081 - kl_loss: 0.0025 - val_loss: 0.6085 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0030 - lr: 1.0000e-06\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6113 - reconstruction_loss: 0.6085 - kl_loss: 0.0024 - val_loss: 0.6083 - val_reconstruction_loss: 0.6053 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6093 - reconstruction_loss: 0.6081 - kl_loss: 0.0024 - val_loss: 0.6084 - val_reconstruction_loss: 0.6056 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6105 - reconstruction_loss: 0.6084 - kl_loss: 0.0024 - val_loss: 0.6083 - val_reconstruction_loss: 0.6053 - val_kl_loss: 0.0030 - lr: 1.0000e-06\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6104 - reconstruction_loss: 0.6082 - kl_loss: 0.0025 - val_loss: 0.6084 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6101 - reconstruction_loss: 0.6081 - kl_loss: 0.0024 - val_loss: 0.6084 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0030 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6116 - reconstruction_loss: 0.6081 - kl_loss: 0.0024 - val_loss: 0.6084 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6113 - reconstruction_loss: 0.6083 - kl_loss: 0.0025 - val_loss: 0.6084 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6100 - reconstruction_loss: 0.6080 - kl_loss: 0.0025 - val_loss: 0.6085 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0030 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6110 - reconstruction_loss: 0.6082 - kl_loss: 0.0024Restoring model weights from the end of the best epoch: 23.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6110 - reconstruction_loss: 0.6082 - kl_loss: 0.0024 - val_loss: 0.6084 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0030 - lr: 1.0000e-06\n",
      "Epoch 00033: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_54 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_265 (Batch  (None, 57)          228         ['input_54[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_265 (Dense)              (None, 32)           1856        ['batch_normalization_265[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_266 (Batch  (None, 32)          128         ['dense_265[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_212 (LeakyReLU)    (None, 32)           0           ['batch_normalization_266[0][0]']\n",
      "                                                                                                  \n",
      " dense_266 (Dense)              (None, 64)           2112        ['leaky_re_lu_212[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_267 (Batch  (None, 64)          256         ['dense_266[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_213 (LeakyReLU)    (None, 64)           0           ['batch_normalization_267[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_213[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_213[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_53 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,100\n",
      "Trainable params: 4,794\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_268 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_214 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_269 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_215 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,665\n",
      "Trainable params: 4,473\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_54 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_265 (Batch  (None, 57)          228         ['input_54[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_265 (Dense)              (None, 32)           1856        ['batch_normalization_265[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_266 (Batch  (None, 32)          128         ['dense_265[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_212 (LeakyReLU)    (None, 32)           0           ['batch_normalization_266[0][0]']\n",
      "                                                                                                  \n",
      " dense_266 (Dense)              (None, 64)           2112        ['leaky_re_lu_212[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_267 (Batch  (None, 64)          256         ['dense_266[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_213 (LeakyReLU)    (None, 64)           0           ['batch_normalization_267[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            260         ['leaky_re_lu_213[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            260         ['leaky_re_lu_213[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_53 (Sampling)         (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,100\n",
      "Trainable params: 4,794\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_267 (Dense)           (None, 64)                320       \n",
      "                                                                 \n",
      " batch_normalization_268 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_214 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_268 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_269 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_215 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_269 (Dense)           (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,665\n",
      "Trainable params: 4,473\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_270 (Batch  (None, 57)          228         ['input_55[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_270 (Dense)              (None, 16)           928         ['batch_normalization_270[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_271 (Batch  (None, 16)          64          ['dense_270[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_216 (LeakyReLU)    (None, 16)           0           ['batch_normalization_271[0][0]']\n",
      "                                                                                                  \n",
      " dense_271 (Dense)              (None, 32)           544         ['leaky_re_lu_216[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_272 (Batch  (None, 32)          128         ['dense_271[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_217 (LeakyReLU)    (None, 32)           0           ['batch_normalization_272[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_217[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_217[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_54 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_273 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_218 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_274 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_219 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.49038291]\n",
      "Beta is  [0.49038291]\n",
      "  3/391 [..............................] - ETA: 12s - loss: 2.0278 - reconstruction_loss: 1.6578 - kl_loss: 0.3561  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4858 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_270 (Batch  (None, 57)          228         ['input_55[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_270 (Dense)              (None, 16)           928         ['batch_normalization_270[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_271 (Batch  (None, 16)          64          ['dense_270[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_216 (LeakyReLU)    (None, 16)           0           ['batch_normalization_271[0][0]']\n",
      "                                                                                                  \n",
      " dense_271 (Dense)              (None, 32)           544         ['leaky_re_lu_216[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_272 (Batch  (None, 32)          128         ['dense_271[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_217 (LeakyReLU)    (None, 32)           0           ['batch_normalization_272[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_217[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_217[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_54 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_273 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_218 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_274 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_219 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_55 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_270 (Batch  (None, 57)          228         ['input_55[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_270 (Dense)              (None, 16)           928         ['batch_normalization_270[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_271 (Batch  (None, 16)          64          ['dense_270[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_216 (LeakyReLU)    (None, 16)           0           ['batch_normalization_271[0][0]']\n",
      "                                                                                                  \n",
      " dense_271 (Dense)              (None, 32)           544         ['leaky_re_lu_216[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_272 (Batch  (None, 32)          128         ['dense_271[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_217 (LeakyReLU)    (None, 32)           0           ['batch_normalization_272[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_217[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_217[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_54 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_272 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_273 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_218 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_273 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_274 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_219 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_274 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_56 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_275 (Batch  (None, 57)          228         ['input_56[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_275 (Dense)              (None, 16)           928         ['batch_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_276 (Batch  (None, 16)          64          ['dense_275[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_220 (LeakyReLU)    (None, 16)           0           ['batch_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " dense_276 (Dense)              (None, 32)           544         ['leaky_re_lu_220[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_277 (Batch  (None, 32)          128         ['dense_276[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_221 (LeakyReLU)    (None, 32)           0           ['batch_normalization_277[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_221[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_221[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_55 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_277 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_278 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_222 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_279 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_223 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44309871]\n",
      "Beta is  [0.44309871]\n",
      "  1/391 [..............................] - ETA: 18:26 - loss: 3.2793 - reconstruction_loss: 1.7319 - kl_loss: 1.5474Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 5s 5ms/step - loss: inf - reconstruction_loss: 1.4718 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_56 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_275 (Batch  (None, 57)          228         ['input_56[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_275 (Dense)              (None, 16)           928         ['batch_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_276 (Batch  (None, 16)          64          ['dense_275[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_220 (LeakyReLU)    (None, 16)           0           ['batch_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " dense_276 (Dense)              (None, 32)           544         ['leaky_re_lu_220[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_277 (Batch  (None, 32)          128         ['dense_276[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_221 (LeakyReLU)    (None, 32)           0           ['batch_normalization_277[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_221[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_221[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_55 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_277 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_278 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_222 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_279 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_223 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_56 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_275 (Batch  (None, 57)          228         ['input_56[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_275 (Dense)              (None, 16)           928         ['batch_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_276 (Batch  (None, 16)          64          ['dense_275[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_220 (LeakyReLU)    (None, 16)           0           ['batch_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " dense_276 (Dense)              (None, 32)           544         ['leaky_re_lu_220[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_277 (Batch  (None, 32)          128         ['dense_276[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_221 (LeakyReLU)    (None, 32)           0           ['batch_normalization_277[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_221[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_221[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_55 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_277 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_278 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_222 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_278 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_279 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_223 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_279 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_57 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_280 (Batch  (None, 57)          228         ['input_57[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_280 (Dense)              (None, 64)           3712        ['batch_normalization_280[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_281 (Batch  (None, 64)          256         ['dense_280[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_224 (LeakyReLU)    (None, 64)           0           ['batch_normalization_281[0][0]']\n",
      "                                                                                                  \n",
      " dense_281 (Dense)              (None, 32)           2080        ['leaky_re_lu_224[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_282 (Batch  (None, 32)          128         ['dense_281[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_225 (LeakyReLU)    (None, 32)           0           ['batch_normalization_282[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_225[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_225[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_56 (Sampling)         (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,602\n",
      "Trainable params: 6,296\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_283 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_226 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_284 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_227 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,329\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.43934402]\n",
      "Beta is  [0.43934402]\n",
      "  3/391 [..............................] - ETA: 14s - loss: 4.0489 - reconstruction_loss: 1.6535 - kl_loss: 1.9594  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6493 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_57 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_280 (Batch  (None, 57)          228         ['input_57[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_280 (Dense)              (None, 64)           3712        ['batch_normalization_280[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_281 (Batch  (None, 64)          256         ['dense_280[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_224 (LeakyReLU)    (None, 64)           0           ['batch_normalization_281[0][0]']\n",
      "                                                                                                  \n",
      " dense_281 (Dense)              (None, 32)           2080        ['leaky_re_lu_224[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_282 (Batch  (None, 32)          128         ['dense_281[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_225 (LeakyReLU)    (None, 32)           0           ['batch_normalization_282[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_225[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_225[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_56 (Sampling)         (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,602\n",
      "Trainable params: 6,296\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_283 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_226 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_284 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_227 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,329\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_57 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_280 (Batch  (None, 57)          228         ['input_57[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_280 (Dense)              (None, 64)           3712        ['batch_normalization_280[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_281 (Batch  (None, 64)          256         ['dense_280[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_224 (LeakyReLU)    (None, 64)           0           ['batch_normalization_281[0][0]']\n",
      "                                                                                                  \n",
      " dense_281 (Dense)              (None, 32)           2080        ['leaky_re_lu_224[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_282 (Batch  (None, 32)          128         ['dense_281[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_225 (LeakyReLU)    (None, 32)           0           ['batch_normalization_282[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            99          ['leaky_re_lu_225[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            99          ['leaky_re_lu_225[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_56 (Sampling)         (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,602\n",
      "Trainable params: 6,296\n",
      "Non-trainable params: 306\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_282 (Dense)           (None, 32)                128       \n",
      "                                                                 \n",
      " batch_normalization_283 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_226 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_283 (Dense)           (None, 64)                2112      \n",
      "                                                                 \n",
      " batch_normalization_284 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_227 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_284 (Dense)           (None, 57)                3705      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,329\n",
      "Trainable params: 6,137\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_58 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_285 (Batch  (None, 57)          228         ['input_58[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_285 (Dense)              (None, 16)           928         ['batch_normalization_285[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_286 (Batch  (None, 16)          64          ['dense_285[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_228 (LeakyReLU)    (None, 16)           0           ['batch_normalization_286[0][0]']\n",
      "                                                                                                  \n",
      " dense_286 (Dense)              (None, 32)           544         ['leaky_re_lu_228[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_287 (Batch  (None, 32)          128         ['dense_286[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_229 (LeakyReLU)    (None, 32)           0           ['batch_normalization_287[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_229[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_229[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_57 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_288 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_230 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_288 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_289 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_231 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.29634918]\n",
      "Beta is  [0.29634918]\n",
      "  1/391 [..............................] - ETA: 19:02 - loss: 3.6315 - reconstruction_loss: 1.6462 - kl_loss: 1.9853Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5330 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_58 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_285 (Batch  (None, 57)          228         ['input_58[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_285 (Dense)              (None, 16)           928         ['batch_normalization_285[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_286 (Batch  (None, 16)          64          ['dense_285[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_228 (LeakyReLU)    (None, 16)           0           ['batch_normalization_286[0][0]']\n",
      "                                                                                                  \n",
      " dense_286 (Dense)              (None, 32)           544         ['leaky_re_lu_228[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_287 (Batch  (None, 32)          128         ['dense_286[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_229 (LeakyReLU)    (None, 32)           0           ['batch_normalization_287[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_229[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_229[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_57 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_288 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_230 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_288 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_289 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_231 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_58 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_285 (Batch  (None, 57)          228         ['input_58[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_285 (Dense)              (None, 16)           928         ['batch_normalization_285[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_286 (Batch  (None, 16)          64          ['dense_285[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_228 (LeakyReLU)    (None, 16)           0           ['batch_normalization_286[0][0]']\n",
      "                                                                                                  \n",
      " dense_286 (Dense)              (None, 32)           544         ['leaky_re_lu_228[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_287 (Batch  (None, 32)          128         ['dense_286[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_229 (LeakyReLU)    (None, 32)           0           ['batch_normalization_287[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_229[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_229[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_57 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_287 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_288 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_230 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_288 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_289 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_231 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_289 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_59 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_290 (Batch  (None, 57)          228         ['input_59[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_290 (Dense)              (None, 16)           928         ['batch_normalization_290[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_291 (Batch  (None, 16)          64          ['dense_290[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_232 (LeakyReLU)    (None, 16)           0           ['batch_normalization_291[0][0]']\n",
      "                                                                                                  \n",
      " dense_291 (Dense)              (None, 32)           544         ['leaky_re_lu_232[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_292 (Batch  (None, 32)          128         ['dense_291[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_233 (LeakyReLU)    (None, 32)           0           ['batch_normalization_292[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_233[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_233[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_58 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_293 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_234 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_293 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_294 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_235 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_294 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.69129464]\n",
      "Beta is  [0.69129464]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.7378 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_59 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_290 (Batch  (None, 57)          228         ['input_59[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_290 (Dense)              (None, 16)           928         ['batch_normalization_290[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_291 (Batch  (None, 16)          64          ['dense_290[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_232 (LeakyReLU)    (None, 16)           0           ['batch_normalization_291[0][0]']\n",
      "                                                                                                  \n",
      " dense_291 (Dense)              (None, 32)           544         ['leaky_re_lu_232[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_292 (Batch  (None, 32)          128         ['dense_291[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_233 (LeakyReLU)    (None, 32)           0           ['batch_normalization_292[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_233[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_233[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_58 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_293 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_234 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_293 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_294 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_235 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_294 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_59 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_290 (Batch  (None, 57)          228         ['input_59[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_290 (Dense)              (None, 16)           928         ['batch_normalization_290[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_291 (Batch  (None, 16)          64          ['dense_290[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_232 (LeakyReLU)    (None, 16)           0           ['batch_normalization_291[0][0]']\n",
      "                                                                                                  \n",
      " dense_291 (Dense)              (None, 32)           544         ['leaky_re_lu_232[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_292 (Batch  (None, 32)          128         ['dense_291[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_233 (LeakyReLU)    (None, 32)           0           ['batch_normalization_292[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_233[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_233[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_58 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_292 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_293 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_234 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_293 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_294 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_235 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_294 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_60 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_295 (Batch  (None, 57)          228         ['input_60[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_295 (Dense)              (None, 16)           928         ['batch_normalization_295[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_296 (Batch  (None, 16)          64          ['dense_295[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_236 (LeakyReLU)    (None, 16)           0           ['batch_normalization_296[0][0]']\n",
      "                                                                                                  \n",
      " dense_296 (Dense)              (None, 64)           1088        ['leaky_re_lu_236[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_297 (Batch  (None, 64)          256         ['dense_296[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_237 (LeakyReLU)    (None, 64)           0           ['batch_normalization_297[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_237[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_237[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_59 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_298 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_238 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_299 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_239 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44245388]\n",
      "Beta is  [0.44245388]\n",
      "  1/391 [..............................] - ETA: 17:35 - loss: 2.5015 - reconstruction_loss: 1.6434 - kl_loss: 0.8581Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6977 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_60 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_295 (Batch  (None, 57)          228         ['input_60[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_295 (Dense)              (None, 16)           928         ['batch_normalization_295[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_296 (Batch  (None, 16)          64          ['dense_295[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_236 (LeakyReLU)    (None, 16)           0           ['batch_normalization_296[0][0]']\n",
      "                                                                                                  \n",
      " dense_296 (Dense)              (None, 64)           1088        ['leaky_re_lu_236[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_297 (Batch  (None, 64)          256         ['dense_296[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_237 (LeakyReLU)    (None, 64)           0           ['batch_normalization_297[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_237[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_237[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_59 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_298 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_238 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_299 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_239 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_60 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_295 (Batch  (None, 57)          228         ['input_60[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_295 (Dense)              (None, 16)           928         ['batch_normalization_295[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_296 (Batch  (None, 16)          64          ['dense_295[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_236 (LeakyReLU)    (None, 16)           0           ['batch_normalization_296[0][0]']\n",
      "                                                                                                  \n",
      " dense_296 (Dense)              (None, 64)           1088        ['leaky_re_lu_236[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_297 (Batch  (None, 64)          256         ['dense_296[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_237 (LeakyReLU)    (None, 64)           0           ['batch_normalization_297[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_237[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_237[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_59 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_298 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_238 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_299 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_239 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_61 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_300 (Batch  (None, 57)          228         ['input_61[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_300 (Dense)              (None, 16)           928         ['batch_normalization_300[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_301 (Batch  (None, 16)          64          ['dense_300[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_240 (LeakyReLU)    (None, 16)           0           ['batch_normalization_301[0][0]']\n",
      "                                                                                                  \n",
      " dense_301 (Dense)              (None, 32)           544         ['leaky_re_lu_240[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_302 (Batch  (None, 32)          128         ['dense_301[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_241 (LeakyReLU)    (None, 32)           0           ['batch_normalization_302[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_241[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_241[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_60 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_303 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_242 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_304 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_243 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.26287386]\n",
      "Beta is  [0.26287386]\n",
      "  7/391 [..............................] - ETA: 12s - loss: 2.6129 - reconstruction_loss: 2.2026 - kl_loss: 0.4004Batch 7: Invalid loss, terminating training\n",
      "391/391 [==============================] - 5s 4ms/step - loss: inf - reconstruction_loss: 2.1841 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_61 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_300 (Batch  (None, 57)          228         ['input_61[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_300 (Dense)              (None, 16)           928         ['batch_normalization_300[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_301 (Batch  (None, 16)          64          ['dense_300[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_240 (LeakyReLU)    (None, 16)           0           ['batch_normalization_301[0][0]']\n",
      "                                                                                                  \n",
      " dense_301 (Dense)              (None, 32)           544         ['leaky_re_lu_240[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_302 (Batch  (None, 32)          128         ['dense_301[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_241 (LeakyReLU)    (None, 32)           0           ['batch_normalization_302[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_241[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_241[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_60 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_303 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_242 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_304 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_243 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_61 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_300 (Batch  (None, 57)          228         ['input_61[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_300 (Dense)              (None, 16)           928         ['batch_normalization_300[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_301 (Batch  (None, 16)          64          ['dense_300[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_240 (LeakyReLU)    (None, 16)           0           ['batch_normalization_301[0][0]']\n",
      "                                                                                                  \n",
      " dense_301 (Dense)              (None, 32)           544         ['leaky_re_lu_240[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_302 (Batch  (None, 32)          128         ['dense_301[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_241 (LeakyReLU)    (None, 32)           0           ['batch_normalization_302[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_241[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_241[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_60 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_303 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_242 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_303 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_304 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_243 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_62 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_305 (Batch  (None, 57)          228         ['input_62[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_305 (Dense)              (None, 16)           928         ['batch_normalization_305[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_306 (Batch  (None, 16)          64          ['dense_305[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_244 (LeakyReLU)    (None, 16)           0           ['batch_normalization_306[0][0]']\n",
      "                                                                                                  \n",
      " dense_306 (Dense)              (None, 32)           544         ['leaky_re_lu_244[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_307 (Batch  (None, 32)          128         ['dense_306[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_245 (LeakyReLU)    (None, 32)           0           ['batch_normalization_307[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_245[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_245[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_61 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_308 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_246 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_308 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_309 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_247 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.50110954]\n",
      "Beta is  [0.50110954]\n",
      "  1/391 [..............................] - ETA: 17:44 - loss: 2.0796 - reconstruction_loss: 1.5159 - kl_loss: 0.5637Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4411 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_62 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_305 (Batch  (None, 57)          228         ['input_62[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_305 (Dense)              (None, 16)           928         ['batch_normalization_305[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_306 (Batch  (None, 16)          64          ['dense_305[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_244 (LeakyReLU)    (None, 16)           0           ['batch_normalization_306[0][0]']\n",
      "                                                                                                  \n",
      " dense_306 (Dense)              (None, 32)           544         ['leaky_re_lu_244[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_307 (Batch  (None, 32)          128         ['dense_306[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_245 (LeakyReLU)    (None, 32)           0           ['batch_normalization_307[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_245[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_245[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_61 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_308 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_246 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_308 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_309 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_247 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_62 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_305 (Batch  (None, 57)          228         ['input_62[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_305 (Dense)              (None, 16)           928         ['batch_normalization_305[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_306 (Batch  (None, 16)          64          ['dense_305[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_244 (LeakyReLU)    (None, 16)           0           ['batch_normalization_306[0][0]']\n",
      "                                                                                                  \n",
      " dense_306 (Dense)              (None, 32)           544         ['leaky_re_lu_244[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_307 (Batch  (None, 32)          128         ['dense_306[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_245 (LeakyReLU)    (None, 32)           0           ['batch_normalization_307[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_245[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_245[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_61 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_308 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_246 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_308 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_309 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_247 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_63 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_310 (Batch  (None, 57)          228         ['input_63[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_310 (Dense)              (None, 16)           928         ['batch_normalization_310[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_311 (Batch  (None, 16)          64          ['dense_310[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_248 (LeakyReLU)    (None, 16)           0           ['batch_normalization_311[0][0]']\n",
      "                                                                                                  \n",
      " dense_311 (Dense)              (None, 32)           544         ['leaky_re_lu_248[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_312 (Batch  (None, 32)          128         ['dense_311[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_249 (LeakyReLU)    (None, 32)           0           ['batch_normalization_312[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_249[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_249[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_62 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_313 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_250 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_314 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_251 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.43627018]\n",
      "Beta is  [0.43627018]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.8261 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_63 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_310 (Batch  (None, 57)          228         ['input_63[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_310 (Dense)              (None, 16)           928         ['batch_normalization_310[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_311 (Batch  (None, 16)          64          ['dense_310[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_248 (LeakyReLU)    (None, 16)           0           ['batch_normalization_311[0][0]']\n",
      "                                                                                                  \n",
      " dense_311 (Dense)              (None, 32)           544         ['leaky_re_lu_248[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_312 (Batch  (None, 32)          128         ['dense_311[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_249 (LeakyReLU)    (None, 32)           0           ['batch_normalization_312[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_249[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_249[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_62 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_313 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_250 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_314 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_251 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_63 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_310 (Batch  (None, 57)          228         ['input_63[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_310 (Dense)              (None, 16)           928         ['batch_normalization_310[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_311 (Batch  (None, 16)          64          ['dense_310[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_248 (LeakyReLU)    (None, 16)           0           ['batch_normalization_311[0][0]']\n",
      "                                                                                                  \n",
      " dense_311 (Dense)              (None, 32)           544         ['leaky_re_lu_248[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_312 (Batch  (None, 32)          128         ['dense_311[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_249 (LeakyReLU)    (None, 32)           0           ['batch_normalization_312[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_249[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_249[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_62 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_313 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_250 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_314 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_251 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_64 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_315 (Batch  (None, 57)          228         ['input_64[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_315 (Dense)              (None, 16)           928         ['batch_normalization_315[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_316 (Batch  (None, 16)          64          ['dense_315[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_252 (LeakyReLU)    (None, 16)           0           ['batch_normalization_316[0][0]']\n",
      "                                                                                                  \n",
      " dense_316 (Dense)              (None, 64)           1088        ['leaky_re_lu_252[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_317 (Batch  (None, 64)          256         ['dense_316[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_253 (LeakyReLU)    (None, 64)           0           ['batch_normalization_317[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_253[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_253[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_63 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_318 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_254 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_319 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_255 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.49262165]\n",
      "Beta is  [0.49262165]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.2463 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_64 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_315 (Batch  (None, 57)          228         ['input_64[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_315 (Dense)              (None, 16)           928         ['batch_normalization_315[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_316 (Batch  (None, 16)          64          ['dense_315[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_252 (LeakyReLU)    (None, 16)           0           ['batch_normalization_316[0][0]']\n",
      "                                                                                                  \n",
      " dense_316 (Dense)              (None, 64)           1088        ['leaky_re_lu_252[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_317 (Batch  (None, 64)          256         ['dense_316[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_253 (LeakyReLU)    (None, 64)           0           ['batch_normalization_317[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_253[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_253[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_63 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_318 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_254 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_319 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_255 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_64 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_315 (Batch  (None, 57)          228         ['input_64[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_315 (Dense)              (None, 16)           928         ['batch_normalization_315[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_316 (Batch  (None, 16)          64          ['dense_315[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_252 (LeakyReLU)    (None, 16)           0           ['batch_normalization_316[0][0]']\n",
      "                                                                                                  \n",
      " dense_316 (Dense)              (None, 64)           1088        ['leaky_re_lu_252[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_317 (Batch  (None, 64)          256         ['dense_316[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_253 (LeakyReLU)    (None, 64)           0           ['batch_normalization_317[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_253[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_253[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_63 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_318 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_254 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_319 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_255 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_65 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_320 (Batch  (None, 57)          228         ['input_65[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_320 (Dense)              (None, 16)           928         ['batch_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_321 (Batch  (None, 16)          64          ['dense_320[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_256 (LeakyReLU)    (None, 16)           0           ['batch_normalization_321[0][0]']\n",
      "                                                                                                  \n",
      " dense_321 (Dense)              (None, 32)           544         ['leaky_re_lu_256[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_322 (Batch  (None, 32)          128         ['dense_321[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_257 (LeakyReLU)    (None, 32)           0           ['batch_normalization_322[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_257[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_257[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_64 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_323 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_258 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_324 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_259 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_324 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.49628772]\n",
      "Beta is  [0.49628772]\n",
      "  1/391 [..............................] - ETA: 16:53 - loss: 2.2824 - reconstruction_loss: 1.7203 - kl_loss: 0.5622Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4244 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_65 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_320 (Batch  (None, 57)          228         ['input_65[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_320 (Dense)              (None, 16)           928         ['batch_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_321 (Batch  (None, 16)          64          ['dense_320[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_256 (LeakyReLU)    (None, 16)           0           ['batch_normalization_321[0][0]']\n",
      "                                                                                                  \n",
      " dense_321 (Dense)              (None, 32)           544         ['leaky_re_lu_256[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_322 (Batch  (None, 32)          128         ['dense_321[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_257 (LeakyReLU)    (None, 32)           0           ['batch_normalization_322[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_257[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_257[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_64 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_323 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_258 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_324 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_259 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_324 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_65 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_320 (Batch  (None, 57)          228         ['input_65[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_320 (Dense)              (None, 16)           928         ['batch_normalization_320[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_321 (Batch  (None, 16)          64          ['dense_320[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_256 (LeakyReLU)    (None, 16)           0           ['batch_normalization_321[0][0]']\n",
      "                                                                                                  \n",
      " dense_321 (Dense)              (None, 32)           544         ['leaky_re_lu_256[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_322 (Batch  (None, 32)          128         ['dense_321[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_257 (LeakyReLU)    (None, 32)           0           ['batch_normalization_322[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_257[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_257[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_64 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_323 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_258 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_324 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_259 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_324 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_66 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_325 (Batch  (None, 57)          228         ['input_66[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_325 (Dense)              (None, 16)           928         ['batch_normalization_325[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_326 (Batch  (None, 16)          64          ['dense_325[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_260 (LeakyReLU)    (None, 16)           0           ['batch_normalization_326[0][0]']\n",
      "                                                                                                  \n",
      " dense_326 (Dense)              (None, 64)           1088        ['leaky_re_lu_260[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_327 (Batch  (None, 64)          256         ['dense_326[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_261 (LeakyReLU)    (None, 64)           0           ['batch_normalization_327[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_261[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_261[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_65 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_328 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_262 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_328 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_329 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_263 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.49380127]\n",
      "Beta is  [0.49380127]\n",
      "  3/391 [..............................] - ETA: 10s - loss: 2.2470 - reconstruction_loss: 1.6548 - kl_loss: 0.5811  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5892 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_66 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_325 (Batch  (None, 57)          228         ['input_66[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_325 (Dense)              (None, 16)           928         ['batch_normalization_325[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_326 (Batch  (None, 16)          64          ['dense_325[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_260 (LeakyReLU)    (None, 16)           0           ['batch_normalization_326[0][0]']\n",
      "                                                                                                  \n",
      " dense_326 (Dense)              (None, 64)           1088        ['leaky_re_lu_260[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_327 (Batch  (None, 64)          256         ['dense_326[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_261 (LeakyReLU)    (None, 64)           0           ['batch_normalization_327[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_261[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_261[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_65 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_328 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_262 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_328 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_329 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_263 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_66 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_325 (Batch  (None, 57)          228         ['input_66[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_325 (Dense)              (None, 16)           928         ['batch_normalization_325[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_326 (Batch  (None, 16)          64          ['dense_325[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_260 (LeakyReLU)    (None, 16)           0           ['batch_normalization_326[0][0]']\n",
      "                                                                                                  \n",
      " dense_326 (Dense)              (None, 64)           1088        ['leaky_re_lu_260[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_327 (Batch  (None, 64)          256         ['dense_326[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_261 (LeakyReLU)    (None, 64)           0           ['batch_normalization_327[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_261[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_261[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_65 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_328 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_262 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_328 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_329 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_263 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_67 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_330 (Batch  (None, 57)          228         ['input_67[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_330 (Dense)              (None, 16)           928         ['batch_normalization_330[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_331 (Batch  (None, 16)          64          ['dense_330[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_264 (LeakyReLU)    (None, 16)           0           ['batch_normalization_331[0][0]']\n",
      "                                                                                                  \n",
      " dense_331 (Dense)              (None, 32)           544         ['leaky_re_lu_264[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_332 (Batch  (None, 32)          128         ['dense_331[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_265 (LeakyReLU)    (None, 32)           0           ['batch_normalization_332[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_265[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_265[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_66 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_332 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_333 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_266 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_334 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_267 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.32101169]\n",
      "Beta is  [0.32101169]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.0841 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_67 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_330 (Batch  (None, 57)          228         ['input_67[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_330 (Dense)              (None, 16)           928         ['batch_normalization_330[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_331 (Batch  (None, 16)          64          ['dense_330[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_264 (LeakyReLU)    (None, 16)           0           ['batch_normalization_331[0][0]']\n",
      "                                                                                                  \n",
      " dense_331 (Dense)              (None, 32)           544         ['leaky_re_lu_264[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_332 (Batch  (None, 32)          128         ['dense_331[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_265 (LeakyReLU)    (None, 32)           0           ['batch_normalization_332[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_265[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_265[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_66 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_332 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_333 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_266 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_334 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_267 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_67 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_330 (Batch  (None, 57)          228         ['input_67[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_330 (Dense)              (None, 16)           928         ['batch_normalization_330[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_331 (Batch  (None, 16)          64          ['dense_330[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_264 (LeakyReLU)    (None, 16)           0           ['batch_normalization_331[0][0]']\n",
      "                                                                                                  \n",
      " dense_331 (Dense)              (None, 32)           544         ['leaky_re_lu_264[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_332 (Batch  (None, 32)          128         ['dense_331[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_265 (LeakyReLU)    (None, 32)           0           ['batch_normalization_332[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_265[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_265[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_66 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_332 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_333 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_266 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_334 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_267 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_335 (Batch  (None, 57)          228         ['input_68[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_335 (Dense)              (None, 16)           928         ['batch_normalization_335[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_336 (Batch  (None, 16)          64          ['dense_335[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_268 (LeakyReLU)    (None, 16)           0           ['batch_normalization_336[0][0]']\n",
      "                                                                                                  \n",
      " dense_336 (Dense)              (None, 32)           544         ['leaky_re_lu_268[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_337 (Batch  (None, 32)          128         ['dense_336[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_269 (LeakyReLU)    (None, 32)           0           ['batch_normalization_337[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_269[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_269[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_67 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_338 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_270 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_339 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_271 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.91854322]\n",
      "Beta is  [0.91854322]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.1555 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_335 (Batch  (None, 57)          228         ['input_68[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_335 (Dense)              (None, 16)           928         ['batch_normalization_335[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_336 (Batch  (None, 16)          64          ['dense_335[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_268 (LeakyReLU)    (None, 16)           0           ['batch_normalization_336[0][0]']\n",
      "                                                                                                  \n",
      " dense_336 (Dense)              (None, 32)           544         ['leaky_re_lu_268[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_337 (Batch  (None, 32)          128         ['dense_336[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_269 (LeakyReLU)    (None, 32)           0           ['batch_normalization_337[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_269[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_269[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_67 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_338 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_270 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_339 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_271 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_335 (Batch  (None, 57)          228         ['input_68[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_335 (Dense)              (None, 16)           928         ['batch_normalization_335[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_336 (Batch  (None, 16)          64          ['dense_335[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_268 (LeakyReLU)    (None, 16)           0           ['batch_normalization_336[0][0]']\n",
      "                                                                                                  \n",
      " dense_336 (Dense)              (None, 32)           544         ['leaky_re_lu_268[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_337 (Batch  (None, 32)          128         ['dense_336[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_269 (LeakyReLU)    (None, 32)           0           ['batch_normalization_337[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_269[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_269[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_67 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_338 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_270 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_339 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_271 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_69 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_340 (Batch  (None, 57)          228         ['input_69[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_340 (Dense)              (None, 16)           928         ['batch_normalization_340[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_341 (Batch  (None, 16)          64          ['dense_340[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_272 (LeakyReLU)    (None, 16)           0           ['batch_normalization_341[0][0]']\n",
      "                                                                                                  \n",
      " dense_341 (Dense)              (None, 64)           1088        ['leaky_re_lu_272[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_342 (Batch  (None, 64)          256         ['dense_341[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_273 (LeakyReLU)    (None, 64)           0           ['batch_normalization_342[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_273[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_273[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_68 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_343 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_274 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_344 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_275 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_344 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.60814742]\n",
      "Beta is  [0.60814742]\n",
      "  3/391 [..............................] - ETA: 14s - loss: 1.8213 - reconstruction_loss: 1.2372 - kl_loss: 0.5736  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 1.1894 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_69 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_340 (Batch  (None, 57)          228         ['input_69[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_340 (Dense)              (None, 16)           928         ['batch_normalization_340[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_341 (Batch  (None, 16)          64          ['dense_340[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_272 (LeakyReLU)    (None, 16)           0           ['batch_normalization_341[0][0]']\n",
      "                                                                                                  \n",
      " dense_341 (Dense)              (None, 64)           1088        ['leaky_re_lu_272[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_342 (Batch  (None, 64)          256         ['dense_341[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_273 (LeakyReLU)    (None, 64)           0           ['batch_normalization_342[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_273[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_273[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_68 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_343 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_274 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_344 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_275 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_344 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_69 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_340 (Batch  (None, 57)          228         ['input_69[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_340 (Dense)              (None, 16)           928         ['batch_normalization_340[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_341 (Batch  (None, 16)          64          ['dense_340[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_272 (LeakyReLU)    (None, 16)           0           ['batch_normalization_341[0][0]']\n",
      "                                                                                                  \n",
      " dense_341 (Dense)              (None, 64)           1088        ['leaky_re_lu_272[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_342 (Batch  (None, 64)          256         ['dense_341[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_273 (LeakyReLU)    (None, 64)           0           ['batch_normalization_342[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_273[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_273[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_68 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_343 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_274 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_344 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_275 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_344 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_70 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_345 (Batch  (None, 57)          228         ['input_70[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_345 (Dense)              (None, 16)           928         ['batch_normalization_345[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_346 (Batch  (None, 16)          64          ['dense_345[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_276 (LeakyReLU)    (None, 16)           0           ['batch_normalization_346[0][0]']\n",
      "                                                                                                  \n",
      " dense_346 (Dense)              (None, 32)           544         ['leaky_re_lu_276[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_347 (Batch  (None, 32)          128         ['dense_346[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_277 (LeakyReLU)    (None, 32)           0           ['batch_normalization_347[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_277[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_277[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_69 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_348 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_278 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_348 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_349 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_279 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.91854285]\n",
      "Beta is  [0.91854285]\n",
      "391/391 [==============================] - 16s 36ms/step - loss: 0.5983 - reconstruction_loss: 0.1458 - kl_loss: 0.2033 - val_loss: inf - val_reconstruction_loss: 0.1338 - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 2/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1561 - reconstruction_loss: 0.1178 - kl_loss: 0.0305\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.1560 - reconstruction_loss: 0.1178 - kl_loss: 0.0305 - val_loss: inf - val_reconstruction_loss: 0.6049 - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.1373 - reconstruction_loss: 0.1170 - kl_loss: 0.0194 - val_loss: inf - val_reconstruction_loss: 0.2393 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.1353 - reconstruction_loss: 0.1169 - kl_loss: 0.0180 - val_loss: inf - val_reconstruction_loss: 0.1418 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1335 - reconstruction_loss: 0.1168 - kl_loss: 0.0164\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.1335 - reconstruction_loss: 0.1168 - kl_loss: 0.0164 - val_loss: inf - val_reconstruction_loss: 0.1753 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.1320 - reconstruction_loss: 0.1168 - kl_loss: 0.0155 - val_loss: inf - val_reconstruction_loss: 0.1804 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 13s 35ms/step - loss: 0.1319 - reconstruction_loss: 0.1168 - kl_loss: 0.0153 - val_loss: inf - val_reconstruction_loss: 0.3964 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1321 - reconstruction_loss: 0.1168 - kl_loss: 0.0151\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.1321 - reconstruction_loss: 0.1168 - kl_loss: 0.0151 - val_loss: inf - val_reconstruction_loss: 0.3955 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.1314 - reconstruction_loss: 0.1168 - kl_loss: 0.0149 - val_loss: inf - val_reconstruction_loss: 0.1641 - val_kl_loss: inf - lr: 1.0000e-06\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1319 - reconstruction_loss: 0.1168 - kl_loss: 0.0149Restoring model weights from the end of the best epoch: 1.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.1319 - reconstruction_loss: 0.1168 - kl_loss: 0.0149 - val_loss: inf - val_reconstruction_loss: 0.1224 - val_kl_loss: inf - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_70 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_345 (Batch  (None, 57)          228         ['input_70[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_345 (Dense)              (None, 16)           928         ['batch_normalization_345[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_346 (Batch  (None, 16)          64          ['dense_345[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_276 (LeakyReLU)    (None, 16)           0           ['batch_normalization_346[0][0]']\n",
      "                                                                                                  \n",
      " dense_346 (Dense)              (None, 32)           544         ['leaky_re_lu_276[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_347 (Batch  (None, 32)          128         ['dense_346[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_277 (LeakyReLU)    (None, 32)           0           ['batch_normalization_347[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_277[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_277[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_69 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_348 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_278 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_348 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_349 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_279 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_70 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_345 (Batch  (None, 57)          228         ['input_70[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_345 (Dense)              (None, 16)           928         ['batch_normalization_345[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_346 (Batch  (None, 16)          64          ['dense_345[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_276 (LeakyReLU)    (None, 16)           0           ['batch_normalization_346[0][0]']\n",
      "                                                                                                  \n",
      " dense_346 (Dense)              (None, 32)           544         ['leaky_re_lu_276[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_347 (Batch  (None, 32)          128         ['dense_346[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_277 (LeakyReLU)    (None, 32)           0           ['batch_normalization_347[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_277[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_277[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_69 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_348 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_278 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_348 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_349 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_279 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_71 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_350 (Batch  (None, 57)          228         ['input_71[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_350 (Dense)              (None, 16)           928         ['batch_normalization_350[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_351 (Batch  (None, 16)          64          ['dense_350[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_280 (LeakyReLU)    (None, 16)           0           ['batch_normalization_351[0][0]']\n",
      "                                                                                                  \n",
      " dense_351 (Dense)              (None, 32)           544         ['leaky_re_lu_280[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_352 (Batch  (None, 32)          128         ['dense_351[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_281 (LeakyReLU)    (None, 32)           0           ['batch_normalization_352[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_281[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_281[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_70 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_352 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_353 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_282 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_354 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_283 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.68753713]\n",
      "Beta is  [0.68753713]\n",
      " 10/391 [..............................] - ETA: 12s - loss: 1.9716 - reconstruction_loss: 0.9862 - kl_loss: 0.9443Batch 11: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 0.9621 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_71 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_350 (Batch  (None, 57)          228         ['input_71[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_350 (Dense)              (None, 16)           928         ['batch_normalization_350[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_351 (Batch  (None, 16)          64          ['dense_350[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_280 (LeakyReLU)    (None, 16)           0           ['batch_normalization_351[0][0]']\n",
      "                                                                                                  \n",
      " dense_351 (Dense)              (None, 32)           544         ['leaky_re_lu_280[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_352 (Batch  (None, 32)          128         ['dense_351[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_281 (LeakyReLU)    (None, 32)           0           ['batch_normalization_352[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_281[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_281[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_70 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_352 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_353 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_282 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_354 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_283 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_71 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_350 (Batch  (None, 57)          228         ['input_71[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_350 (Dense)              (None, 16)           928         ['batch_normalization_350[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_351 (Batch  (None, 16)          64          ['dense_350[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_280 (LeakyReLU)    (None, 16)           0           ['batch_normalization_351[0][0]']\n",
      "                                                                                                  \n",
      " dense_351 (Dense)              (None, 32)           544         ['leaky_re_lu_280[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_352 (Batch  (None, 32)          128         ['dense_351[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_281 (LeakyReLU)    (None, 32)           0           ['batch_normalization_352[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_281[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_281[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_70 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_352 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_353 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_282 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_354 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_283 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_72 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_355 (Batch  (None, 57)          228         ['input_72[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_355 (Dense)              (None, 16)           928         ['batch_normalization_355[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_356 (Batch  (None, 16)          64          ['dense_355[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_284 (LeakyReLU)    (None, 16)           0           ['batch_normalization_356[0][0]']\n",
      "                                                                                                  \n",
      " dense_356 (Dense)              (None, 32)           544         ['leaky_re_lu_284[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_357 (Batch  (None, 32)          128         ['dense_356[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_285 (LeakyReLU)    (None, 32)           0           ['batch_normalization_357[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_285[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_285[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_71 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_358 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_286 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_359 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_287 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.69420174]\n",
      "Beta is  [0.69420174]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 1.1109 - reconstruction_loss: 0.5520 - kl_loss: 0.2046 - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4944 - reconstruction_loss: 0.4438 - kl_loss: 0.0376\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.4944 - reconstruction_loss: 0.4438 - kl_loss: 0.0376 - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4634 - reconstruction_loss: 0.4403 - kl_loss: 0.0233 - val_loss: inf - val_reconstruction_loss: 1.5620 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4608 - reconstruction_loss: 0.4399 - kl_loss: 0.0212 - val_loss: inf - val_reconstruction_loss: 1.0687 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4589 - reconstruction_loss: 0.4396 - kl_loss: 0.0191\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4589 - reconstruction_loss: 0.4396 - kl_loss: 0.0191 - val_loss: inf - val_reconstruction_loss: 0.8472 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4567 - reconstruction_loss: 0.4395 - kl_loss: 0.0179 - val_loss: inf - val_reconstruction_loss: 0.5333 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4571 - reconstruction_loss: 0.4394 - kl_loss: 0.0176 - val_loss: inf - val_reconstruction_loss: 0.5293 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4590 - reconstruction_loss: 0.4394 - kl_loss: 0.0173\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.4590 - reconstruction_loss: 0.4394 - kl_loss: 0.0173 - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.4563 - reconstruction_loss: 0.4394 - kl_loss: 0.0171 - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 1.0000e-06\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4548 - reconstruction_loss: 0.4394 - kl_loss: 0.0171Restoring model weights from the end of the best epoch: 1.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.4548 - reconstruction_loss: 0.4394 - kl_loss: 0.0171 - val_loss: inf - val_reconstruction_loss: 0.4809 - val_kl_loss: inf - lr: 1.0000e-06\n",
      "Epoch 00010: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_72 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_355 (Batch  (None, 57)          228         ['input_72[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_355 (Dense)              (None, 16)           928         ['batch_normalization_355[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_356 (Batch  (None, 16)          64          ['dense_355[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_284 (LeakyReLU)    (None, 16)           0           ['batch_normalization_356[0][0]']\n",
      "                                                                                                  \n",
      " dense_356 (Dense)              (None, 32)           544         ['leaky_re_lu_284[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_357 (Batch  (None, 32)          128         ['dense_356[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_285 (LeakyReLU)    (None, 32)           0           ['batch_normalization_357[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_285[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_285[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_71 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_358 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_286 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_359 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_287 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_72 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_355 (Batch  (None, 57)          228         ['input_72[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_355 (Dense)              (None, 16)           928         ['batch_normalization_355[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_356 (Batch  (None, 16)          64          ['dense_355[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_284 (LeakyReLU)    (None, 16)           0           ['batch_normalization_356[0][0]']\n",
      "                                                                                                  \n",
      " dense_356 (Dense)              (None, 32)           544         ['leaky_re_lu_284[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_357 (Batch  (None, 32)          128         ['dense_356[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_285 (LeakyReLU)    (None, 32)           0           ['batch_normalization_357[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_285[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_285[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_71 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_357 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_358 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_286 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_358 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_359 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_287 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_359 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_73 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_360 (Batch  (None, 57)          228         ['input_73[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_360 (Dense)              (None, 16)           928         ['batch_normalization_360[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_361 (Batch  (None, 16)          64          ['dense_360[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_288 (LeakyReLU)    (None, 16)           0           ['batch_normalization_361[0][0]']\n",
      "                                                                                                  \n",
      " dense_361 (Dense)              (None, 32)           544         ['leaky_re_lu_288[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_362 (Batch  (None, 32)          128         ['dense_361[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_289 (LeakyReLU)    (None, 32)           0           ['batch_normalization_362[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_289[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_289[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_72 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_362 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_363 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_290 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_363 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_364 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_291 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_364 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44673815]\n",
      "Beta is  [0.44673815]\n",
      "  1/391 [..............................] - ETA: 18:49 - loss: 2.3451 - reconstruction_loss: 1.7661 - kl_loss: 0.5790Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.7479 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_73 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_360 (Batch  (None, 57)          228         ['input_73[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_360 (Dense)              (None, 16)           928         ['batch_normalization_360[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_361 (Batch  (None, 16)          64          ['dense_360[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_288 (LeakyReLU)    (None, 16)           0           ['batch_normalization_361[0][0]']\n",
      "                                                                                                  \n",
      " dense_361 (Dense)              (None, 32)           544         ['leaky_re_lu_288[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_362 (Batch  (None, 32)          128         ['dense_361[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_289 (LeakyReLU)    (None, 32)           0           ['batch_normalization_362[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_289[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_289[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_72 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_362 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_363 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_290 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_363 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_364 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_291 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_364 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_73 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_360 (Batch  (None, 57)          228         ['input_73[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_360 (Dense)              (None, 16)           928         ['batch_normalization_360[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_361 (Batch  (None, 16)          64          ['dense_360[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_288 (LeakyReLU)    (None, 16)           0           ['batch_normalization_361[0][0]']\n",
      "                                                                                                  \n",
      " dense_361 (Dense)              (None, 32)           544         ['leaky_re_lu_288[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_362 (Batch  (None, 32)          128         ['dense_361[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_289 (LeakyReLU)    (None, 32)           0           ['batch_normalization_362[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_289[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_289[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_72 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_362 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_363 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_290 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_363 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_364 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_291 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_364 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_74 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_365 (Batch  (None, 57)          228         ['input_74[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_365 (Dense)              (None, 16)           928         ['batch_normalization_365[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_366 (Batch  (None, 16)          64          ['dense_365[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_292 (LeakyReLU)    (None, 16)           0           ['batch_normalization_366[0][0]']\n",
      "                                                                                                  \n",
      " dense_366 (Dense)              (None, 64)           1088        ['leaky_re_lu_292[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_367 (Batch  (None, 64)          256         ['dense_366[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_293 (LeakyReLU)    (None, 64)           0           ['batch_normalization_367[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_293[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_293[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_73 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_367 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_368 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_294 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_368 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_369 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_295 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_369 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.47700984]\n",
      "Beta is  [0.47700984]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 1.3461 - reconstruction_loss: 0.8896 - kl_loss: 0.1151 - val_loss: 0.7863 - val_reconstruction_loss: 0.7511 - val_kl_loss: 0.0353 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7745 - reconstruction_loss: 0.7431 - kl_loss: 0.0274 - val_loss: 0.7516 - val_reconstruction_loss: 0.7271 - val_kl_loss: 0.0246 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7529 - reconstruction_loss: 0.7281 - kl_loss: 0.0232 - val_loss: 0.7406 - val_reconstruction_loss: 0.7185 - val_kl_loss: 0.0222 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7464 - reconstruction_loss: 0.7209 - kl_loss: 0.0221 - val_loss: 0.7342 - val_reconstruction_loss: 0.7126 - val_kl_loss: 0.0217 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7420 - reconstruction_loss: 0.7158 - kl_loss: 0.0224 - val_loss: 0.7298 - val_reconstruction_loss: 0.7073 - val_kl_loss: 0.0226 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7395 - reconstruction_loss: 0.7111 - kl_loss: 0.0239 - val_loss: 0.7270 - val_reconstruction_loss: 0.7016 - val_kl_loss: 0.0254 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7373 - reconstruction_loss: 0.7065 - kl_loss: 0.0260 - val_loss: 0.7245 - val_reconstruction_loss: 0.6978 - val_kl_loss: 0.0268 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7340 - reconstruction_loss: 0.7030 - kl_loss: 0.0277 - val_loss: 0.7227 - val_reconstruction_loss: 0.6950 - val_kl_loss: 0.0277 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7270 - reconstruction_loss: 0.7000 - kl_loss: 0.0286 - val_loss: 0.7221 - val_reconstruction_loss: 0.6923 - val_kl_loss: 0.0299 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7271 - reconstruction_loss: 0.6978 - kl_loss: 0.0292 - val_loss: 0.7203 - val_reconstruction_loss: 0.6904 - val_kl_loss: 0.0300 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7280 - reconstruction_loss: 0.6961 - kl_loss: 0.0297 - val_loss: 0.7187 - val_reconstruction_loss: 0.6874 - val_kl_loss: 0.0313 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7243 - reconstruction_loss: 0.6934 - kl_loss: 0.0307 - val_loss: 0.7170 - val_reconstruction_loss: 0.6856 - val_kl_loss: 0.0315 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7223 - reconstruction_loss: 0.6899 - kl_loss: 0.0325 - val_loss: 0.7155 - val_reconstruction_loss: 0.6818 - val_kl_loss: 0.0337 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7247 - reconstruction_loss: 0.6847 - kl_loss: 0.0362 - val_loss: 0.7133 - val_reconstruction_loss: 0.6754 - val_kl_loss: 0.0380 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7185 - reconstruction_loss: 0.6803 - kl_loss: 0.0389 - val_loss: 0.7119 - val_reconstruction_loss: 0.6729 - val_kl_loss: 0.0390 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7197 - reconstruction_loss: 0.6776 - kl_loss: 0.0402 - val_loss: 0.7115 - val_reconstruction_loss: 0.6698 - val_kl_loss: 0.0418 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7153 - reconstruction_loss: 0.6762 - kl_loss: 0.0410 - val_loss: 0.7111 - val_reconstruction_loss: 0.6705 - val_kl_loss: 0.0406 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7161 - reconstruction_loss: 0.6751 - kl_loss: 0.0413 - val_loss: 0.7105 - val_reconstruction_loss: 0.6678 - val_kl_loss: 0.0428 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7157 - reconstruction_loss: 0.6740 - kl_loss: 0.0420 - val_loss: 0.7114 - val_reconstruction_loss: 0.6692 - val_kl_loss: 0.0422 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7168 - reconstruction_loss: 0.6733 - kl_loss: 0.0422 - val_loss: 0.7094 - val_reconstruction_loss: 0.6651 - val_kl_loss: 0.0444 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7184 - reconstruction_loss: 0.6728 - kl_loss: 0.0425 - val_loss: 0.7092 - val_reconstruction_loss: 0.6648 - val_kl_loss: 0.0444 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7135 - reconstruction_loss: 0.6719 - kl_loss: 0.0430 - val_loss: 0.7094 - val_reconstruction_loss: 0.6655 - val_kl_loss: 0.0439 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7142 - reconstruction_loss: 0.6711 - kl_loss: 0.0433\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7142 - reconstruction_loss: 0.6711 - kl_loss: 0.0433 - val_loss: 0.7092 - val_reconstruction_loss: 0.6650 - val_kl_loss: 0.0442 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7140 - reconstruction_loss: 0.6697 - kl_loss: 0.0435 - val_loss: 0.7084 - val_reconstruction_loss: 0.6656 - val_kl_loss: 0.0429 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7147 - reconstruction_loss: 0.6699 - kl_loss: 0.0432 - val_loss: 0.7084 - val_reconstruction_loss: 0.6655 - val_kl_loss: 0.0430 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7144 - reconstruction_loss: 0.6697 - kl_loss: 0.0434\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7144 - reconstruction_loss: 0.6697 - kl_loss: 0.0434 - val_loss: 0.7088 - val_reconstruction_loss: 0.6656 - val_kl_loss: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7130 - reconstruction_loss: 0.6699 - kl_loss: 0.0433 - val_loss: 0.7083 - val_reconstruction_loss: 0.6652 - val_kl_loss: 0.0431 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7109 - reconstruction_loss: 0.6699 - kl_loss: 0.0433 - val_loss: 0.7080 - val_reconstruction_loss: 0.6650 - val_kl_loss: 0.0430 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7116 - reconstruction_loss: 0.6698 - kl_loss: 0.0433 - val_loss: 0.7081 - val_reconstruction_loss: 0.6652 - val_kl_loss: 0.0430 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7157 - reconstruction_loss: 0.6702 - kl_loss: 0.0434\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7157 - reconstruction_loss: 0.6702 - kl_loss: 0.0434 - val_loss: 0.7090 - val_reconstruction_loss: 0.6658 - val_kl_loss: 0.0433 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7122 - reconstruction_loss: 0.6702 - kl_loss: 0.0433 - val_loss: 0.7082 - val_reconstruction_loss: 0.6652 - val_kl_loss: 0.0431 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7150 - reconstruction_loss: 0.6699 - kl_loss: 0.0434 - val_loss: 0.7082 - val_reconstruction_loss: 0.6652 - val_kl_loss: 0.0430 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7120 - reconstruction_loss: 0.6699 - kl_loss: 0.0434\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7120 - reconstruction_loss: 0.6699 - kl_loss: 0.0434 - val_loss: 0.7083 - val_reconstruction_loss: 0.6654 - val_kl_loss: 0.0429 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7137 - reconstruction_loss: 0.6697 - kl_loss: 0.0434 - val_loss: 0.7077 - val_reconstruction_loss: 0.6649 - val_kl_loss: 0.0429 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7129 - reconstruction_loss: 0.6697 - kl_loss: 0.0434 - val_loss: 0.7084 - val_reconstruction_loss: 0.6656 - val_kl_loss: 0.0429 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7118 - reconstruction_loss: 0.6698 - kl_loss: 0.0434 - val_loss: 0.7086 - val_reconstruction_loss: 0.6656 - val_kl_loss: 0.0430 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7120 - reconstruction_loss: 0.6695 - kl_loss: 0.0434 - val_loss: 0.7085 - val_reconstruction_loss: 0.6655 - val_kl_loss: 0.0431 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7137 - reconstruction_loss: 0.6698 - kl_loss: 0.0434 - val_loss: 0.7083 - val_reconstruction_loss: 0.6653 - val_kl_loss: 0.0431 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7100 - reconstruction_loss: 0.6694 - kl_loss: 0.0434 - val_loss: 0.7083 - val_reconstruction_loss: 0.6656 - val_kl_loss: 0.0428 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7160 - reconstruction_loss: 0.6697 - kl_loss: 0.0434 - val_loss: 0.7081 - val_reconstruction_loss: 0.6650 - val_kl_loss: 0.0432 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7110 - reconstruction_loss: 0.6700 - kl_loss: 0.0433 - val_loss: 0.7083 - val_reconstruction_loss: 0.6654 - val_kl_loss: 0.0429 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7146 - reconstruction_loss: 0.6698 - kl_loss: 0.0433 - val_loss: 0.7086 - val_reconstruction_loss: 0.6656 - val_kl_loss: 0.0431 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7110 - reconstruction_loss: 0.6699 - kl_loss: 0.0434 - val_loss: 0.7082 - val_reconstruction_loss: 0.6653 - val_kl_loss: 0.0429 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7119 - reconstruction_loss: 0.6697 - kl_loss: 0.0434Restoring model weights from the end of the best epoch: 34.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7119 - reconstruction_loss: 0.6697 - kl_loss: 0.0434 - val_loss: 0.7083 - val_reconstruction_loss: 0.6652 - val_kl_loss: 0.0431 - lr: 1.0000e-06\n",
      "Epoch 00044: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_74 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_365 (Batch  (None, 57)          228         ['input_74[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_365 (Dense)              (None, 16)           928         ['batch_normalization_365[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_366 (Batch  (None, 16)          64          ['dense_365[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_292 (LeakyReLU)    (None, 16)           0           ['batch_normalization_366[0][0]']\n",
      "                                                                                                  \n",
      " dense_366 (Dense)              (None, 64)           1088        ['leaky_re_lu_292[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_367 (Batch  (None, 64)          256         ['dense_366[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_293 (LeakyReLU)    (None, 64)           0           ['batch_normalization_367[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_293[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_293[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_73 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_367 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_368 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_294 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_368 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_369 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_295 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_369 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_74 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_365 (Batch  (None, 57)          228         ['input_74[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_365 (Dense)              (None, 16)           928         ['batch_normalization_365[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_366 (Batch  (None, 16)          64          ['dense_365[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_292 (LeakyReLU)    (None, 16)           0           ['batch_normalization_366[0][0]']\n",
      "                                                                                                  \n",
      " dense_366 (Dense)              (None, 64)           1088        ['leaky_re_lu_292[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_367 (Batch  (None, 64)          256         ['dense_366[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_293 (LeakyReLU)    (None, 64)           0           ['batch_normalization_367[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_293[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_293[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_73 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_367 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_368 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_294 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_368 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_369 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_295 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_369 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_75 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_370 (Batch  (None, 57)          228         ['input_75[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_370 (Dense)              (None, 16)           928         ['batch_normalization_370[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_371 (Batch  (None, 16)          64          ['dense_370[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_296 (LeakyReLU)    (None, 16)           0           ['batch_normalization_371[0][0]']\n",
      "                                                                                                  \n",
      " dense_371 (Dense)              (None, 32)           544         ['leaky_re_lu_296[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_372 (Batch  (None, 32)          128         ['dense_371[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_297 (LeakyReLU)    (None, 32)           0           ['batch_normalization_372[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_297[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_297[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_74 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_372 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_373 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_298 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_373 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_374 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_299 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_374 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44668756]\n",
      "Beta is  [0.44668756]\n",
      "  7/391 [..............................] - ETA: 13s - loss: 2.7414 - reconstruction_loss: 1.7263 - kl_loss: 1.1142Batch 7: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 1.7084 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_75 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_370 (Batch  (None, 57)          228         ['input_75[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_370 (Dense)              (None, 16)           928         ['batch_normalization_370[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_371 (Batch  (None, 16)          64          ['dense_370[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_296 (LeakyReLU)    (None, 16)           0           ['batch_normalization_371[0][0]']\n",
      "                                                                                                  \n",
      " dense_371 (Dense)              (None, 32)           544         ['leaky_re_lu_296[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_372 (Batch  (None, 32)          128         ['dense_371[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_297 (LeakyReLU)    (None, 32)           0           ['batch_normalization_372[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_297[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_297[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_74 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_372 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_373 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_298 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_373 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_374 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_299 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_374 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_75 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_370 (Batch  (None, 57)          228         ['input_75[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_370 (Dense)              (None, 16)           928         ['batch_normalization_370[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_371 (Batch  (None, 16)          64          ['dense_370[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_296 (LeakyReLU)    (None, 16)           0           ['batch_normalization_371[0][0]']\n",
      "                                                                                                  \n",
      " dense_371 (Dense)              (None, 32)           544         ['leaky_re_lu_296[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_372 (Batch  (None, 32)          128         ['dense_371[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_297 (LeakyReLU)    (None, 32)           0           ['batch_normalization_372[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_297[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_297[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_74 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_372 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_373 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_298 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_373 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_374 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_299 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_374 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_76 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_375 (Batch  (None, 57)          228         ['input_76[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_375 (Dense)              (None, 16)           928         ['batch_normalization_375[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_376 (Batch  (None, 16)          64          ['dense_375[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_300 (LeakyReLU)    (None, 16)           0           ['batch_normalization_376[0][0]']\n",
      "                                                                                                  \n",
      " dense_376 (Dense)              (None, 32)           544         ['leaky_re_lu_300[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_377 (Batch  (None, 32)          128         ['dense_376[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_301 (LeakyReLU)    (None, 32)           0           ['batch_normalization_377[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_301[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_301[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_75 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_377 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_378 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_302 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_378 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_379 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_303 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44598911]\n",
      "Beta is  [0.44598911]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.9932 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_76 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_375 (Batch  (None, 57)          228         ['input_76[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_375 (Dense)              (None, 16)           928         ['batch_normalization_375[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_376 (Batch  (None, 16)          64          ['dense_375[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_300 (LeakyReLU)    (None, 16)           0           ['batch_normalization_376[0][0]']\n",
      "                                                                                                  \n",
      " dense_376 (Dense)              (None, 32)           544         ['leaky_re_lu_300[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_377 (Batch  (None, 32)          128         ['dense_376[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_301 (LeakyReLU)    (None, 32)           0           ['batch_normalization_377[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_301[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_301[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_75 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_377 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_378 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_302 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_378 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_379 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_303 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_76 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_375 (Batch  (None, 57)          228         ['input_76[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_375 (Dense)              (None, 16)           928         ['batch_normalization_375[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_376 (Batch  (None, 16)          64          ['dense_375[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_300 (LeakyReLU)    (None, 16)           0           ['batch_normalization_376[0][0]']\n",
      "                                                                                                  \n",
      " dense_376 (Dense)              (None, 32)           544         ['leaky_re_lu_300[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_377 (Batch  (None, 32)          128         ['dense_376[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_301 (LeakyReLU)    (None, 32)           0           ['batch_normalization_377[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_301[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_301[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_75 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_377 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_378 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_302 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_378 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_379 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_303 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_77 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_380 (Batch  (None, 57)          228         ['input_77[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_380 (Dense)              (None, 16)           928         ['batch_normalization_380[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_381 (Batch  (None, 16)          64          ['dense_380[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_304 (LeakyReLU)    (None, 16)           0           ['batch_normalization_381[0][0]']\n",
      "                                                                                                  \n",
      " dense_381 (Dense)              (None, 32)           544         ['leaky_re_lu_304[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_382 (Batch  (None, 32)          128         ['dense_381[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_305 (LeakyReLU)    (None, 32)           0           ['batch_normalization_382[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_305[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_305[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_76 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_383 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_306 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_384 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_307 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44602475]\n",
      "Beta is  [0.44602475]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4329 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_77 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_380 (Batch  (None, 57)          228         ['input_77[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_380 (Dense)              (None, 16)           928         ['batch_normalization_380[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_381 (Batch  (None, 16)          64          ['dense_380[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_304 (LeakyReLU)    (None, 16)           0           ['batch_normalization_381[0][0]']\n",
      "                                                                                                  \n",
      " dense_381 (Dense)              (None, 32)           544         ['leaky_re_lu_304[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_382 (Batch  (None, 32)          128         ['dense_381[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_305 (LeakyReLU)    (None, 32)           0           ['batch_normalization_382[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_305[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_305[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_76 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_383 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_306 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_384 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_307 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_77 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_380 (Batch  (None, 57)          228         ['input_77[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_380 (Dense)              (None, 16)           928         ['batch_normalization_380[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_381 (Batch  (None, 16)          64          ['dense_380[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_304 (LeakyReLU)    (None, 16)           0           ['batch_normalization_381[0][0]']\n",
      "                                                                                                  \n",
      " dense_381 (Dense)              (None, 32)           544         ['leaky_re_lu_304[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_382 (Batch  (None, 32)          128         ['dense_381[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_305 (LeakyReLU)    (None, 32)           0           ['batch_normalization_382[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_305[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_305[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_76 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_383 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_306 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_384 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_307 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_78 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_385 (Batch  (None, 57)          228         ['input_78[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_385 (Dense)              (None, 16)           928         ['batch_normalization_385[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_386 (Batch  (None, 16)          64          ['dense_385[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_308 (LeakyReLU)    (None, 16)           0           ['batch_normalization_386[0][0]']\n",
      "                                                                                                  \n",
      " dense_386 (Dense)              (None, 32)           544         ['leaky_re_lu_308[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_387 (Batch  (None, 32)          128         ['dense_386[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_309 (LeakyReLU)    (None, 32)           0           ['batch_normalization_387[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_309[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_309[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_77 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_387 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_388 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_310 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_388 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_389 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_311 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_389 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44598252]\n",
      "Beta is  [0.44598252]\n",
      "  1/391 [..............................] - ETA: 15:34 - loss: 2.7991 - reconstruction_loss: 1.5880 - kl_loss: 1.2111Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5083 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_78 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_385 (Batch  (None, 57)          228         ['input_78[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_385 (Dense)              (None, 16)           928         ['batch_normalization_385[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_386 (Batch  (None, 16)          64          ['dense_385[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_308 (LeakyReLU)    (None, 16)           0           ['batch_normalization_386[0][0]']\n",
      "                                                                                                  \n",
      " dense_386 (Dense)              (None, 32)           544         ['leaky_re_lu_308[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_387 (Batch  (None, 32)          128         ['dense_386[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_309 (LeakyReLU)    (None, 32)           0           ['batch_normalization_387[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_309[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_309[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_77 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_387 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_388 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_310 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_388 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_389 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_311 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_389 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_78 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_385 (Batch  (None, 57)          228         ['input_78[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_385 (Dense)              (None, 16)           928         ['batch_normalization_385[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_386 (Batch  (None, 16)          64          ['dense_385[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_308 (LeakyReLU)    (None, 16)           0           ['batch_normalization_386[0][0]']\n",
      "                                                                                                  \n",
      " dense_386 (Dense)              (None, 32)           544         ['leaky_re_lu_308[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_387 (Batch  (None, 32)          128         ['dense_386[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_309 (LeakyReLU)    (None, 32)           0           ['batch_normalization_387[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_309[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_309[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_77 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_387 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_388 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_310 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_388 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_389 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_311 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_389 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_79 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_390 (Batch  (None, 57)          228         ['input_79[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_390 (Dense)              (None, 16)           928         ['batch_normalization_390[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_391 (Batch  (None, 16)          64          ['dense_390[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_312 (LeakyReLU)    (None, 16)           0           ['batch_normalization_391[0][0]']\n",
      "                                                                                                  \n",
      " dense_391 (Dense)              (None, 32)           544         ['leaky_re_lu_312[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_392 (Batch  (None, 32)          128         ['dense_391[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_313 (LeakyReLU)    (None, 32)           0           ['batch_normalization_392[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_313[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_313[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_78 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_392 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_393 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_314 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_393 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_394 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_315 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_394 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44600068]\n",
      "Beta is  [0.44600068]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.3653 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_79 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_390 (Batch  (None, 57)          228         ['input_79[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_390 (Dense)              (None, 16)           928         ['batch_normalization_390[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_391 (Batch  (None, 16)          64          ['dense_390[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_312 (LeakyReLU)    (None, 16)           0           ['batch_normalization_391[0][0]']\n",
      "                                                                                                  \n",
      " dense_391 (Dense)              (None, 32)           544         ['leaky_re_lu_312[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_392 (Batch  (None, 32)          128         ['dense_391[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_313 (LeakyReLU)    (None, 32)           0           ['batch_normalization_392[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_313[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_313[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_78 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_392 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_393 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_314 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_393 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_394 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_315 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_394 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_79 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_390 (Batch  (None, 57)          228         ['input_79[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_390 (Dense)              (None, 16)           928         ['batch_normalization_390[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_391 (Batch  (None, 16)          64          ['dense_390[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_312 (LeakyReLU)    (None, 16)           0           ['batch_normalization_391[0][0]']\n",
      "                                                                                                  \n",
      " dense_391 (Dense)              (None, 32)           544         ['leaky_re_lu_312[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_392 (Batch  (None, 32)          128         ['dense_391[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_313 (LeakyReLU)    (None, 32)           0           ['batch_normalization_392[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_313[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_313[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_78 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_392 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_393 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_314 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_393 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_394 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_315 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_394 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_80 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_395 (Batch  (None, 57)          228         ['input_80[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_395 (Dense)              (None, 16)           928         ['batch_normalization_395[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_396 (Batch  (None, 16)          64          ['dense_395[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_316 (LeakyReLU)    (None, 16)           0           ['batch_normalization_396[0][0]']\n",
      "                                                                                                  \n",
      " dense_396 (Dense)              (None, 32)           544         ['leaky_re_lu_316[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_397 (Batch  (None, 32)          128         ['dense_396[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_317 (LeakyReLU)    (None, 32)           0           ['batch_normalization_397[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_317[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_317[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_79 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_397 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_398 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_318 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_398 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_399 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_319 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_399 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44603139]\n",
      "Beta is  [0.44603139]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.1902 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_80 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_395 (Batch  (None, 57)          228         ['input_80[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_395 (Dense)              (None, 16)           928         ['batch_normalization_395[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_396 (Batch  (None, 16)          64          ['dense_395[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_316 (LeakyReLU)    (None, 16)           0           ['batch_normalization_396[0][0]']\n",
      "                                                                                                  \n",
      " dense_396 (Dense)              (None, 32)           544         ['leaky_re_lu_316[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_397 (Batch  (None, 32)          128         ['dense_396[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_317 (LeakyReLU)    (None, 32)           0           ['batch_normalization_397[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_317[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_317[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_79 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_397 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_398 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_318 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_398 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_399 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_319 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_399 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_80 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_395 (Batch  (None, 57)          228         ['input_80[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_395 (Dense)              (None, 16)           928         ['batch_normalization_395[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_396 (Batch  (None, 16)          64          ['dense_395[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_316 (LeakyReLU)    (None, 16)           0           ['batch_normalization_396[0][0]']\n",
      "                                                                                                  \n",
      " dense_396 (Dense)              (None, 32)           544         ['leaky_re_lu_316[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_397 (Batch  (None, 32)          128         ['dense_396[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_317 (LeakyReLU)    (None, 32)           0           ['batch_normalization_397[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_317[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_317[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_79 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_397 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_398 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_318 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_398 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_399 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_319 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_399 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_81 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_400 (Batch  (None, 57)          228         ['input_81[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_400 (Dense)              (None, 16)           928         ['batch_normalization_400[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_401 (Batch  (None, 16)          64          ['dense_400[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_320 (LeakyReLU)    (None, 16)           0           ['batch_normalization_401[0][0]']\n",
      "                                                                                                  \n",
      " dense_401 (Dense)              (None, 32)           544         ['leaky_re_lu_320[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_402 (Batch  (None, 32)          128         ['dense_401[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_321 (LeakyReLU)    (None, 32)           0           ['batch_normalization_402[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_321[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_321[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_80 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_403 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_322 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_403 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_404 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_323 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_404 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.6897641]\n",
      "Beta is  [0.6897641]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.7744 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_81 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_400 (Batch  (None, 57)          228         ['input_81[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_400 (Dense)              (None, 16)           928         ['batch_normalization_400[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_401 (Batch  (None, 16)          64          ['dense_400[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_320 (LeakyReLU)    (None, 16)           0           ['batch_normalization_401[0][0]']\n",
      "                                                                                                  \n",
      " dense_401 (Dense)              (None, 32)           544         ['leaky_re_lu_320[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_402 (Batch  (None, 32)          128         ['dense_401[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_321 (LeakyReLU)    (None, 32)           0           ['batch_normalization_402[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_321[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_321[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_80 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_403 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_322 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_403 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_404 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_323 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_404 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_81 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_400 (Batch  (None, 57)          228         ['input_81[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_400 (Dense)              (None, 16)           928         ['batch_normalization_400[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_401 (Batch  (None, 16)          64          ['dense_400[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_320 (LeakyReLU)    (None, 16)           0           ['batch_normalization_401[0][0]']\n",
      "                                                                                                  \n",
      " dense_401 (Dense)              (None, 32)           544         ['leaky_re_lu_320[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_402 (Batch  (None, 32)          128         ['dense_401[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_321 (LeakyReLU)    (None, 32)           0           ['batch_normalization_402[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_321[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_321[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_80 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_403 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_322 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_403 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_404 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_323 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_404 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_82 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_405 (Batch  (None, 57)          228         ['input_82[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_405 (Dense)              (None, 16)           928         ['batch_normalization_405[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_406 (Batch  (None, 16)          64          ['dense_405[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_324 (LeakyReLU)    (None, 16)           0           ['batch_normalization_406[0][0]']\n",
      "                                                                                                  \n",
      " dense_406 (Dense)              (None, 32)           544         ['leaky_re_lu_324[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_407 (Batch  (None, 32)          128         ['dense_406[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_325 (LeakyReLU)    (None, 32)           0           ['batch_normalization_407[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_325[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_325[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_81 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_407 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_408 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_326 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_408 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_409 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_327 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_409 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44610575]\n",
      "Beta is  [0.44610575]\n",
      "  3/391 [..............................] - ETA: 15s - loss: 2.7289 - reconstruction_loss: 1.6538 - kl_loss: 1.0565  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.6389 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_82 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_405 (Batch  (None, 57)          228         ['input_82[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_405 (Dense)              (None, 16)           928         ['batch_normalization_405[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_406 (Batch  (None, 16)          64          ['dense_405[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_324 (LeakyReLU)    (None, 16)           0           ['batch_normalization_406[0][0]']\n",
      "                                                                                                  \n",
      " dense_406 (Dense)              (None, 32)           544         ['leaky_re_lu_324[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_407 (Batch  (None, 32)          128         ['dense_406[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_325 (LeakyReLU)    (None, 32)           0           ['batch_normalization_407[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_325[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_325[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_81 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_407 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_408 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_326 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_408 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_409 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_327 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_409 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_82 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_405 (Batch  (None, 57)          228         ['input_82[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_405 (Dense)              (None, 16)           928         ['batch_normalization_405[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_406 (Batch  (None, 16)          64          ['dense_405[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_324 (LeakyReLU)    (None, 16)           0           ['batch_normalization_406[0][0]']\n",
      "                                                                                                  \n",
      " dense_406 (Dense)              (None, 32)           544         ['leaky_re_lu_324[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_407 (Batch  (None, 32)          128         ['dense_406[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_325 (LeakyReLU)    (None, 32)           0           ['batch_normalization_407[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_325[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_325[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_81 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_407 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_408 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_326 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_408 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_409 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_327 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_409 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_83 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_410 (Batch  (None, 57)          228         ['input_83[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_410 (Dense)              (None, 16)           928         ['batch_normalization_410[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_411 (Batch  (None, 16)          64          ['dense_410[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_328 (LeakyReLU)    (None, 16)           0           ['batch_normalization_411[0][0]']\n",
      "                                                                                                  \n",
      " dense_411 (Dense)              (None, 32)           544         ['leaky_re_lu_328[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_412 (Batch  (None, 32)          128         ['dense_411[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_329 (LeakyReLU)    (None, 32)           0           ['batch_normalization_412[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_329[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_329[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_82 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_412 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_413 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_330 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_413 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_414 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_331 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_414 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44608462]\n",
      "Beta is  [0.44608462]\n",
      "391/391 [==============================] - 17s 37ms/step - loss: 1.5156 - reconstruction_loss: 0.9921 - kl_loss: 0.1352 - val_loss: 0.8403 - val_reconstruction_loss: 0.7990 - val_kl_loss: 0.0413 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.8243 - reconstruction_loss: 0.7882 - kl_loss: 0.0296 - val_loss: 0.8047 - val_reconstruction_loss: 0.7806 - val_kl_loss: 0.0240 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.8003 - reconstruction_loss: 0.7770 - kl_loss: 0.0211 - val_loss: 0.7921 - val_reconstruction_loss: 0.7716 - val_kl_loss: 0.0205 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7905 - reconstruction_loss: 0.7693 - kl_loss: 0.0205 - val_loss: 0.7850 - val_reconstruction_loss: 0.7626 - val_kl_loss: 0.0225 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7851 - reconstruction_loss: 0.7616 - kl_loss: 0.0226 - val_loss: 0.7799 - val_reconstruction_loss: 0.7550 - val_kl_loss: 0.0249 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7797 - reconstruction_loss: 0.7539 - kl_loss: 0.0257 - val_loss: 0.7758 - val_reconstruction_loss: 0.7465 - val_kl_loss: 0.0292 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7754 - reconstruction_loss: 0.7462 - kl_loss: 0.0294 - val_loss: 0.7713 - val_reconstruction_loss: 0.7403 - val_kl_loss: 0.0310 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7734 - reconstruction_loss: 0.7394 - kl_loss: 0.0322 - val_loss: 0.7678 - val_reconstruction_loss: 0.7329 - val_kl_loss: 0.0349 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7683 - reconstruction_loss: 0.7342 - kl_loss: 0.0342 - val_loss: 0.7654 - val_reconstruction_loss: 0.7304 - val_kl_loss: 0.0349 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7667 - reconstruction_loss: 0.7302 - kl_loss: 0.0355 - val_loss: 0.7621 - val_reconstruction_loss: 0.7237 - val_kl_loss: 0.0383 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7626 - reconstruction_loss: 0.7259 - kl_loss: 0.0374 - val_loss: 0.7603 - val_reconstruction_loss: 0.7210 - val_kl_loss: 0.0393 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7603 - reconstruction_loss: 0.7216 - kl_loss: 0.0395 - val_loss: 0.7581 - val_reconstruction_loss: 0.7169 - val_kl_loss: 0.0412 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7594 - reconstruction_loss: 0.7171 - kl_loss: 0.0419 - val_loss: 0.7561 - val_reconstruction_loss: 0.7124 - val_kl_loss: 0.0437 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7556 - reconstruction_loss: 0.7129 - kl_loss: 0.0444 - val_loss: 0.7549 - val_reconstruction_loss: 0.7093 - val_kl_loss: 0.0456 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7571 - reconstruction_loss: 0.7096 - kl_loss: 0.0467 - val_loss: 0.7542 - val_reconstruction_loss: 0.7063 - val_kl_loss: 0.0478 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7545 - reconstruction_loss: 0.7062 - kl_loss: 0.0487 - val_loss: 0.7534 - val_reconstruction_loss: 0.7038 - val_kl_loss: 0.0495 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7532 - reconstruction_loss: 0.7042 - kl_loss: 0.0502 - val_loss: 0.7528 - val_reconstruction_loss: 0.7012 - val_kl_loss: 0.0516 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7519 - reconstruction_loss: 0.7029 - kl_loss: 0.0507 - val_loss: 0.7520 - val_reconstruction_loss: 0.7010 - val_kl_loss: 0.0510 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7578 - reconstruction_loss: 0.7021 - kl_loss: 0.0514 - val_loss: 0.7520 - val_reconstruction_loss: 0.7006 - val_kl_loss: 0.0514 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7537 - reconstruction_loss: 0.7014 - kl_loss: 0.0517 - val_loss: 0.7516 - val_reconstruction_loss: 0.6997 - val_kl_loss: 0.0518 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7509 - reconstruction_loss: 0.7004 - kl_loss: 0.0522 - val_loss: 0.7520 - val_reconstruction_loss: 0.6994 - val_kl_loss: 0.0525 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7560 - reconstruction_loss: 0.6999 - kl_loss: 0.0526\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7560 - reconstruction_loss: 0.6999 - kl_loss: 0.0526 - val_loss: 0.7529 - val_reconstruction_loss: 0.6992 - val_kl_loss: 0.0536 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7517 - reconstruction_loss: 0.6987 - kl_loss: 0.0532 - val_loss: 0.7511 - val_reconstruction_loss: 0.6983 - val_kl_loss: 0.0528 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7501 - reconstruction_loss: 0.6987 - kl_loss: 0.0530 - val_loss: 0.7512 - val_reconstruction_loss: 0.6979 - val_kl_loss: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7504 - reconstruction_loss: 0.6986 - kl_loss: 0.0530\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7504 - reconstruction_loss: 0.6986 - kl_loss: 0.0530 - val_loss: 0.7515 - val_reconstruction_loss: 0.6982 - val_kl_loss: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7515 - reconstruction_loss: 0.6986 - kl_loss: 0.0531 - val_loss: 0.7505 - val_reconstruction_loss: 0.6973 - val_kl_loss: 0.0531 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7522 - reconstruction_loss: 0.6986 - kl_loss: 0.0531 - val_loss: 0.7511 - val_reconstruction_loss: 0.6976 - val_kl_loss: 0.0534 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7514 - reconstruction_loss: 0.6987 - kl_loss: 0.0531\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7514 - reconstruction_loss: 0.6987 - kl_loss: 0.0531 - val_loss: 0.7509 - val_reconstruction_loss: 0.6978 - val_kl_loss: 0.0531 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7526 - reconstruction_loss: 0.6985 - kl_loss: 0.0531 - val_loss: 0.7509 - val_reconstruction_loss: 0.6975 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7523 - reconstruction_loss: 0.6988 - kl_loss: 0.0531 - val_loss: 0.7511 - val_reconstruction_loss: 0.6978 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.7525 - reconstruction_loss: 0.6987 - kl_loss: 0.0531\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7525 - reconstruction_loss: 0.6987 - kl_loss: 0.0531 - val_loss: 0.7513 - val_reconstruction_loss: 0.6980 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7512 - reconstruction_loss: 0.6984 - kl_loss: 0.0531 - val_loss: 0.7512 - val_reconstruction_loss: 0.6979 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7494 - reconstruction_loss: 0.6987 - kl_loss: 0.0531 - val_loss: 0.7507 - val_reconstruction_loss: 0.6977 - val_kl_loss: 0.0530 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7549 - reconstruction_loss: 0.6985 - kl_loss: 0.0531 - val_loss: 0.7510 - val_reconstruction_loss: 0.6977 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7515 - reconstruction_loss: 0.6984 - kl_loss: 0.0531 - val_loss: 0.7506 - val_reconstruction_loss: 0.6975 - val_kl_loss: 0.0531 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7492 - reconstruction_loss: 0.6984 - kl_loss: 0.0531 - val_loss: 0.7503 - val_reconstruction_loss: 0.6971 - val_kl_loss: 0.0532 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7496 - reconstruction_loss: 0.6986 - kl_loss: 0.0531 - val_loss: 0.7504 - val_reconstruction_loss: 0.6973 - val_kl_loss: 0.0531 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7486 - reconstruction_loss: 0.6985 - kl_loss: 0.0531 - val_loss: 0.7502 - val_reconstruction_loss: 0.6972 - val_kl_loss: 0.0530 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7527 - reconstruction_loss: 0.6984 - kl_loss: 0.0531 - val_loss: 0.7515 - val_reconstruction_loss: 0.6981 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7493 - reconstruction_loss: 0.6986 - kl_loss: 0.0531 - val_loss: 0.7509 - val_reconstruction_loss: 0.6978 - val_kl_loss: 0.0531 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7507 - reconstruction_loss: 0.6987 - kl_loss: 0.0531 - val_loss: 0.7513 - val_reconstruction_loss: 0.6981 - val_kl_loss: 0.0532 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7501 - reconstruction_loss: 0.6984 - kl_loss: 0.0531 - val_loss: 0.7508 - val_reconstruction_loss: 0.6975 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7506 - reconstruction_loss: 0.6985 - kl_loss: 0.0531 - val_loss: 0.7510 - val_reconstruction_loss: 0.6976 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7498 - reconstruction_loss: 0.6985 - kl_loss: 0.0531 - val_loss: 0.7509 - val_reconstruction_loss: 0.6976 - val_kl_loss: 0.0532 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7508 - reconstruction_loss: 0.6983 - kl_loss: 0.0531 - val_loss: 0.7509 - val_reconstruction_loss: 0.6975 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 46/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7517 - reconstruction_loss: 0.6986 - kl_loss: 0.0531 - val_loss: 0.7514 - val_reconstruction_loss: 0.6981 - val_kl_loss: 0.0532 - lr: 1.0000e-06\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7513 - reconstruction_loss: 0.6984 - kl_loss: 0.0531 - val_loss: 0.7515 - val_reconstruction_loss: 0.6982 - val_kl_loss: 0.0533 - lr: 1.0000e-06\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7517 - reconstruction_loss: 0.6986 - kl_loss: 0.0531Restoring model weights from the end of the best epoch: 38.\n",
      "391/391 [==============================] - 14s 37ms/step - loss: 0.7517 - reconstruction_loss: 0.6986 - kl_loss: 0.0531 - val_loss: 0.7521 - val_reconstruction_loss: 0.6987 - val_kl_loss: 0.0534 - lr: 1.0000e-06\n",
      "Epoch 00048: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_83 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_410 (Batch  (None, 57)          228         ['input_83[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_410 (Dense)              (None, 16)           928         ['batch_normalization_410[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_411 (Batch  (None, 16)          64          ['dense_410[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_328 (LeakyReLU)    (None, 16)           0           ['batch_normalization_411[0][0]']\n",
      "                                                                                                  \n",
      " dense_411 (Dense)              (None, 32)           544         ['leaky_re_lu_328[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_412 (Batch  (None, 32)          128         ['dense_411[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_329 (LeakyReLU)    (None, 32)           0           ['batch_normalization_412[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_329[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_329[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_82 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_412 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_413 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_330 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_413 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_414 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_331 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_414 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_83 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_410 (Batch  (None, 57)          228         ['input_83[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_410 (Dense)              (None, 16)           928         ['batch_normalization_410[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_411 (Batch  (None, 16)          64          ['dense_410[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_328 (LeakyReLU)    (None, 16)           0           ['batch_normalization_411[0][0]']\n",
      "                                                                                                  \n",
      " dense_411 (Dense)              (None, 32)           544         ['leaky_re_lu_328[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_412 (Batch  (None, 32)          128         ['dense_411[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_329 (LeakyReLU)    (None, 32)           0           ['batch_normalization_412[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_329[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_329[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_82 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_412 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_413 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_330 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_413 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_414 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_331 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_414 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_84 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_415 (Batch  (None, 57)          228         ['input_84[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_415 (Dense)              (None, 16)           928         ['batch_normalization_415[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_416 (Batch  (None, 16)          64          ['dense_415[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_332 (LeakyReLU)    (None, 16)           0           ['batch_normalization_416[0][0]']\n",
      "                                                                                                  \n",
      " dense_416 (Dense)              (None, 32)           544         ['leaky_re_lu_332[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_417 (Batch  (None, 32)          128         ['dense_416[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_333 (LeakyReLU)    (None, 32)           0           ['batch_normalization_417[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_333[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_333[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_83 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_417 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_418 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_334 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_418 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_419 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_335 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_419 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44579786]\n",
      "Beta is  [0.44579786]\n",
      "  3/391 [..............................] - ETA: 14s - loss: 2.7407 - reconstruction_loss: 1.7374 - kl_loss: 0.9399  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.7148 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_84 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_415 (Batch  (None, 57)          228         ['input_84[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_415 (Dense)              (None, 16)           928         ['batch_normalization_415[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_416 (Batch  (None, 16)          64          ['dense_415[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_332 (LeakyReLU)    (None, 16)           0           ['batch_normalization_416[0][0]']\n",
      "                                                                                                  \n",
      " dense_416 (Dense)              (None, 32)           544         ['leaky_re_lu_332[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_417 (Batch  (None, 32)          128         ['dense_416[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_333 (LeakyReLU)    (None, 32)           0           ['batch_normalization_417[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_333[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_333[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_83 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_417 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_418 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_334 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_418 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_419 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_335 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_419 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_84 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_415 (Batch  (None, 57)          228         ['input_84[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_415 (Dense)              (None, 16)           928         ['batch_normalization_415[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_416 (Batch  (None, 16)          64          ['dense_415[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_332 (LeakyReLU)    (None, 16)           0           ['batch_normalization_416[0][0]']\n",
      "                                                                                                  \n",
      " dense_416 (Dense)              (None, 32)           544         ['leaky_re_lu_332[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_417 (Batch  (None, 32)          128         ['dense_416[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_333 (LeakyReLU)    (None, 32)           0           ['batch_normalization_417[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_333[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_333[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_83 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_417 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_418 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_334 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_418 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_419 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_335 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_419 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_85 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_420 (Batch  (None, 57)          228         ['input_85[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_420 (Dense)              (None, 16)           928         ['batch_normalization_420[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_421 (Batch  (None, 16)          64          ['dense_420[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_336 (LeakyReLU)    (None, 16)           0           ['batch_normalization_421[0][0]']\n",
      "                                                                                                  \n",
      " dense_421 (Dense)              (None, 32)           544         ['leaky_re_lu_336[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_422 (Batch  (None, 32)          128         ['dense_421[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_337 (LeakyReLU)    (None, 32)           0           ['batch_normalization_422[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_337[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_337[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_84 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_422 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_423 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_338 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_423 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_424 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_339 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_424 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44578788]\n",
      "Beta is  [0.44578788]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 1.5215 - reconstruction_loss: 0.9954 - kl_loss: 0.1363 - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8265 - reconstruction_loss: 0.7811 - kl_loss: 0.0346\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.8264 - reconstruction_loss: 0.7811 - kl_loss: 0.0346 - val_loss: inf - val_reconstruction_loss: 0.8566 - val_kl_loss: inf - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.8027 - reconstruction_loss: 0.7714 - kl_loss: 0.0293 - val_loss: inf - val_reconstruction_loss: 5.1041 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 13s 35ms/step - loss: 0.7961 - reconstruction_loss: 0.7701 - kl_loss: 0.0286 - val_loss: inf - val_reconstruction_loss: 1.0782 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8018 - reconstruction_loss: 0.7687 - kl_loss: 0.0280\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.8018 - reconstruction_loss: 0.7687 - kl_loss: 0.0280 - val_loss: inf - val_reconstruction_loss: 0.9215 - val_kl_loss: inf - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7995 - reconstruction_loss: 0.7681 - kl_loss: 0.0276 - val_loss: inf - val_reconstruction_loss: 1.0205 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7990 - reconstruction_loss: 0.7680 - kl_loss: 0.0275 - val_loss: inf - val_reconstruction_loss: 1.7375 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7884 - reconstruction_loss: 0.7680 - kl_loss: 0.0274\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7884 - reconstruction_loss: 0.7680 - kl_loss: 0.0274 - val_loss: inf - val_reconstruction_loss: 0.7753 - val_kl_loss: inf - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7895 - reconstruction_loss: 0.7679 - kl_loss: 0.0274 - val_loss: inf - val_reconstruction_loss: 0.9857 - val_kl_loss: inf - lr: 1.0000e-06\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7890 - reconstruction_loss: 0.7676 - kl_loss: 0.0274Restoring model weights from the end of the best epoch: 1.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.7890 - reconstruction_loss: 0.7676 - kl_loss: 0.0274 - val_loss: inf - val_reconstruction_loss: 0.7847 - val_kl_loss: inf - lr: 1.0000e-06\n",
      "Epoch 00010: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_85 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_420 (Batch  (None, 57)          228         ['input_85[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_420 (Dense)              (None, 16)           928         ['batch_normalization_420[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_421 (Batch  (None, 16)          64          ['dense_420[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_336 (LeakyReLU)    (None, 16)           0           ['batch_normalization_421[0][0]']\n",
      "                                                                                                  \n",
      " dense_421 (Dense)              (None, 32)           544         ['leaky_re_lu_336[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_422 (Batch  (None, 32)          128         ['dense_421[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_337 (LeakyReLU)    (None, 32)           0           ['batch_normalization_422[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_337[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_337[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_84 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_422 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_423 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_338 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_423 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_424 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_339 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_424 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_85 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_420 (Batch  (None, 57)          228         ['input_85[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_420 (Dense)              (None, 16)           928         ['batch_normalization_420[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_421 (Batch  (None, 16)          64          ['dense_420[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_336 (LeakyReLU)    (None, 16)           0           ['batch_normalization_421[0][0]']\n",
      "                                                                                                  \n",
      " dense_421 (Dense)              (None, 32)           544         ['leaky_re_lu_336[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_422 (Batch  (None, 32)          128         ['dense_421[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_337 (LeakyReLU)    (None, 32)           0           ['batch_normalization_422[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_337[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_337[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_84 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_422 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_423 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_338 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_423 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_424 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_339 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_424 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_86 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_425 (Batch  (None, 57)          228         ['input_86[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_425 (Dense)              (None, 16)           928         ['batch_normalization_425[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_426 (Batch  (None, 16)          64          ['dense_425[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_340 (LeakyReLU)    (None, 16)           0           ['batch_normalization_426[0][0]']\n",
      "                                                                                                  \n",
      " dense_426 (Dense)              (None, 32)           544         ['leaky_re_lu_340[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_427 (Batch  (None, 32)          128         ['dense_426[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_341 (LeakyReLU)    (None, 32)           0           ['batch_normalization_427[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_341[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_341[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_85 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_427 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_428 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_342 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_428 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_429 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_343 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_429 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.4458193]\n",
      "Beta is  [0.4458193]\n",
      " 13/391 [..............................] - ETA: 13s - loss: 2.4238 - reconstruction_loss: 1.6586 - kl_loss: 0.6831Batch 13: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 1.6430 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_86 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_425 (Batch  (None, 57)          228         ['input_86[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_425 (Dense)              (None, 16)           928         ['batch_normalization_425[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_426 (Batch  (None, 16)          64          ['dense_425[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_340 (LeakyReLU)    (None, 16)           0           ['batch_normalization_426[0][0]']\n",
      "                                                                                                  \n",
      " dense_426 (Dense)              (None, 32)           544         ['leaky_re_lu_340[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_427 (Batch  (None, 32)          128         ['dense_426[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_341 (LeakyReLU)    (None, 32)           0           ['batch_normalization_427[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_341[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_341[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_85 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_427 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_428 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_342 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_428 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_429 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_343 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_429 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_86 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_425 (Batch  (None, 57)          228         ['input_86[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_425 (Dense)              (None, 16)           928         ['batch_normalization_425[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_426 (Batch  (None, 16)          64          ['dense_425[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_340 (LeakyReLU)    (None, 16)           0           ['batch_normalization_426[0][0]']\n",
      "                                                                                                  \n",
      " dense_426 (Dense)              (None, 32)           544         ['leaky_re_lu_340[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_427 (Batch  (None, 32)          128         ['dense_426[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_341 (LeakyReLU)    (None, 32)           0           ['batch_normalization_427[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_341[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_341[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_85 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_427 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_428 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_342 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_428 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_429 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_343 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_429 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_87 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_430 (Batch  (None, 57)          228         ['input_87[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_430 (Dense)              (None, 16)           928         ['batch_normalization_430[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_431 (Batch  (None, 16)          64          ['dense_430[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_344 (LeakyReLU)    (None, 16)           0           ['batch_normalization_431[0][0]']\n",
      "                                                                                                  \n",
      " dense_431 (Dense)              (None, 32)           544         ['leaky_re_lu_344[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_432 (Batch  (None, 32)          128         ['dense_431[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_345 (LeakyReLU)    (None, 32)           0           ['batch_normalization_432[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_345[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_345[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_86 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_432 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_433 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_346 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_433 (Dense)           (None, 16)                528       \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_434 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_347 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_434 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44586785]\n",
      "Beta is  [0.44586785]\n",
      "  1/391 [..............................] - ETA: 18:53 - loss: 2.6275 - reconstruction_loss: 1.7806 - kl_loss: 0.8469Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.7490 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_87 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_430 (Batch  (None, 57)          228         ['input_87[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_430 (Dense)              (None, 16)           928         ['batch_normalization_430[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_431 (Batch  (None, 16)          64          ['dense_430[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_344 (LeakyReLU)    (None, 16)           0           ['batch_normalization_431[0][0]']\n",
      "                                                                                                  \n",
      " dense_431 (Dense)              (None, 32)           544         ['leaky_re_lu_344[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_432 (Batch  (None, 32)          128         ['dense_431[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_345 (LeakyReLU)    (None, 32)           0           ['batch_normalization_432[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_345[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_345[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_86 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_432 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_433 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_346 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_433 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_434 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_347 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_434 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_87 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_430 (Batch  (None, 57)          228         ['input_87[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_430 (Dense)              (None, 16)           928         ['batch_normalization_430[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_431 (Batch  (None, 16)          64          ['dense_430[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_344 (LeakyReLU)    (None, 16)           0           ['batch_normalization_431[0][0]']\n",
      "                                                                                                  \n",
      " dense_431 (Dense)              (None, 32)           544         ['leaky_re_lu_344[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_432 (Batch  (None, 32)          128         ['dense_431[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_345 (LeakyReLU)    (None, 32)           0           ['batch_normalization_432[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_345[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_345[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_86 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_432 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_433 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_346 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_433 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_434 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_347 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_434 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_88 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_435 (Batch  (None, 57)          228         ['input_88[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_435 (Dense)              (None, 16)           928         ['batch_normalization_435[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_436 (Batch  (None, 16)          64          ['dense_435[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_348 (LeakyReLU)    (None, 16)           0           ['batch_normalization_436[0][0]']\n",
      "                                                                                                  \n",
      " dense_436 (Dense)              (None, 32)           544         ['leaky_re_lu_348[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_437 (Batch  (None, 32)          128         ['dense_436[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_349 (LeakyReLU)    (None, 32)           0           ['batch_normalization_437[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_349[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_349[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_87 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_437 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_438 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_350 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_438 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_439 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_351 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_439 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44583503]\n",
      "Beta is  [0.44583503]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.4551 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_88 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_435 (Batch  (None, 57)          228         ['input_88[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_435 (Dense)              (None, 16)           928         ['batch_normalization_435[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_436 (Batch  (None, 16)          64          ['dense_435[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_348 (LeakyReLU)    (None, 16)           0           ['batch_normalization_436[0][0]']\n",
      "                                                                                                  \n",
      " dense_436 (Dense)              (None, 32)           544         ['leaky_re_lu_348[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_437 (Batch  (None, 32)          128         ['dense_436[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_349 (LeakyReLU)    (None, 32)           0           ['batch_normalization_437[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_349[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_349[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_87 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_437 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_438 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_350 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_438 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_439 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_351 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_439 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_88 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_435 (Batch  (None, 57)          228         ['input_88[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_435 (Dense)              (None, 16)           928         ['batch_normalization_435[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_436 (Batch  (None, 16)          64          ['dense_435[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_348 (LeakyReLU)    (None, 16)           0           ['batch_normalization_436[0][0]']\n",
      "                                                                                                  \n",
      " dense_436 (Dense)              (None, 32)           544         ['leaky_re_lu_348[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_437 (Batch  (None, 32)          128         ['dense_436[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_349 (LeakyReLU)    (None, 32)           0           ['batch_normalization_437[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_349[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_349[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_87 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_437 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_438 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_350 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_438 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_439 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_351 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_439 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_89 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_440 (Batch  (None, 57)          228         ['input_89[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_440 (Dense)              (None, 16)           928         ['batch_normalization_440[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_441 (Batch  (None, 16)          64          ['dense_440[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_352 (LeakyReLU)    (None, 16)           0           ['batch_normalization_441[0][0]']\n",
      "                                                                                                  \n",
      " dense_441 (Dense)              (None, 32)           544         ['leaky_re_lu_352[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_442 (Batch  (None, 32)          128         ['dense_441[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_353 (LeakyReLU)    (None, 32)           0           ['batch_normalization_442[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_353[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_353[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_88 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_442 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_443 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_354 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_443 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_444 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_355 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_444 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.69015871]\n",
      "Beta is  [0.69015871]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 0.8439 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_89 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_440 (Batch  (None, 57)          228         ['input_89[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_440 (Dense)              (None, 16)           928         ['batch_normalization_440[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_441 (Batch  (None, 16)          64          ['dense_440[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_352 (LeakyReLU)    (None, 16)           0           ['batch_normalization_441[0][0]']\n",
      "                                                                                                  \n",
      " dense_441 (Dense)              (None, 32)           544         ['leaky_re_lu_352[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_442 (Batch  (None, 32)          128         ['dense_441[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_353 (LeakyReLU)    (None, 32)           0           ['batch_normalization_442[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_353[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_353[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_88 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_442 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_443 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_354 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_443 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_444 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_355 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_444 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_89 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_440 (Batch  (None, 57)          228         ['input_89[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_440 (Dense)              (None, 16)           928         ['batch_normalization_440[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_441 (Batch  (None, 16)          64          ['dense_440[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_352 (LeakyReLU)    (None, 16)           0           ['batch_normalization_441[0][0]']\n",
      "                                                                                                  \n",
      " dense_441 (Dense)              (None, 32)           544         ['leaky_re_lu_352[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_442 (Batch  (None, 32)          128         ['dense_441[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_353 (LeakyReLU)    (None, 32)           0           ['batch_normalization_442[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_353[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_353[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_88 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_442 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_443 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_354 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_443 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_444 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_355 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_444 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_90 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_445 (Batch  (None, 57)          228         ['input_90[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_445 (Dense)              (None, 16)           928         ['batch_normalization_445[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_446 (Batch  (None, 16)          64          ['dense_445[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_356 (LeakyReLU)    (None, 16)           0           ['batch_normalization_446[0][0]']\n",
      "                                                                                                  \n",
      " dense_446 (Dense)              (None, 32)           544         ['leaky_re_lu_356[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_447 (Batch  (None, 32)          128         ['dense_446[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_357 (LeakyReLU)    (None, 32)           0           ['batch_normalization_447[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_357[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_357[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_89 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_447 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_448 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_358 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_448 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_449 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_359 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_449 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44580126]\n",
      "Beta is  [0.44580126]\n",
      "  9/391 [..............................] - ETA: 13s - loss: 2.3164 - reconstruction_loss: 1.8165 - kl_loss: 0.4929Batch 9: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 1.7966 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_90 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_445 (Batch  (None, 57)          228         ['input_90[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_445 (Dense)              (None, 16)           928         ['batch_normalization_445[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_446 (Batch  (None, 16)          64          ['dense_445[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_356 (LeakyReLU)    (None, 16)           0           ['batch_normalization_446[0][0]']\n",
      "                                                                                                  \n",
      " dense_446 (Dense)              (None, 32)           544         ['leaky_re_lu_356[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_447 (Batch  (None, 32)          128         ['dense_446[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_357 (LeakyReLU)    (None, 32)           0           ['batch_normalization_447[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_357[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_357[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_89 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_447 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_448 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_358 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_448 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_449 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_359 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_449 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_90 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_445 (Batch  (None, 57)          228         ['input_90[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_445 (Dense)              (None, 16)           928         ['batch_normalization_445[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_446 (Batch  (None, 16)          64          ['dense_445[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_356 (LeakyReLU)    (None, 16)           0           ['batch_normalization_446[0][0]']\n",
      "                                                                                                  \n",
      " dense_446 (Dense)              (None, 32)           544         ['leaky_re_lu_356[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_447 (Batch  (None, 32)          128         ['dense_446[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_357 (LeakyReLU)    (None, 32)           0           ['batch_normalization_447[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_357[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_357[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_89 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_447 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_448 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_358 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_448 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_449 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_359 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_449 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_91 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_450 (Batch  (None, 57)          228         ['input_91[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_450 (Dense)              (None, 16)           928         ['batch_normalization_450[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_451 (Batch  (None, 16)          64          ['dense_450[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_360 (LeakyReLU)    (None, 16)           0           ['batch_normalization_451[0][0]']\n",
      "                                                                                                  \n",
      " dense_451 (Dense)              (None, 32)           544         ['leaky_re_lu_360[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_452 (Batch  (None, 32)          128         ['dense_451[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_361 (LeakyReLU)    (None, 32)           0           ['batch_normalization_452[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_361[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_361[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_90 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_452 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_453 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_362 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_453 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_454 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_363 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_454 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.44588122]\n",
      "Beta is  [0.44588122]\n",
      "  1/391 [..............................] - ETA: 16:29 - loss: 2.4481 - reconstruction_loss: 1.8390 - kl_loss: 0.6090Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5897 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_91 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_450 (Batch  (None, 57)          228         ['input_91[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_450 (Dense)              (None, 16)           928         ['batch_normalization_450[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_451 (Batch  (None, 16)          64          ['dense_450[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_360 (LeakyReLU)    (None, 16)           0           ['batch_normalization_451[0][0]']\n",
      "                                                                                                  \n",
      " dense_451 (Dense)              (None, 32)           544         ['leaky_re_lu_360[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_452 (Batch  (None, 32)          128         ['dense_451[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_361 (LeakyReLU)    (None, 32)           0           ['batch_normalization_452[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_361[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_361[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_90 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_452 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_453 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_362 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_453 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_454 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_363 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_454 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_91 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_450 (Batch  (None, 57)          228         ['input_91[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_450 (Dense)              (None, 16)           928         ['batch_normalization_450[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_451 (Batch  (None, 16)          64          ['dense_450[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_360 (LeakyReLU)    (None, 16)           0           ['batch_normalization_451[0][0]']\n",
      "                                                                                                  \n",
      " dense_451 (Dense)              (None, 32)           544         ['leaky_re_lu_360[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_452 (Batch  (None, 32)          128         ['dense_451[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_361 (LeakyReLU)    (None, 32)           0           ['batch_normalization_452[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_361[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_361[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_90 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_452 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_453 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_362 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_453 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_454 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_363 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_454 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_92 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_455 (Batch  (None, 57)          228         ['input_92[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_455 (Dense)              (None, 16)           928         ['batch_normalization_455[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_456 (Batch  (None, 16)          64          ['dense_455[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_364 (LeakyReLU)    (None, 16)           0           ['batch_normalization_456[0][0]']\n",
      "                                                                                                  \n",
      " dense_456 (Dense)              (None, 32)           544         ['leaky_re_lu_364[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_457 (Batch  (None, 32)          128         ['dense_456[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_365 (LeakyReLU)    (None, 32)           0           ['batch_normalization_457[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_365[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_365[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_91 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_457 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_458 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_366 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_458 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_459 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_367 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_459 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.44586287]\n",
      "Beta is  [0.44586287]\n",
      "  9/391 [..............................] - ETA: 12s - loss: 2.2024 - reconstruction_loss: 1.7180 - kl_loss: 0.4715Batch 10: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 1.6702 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_92 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_455 (Batch  (None, 57)          228         ['input_92[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_455 (Dense)              (None, 16)           928         ['batch_normalization_455[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_456 (Batch  (None, 16)          64          ['dense_455[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_364 (LeakyReLU)    (None, 16)           0           ['batch_normalization_456[0][0]']\n",
      "                                                                                                  \n",
      " dense_456 (Dense)              (None, 32)           544         ['leaky_re_lu_364[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_457 (Batch  (None, 32)          128         ['dense_456[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_365 (LeakyReLU)    (None, 32)           0           ['batch_normalization_457[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_365[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_365[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_91 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_457 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_458 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_366 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_458 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_459 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_367 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_459 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_92 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_455 (Batch  (None, 57)          228         ['input_92[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_455 (Dense)              (None, 16)           928         ['batch_normalization_455[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_456 (Batch  (None, 16)          64          ['dense_455[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_364 (LeakyReLU)    (None, 16)           0           ['batch_normalization_456[0][0]']\n",
      "                                                                                                  \n",
      " dense_456 (Dense)              (None, 32)           544         ['leaky_re_lu_364[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_457 (Batch  (None, 32)          128         ['dense_456[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_365 (LeakyReLU)    (None, 32)           0           ['batch_normalization_457[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_365[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_365[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_91 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_457 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_458 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_366 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_458 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_459 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_367 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_459 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_93 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_460 (Batch  (None, 57)          228         ['input_93[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_460 (Dense)              (None, 16)           928         ['batch_normalization_460[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_461 (Batch  (None, 16)          64          ['dense_460[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_368 (LeakyReLU)    (None, 16)           0           ['batch_normalization_461[0][0]']\n",
      "                                                                                                  \n",
      " dense_461 (Dense)              (None, 32)           544         ['leaky_re_lu_368[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_462 (Batch  (None, 32)          128         ['dense_461[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_369 (LeakyReLU)    (None, 32)           0           ['batch_normalization_462[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_369[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_369[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_92 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_462 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_463 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_370 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_463 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_464 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_371 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_464 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.69035906]\n",
      "Beta is  [0.69035906]\n",
      "391/391 [==============================] - 17s 36ms/step - loss: 0.9019 - reconstruction_loss: 0.5459 - kl_loss: 0.0947 - val_loss: 0.4751 - val_reconstruction_loss: 0.4562 - val_kl_loss: 0.0189 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4603 - reconstruction_loss: 0.4472 - kl_loss: 0.0091 - val_loss: 0.4511 - val_reconstruction_loss: 0.4433 - val_kl_loss: 0.0077 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4493 - reconstruction_loss: 0.4425 - kl_loss: 0.0048 - val_loss: 0.4445 - val_reconstruction_loss: 0.4393 - val_kl_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4430 - reconstruction_loss: 0.4407 - kl_loss: 0.0035 - val_loss: 0.4427 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0046 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4402 - reconstruction_loss: 0.4398 - kl_loss: 0.0028 - val_loss: 0.4418 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4401 - reconstruction_loss: 0.4394 - kl_loss: 0.0025 - val_loss: 0.4413 - val_reconstruction_loss: 0.4385 - val_kl_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4393 - reconstruction_loss: 0.4391 - kl_loss: 0.0022 - val_loss: 0.4411 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4415 - reconstruction_loss: 0.4390 - kl_loss: 0.0021 - val_loss: 0.4425 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4394 - reconstruction_loss: 0.4387 - kl_loss: 0.0021\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4394 - reconstruction_loss: 0.4388 - kl_loss: 0.0021 - val_loss: 0.4410 - val_reconstruction_loss: 0.4387 - val_kl_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4394 - reconstruction_loss: 0.4385 - kl_loss: 0.0017 - val_loss: 0.4407 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0026 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4400 - reconstruction_loss: 0.4383 - kl_loss: 0.0019 - val_loss: 0.4407 - val_reconstruction_loss: 0.4379 - val_kl_loss: 0.0028 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4388 - reconstruction_loss: 0.4386 - kl_loss: 0.0018\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4388 - reconstruction_loss: 0.4385 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0026 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4427 - reconstruction_loss: 0.4384 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4407 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4380 - val_kl_loss: 0.0027 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4383 - reconstruction_loss: 0.4382 - kl_loss: 0.0018\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4383 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0026 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4381 - reconstruction_loss: 0.4384 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4380 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4422 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4408 - val_reconstruction_loss: 0.4380 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 18/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4384 - reconstruction_loss: 0.4381 - kl_loss: 0.0018\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4384 - reconstruction_loss: 0.4380 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4384 - val_kl_loss: 0.0024 - lr: 1.0000e-06\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4384 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0025 - lr: 1.0000e-06\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4488 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4379 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4379 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4398 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4380 - reconstruction_loss: 0.4385 - kl_loss: 0.0018 - val_loss: 0.4409 - val_reconstruction_loss: 0.4383 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4376 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4408 - val_reconstruction_loss: 0.4384 - val_kl_loss: 0.0024 - lr: 1.0000e-06\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4393 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4378 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4410 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4408 - val_reconstruction_loss: 0.4379 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4392 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4379 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4391 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4380 - val_kl_loss: 0.0027 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4424 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4379 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4405 - reconstruction_loss: 0.4382 - kl_loss: 0.0018 - val_loss: 0.4405 - val_reconstruction_loss: 0.4377 - val_kl_loss: 0.0029 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4381 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4376 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4405 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0024 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4386 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4382 - val_kl_loss: 0.0025 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4420 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4379 - val_kl_loss: 0.0027 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4522 - reconstruction_loss: 0.4385 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4378 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4436 - reconstruction_loss: 0.4383 - kl_loss: 0.0018 - val_loss: 0.4405 - val_reconstruction_loss: 0.4378 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4406 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4416 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4407 - val_reconstruction_loss: 0.4380 - val_kl_loss: 0.0028 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4398 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4406 - val_reconstruction_loss: 0.4380 - val_kl_loss: 0.0026 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4380 - reconstruction_loss: 0.4381 - kl_loss: 0.0018Restoring model weights from the end of the best epoch: 30.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.4380 - reconstruction_loss: 0.4381 - kl_loss: 0.0018 - val_loss: 0.4405 - val_reconstruction_loss: 0.4381 - val_kl_loss: 0.0025 - lr: 1.0000e-06\n",
      "Epoch 00040: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_93 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_460 (Batch  (None, 57)          228         ['input_93[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_460 (Dense)              (None, 16)           928         ['batch_normalization_460[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_461 (Batch  (None, 16)          64          ['dense_460[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_368 (LeakyReLU)    (None, 16)           0           ['batch_normalization_461[0][0]']\n",
      "                                                                                                  \n",
      " dense_461 (Dense)              (None, 32)           544         ['leaky_re_lu_368[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_462 (Batch  (None, 32)          128         ['dense_461[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_369 (LeakyReLU)    (None, 32)           0           ['batch_normalization_462[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_369[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_369[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_92 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_462 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_463 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_370 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_463 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_464 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_371 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_464 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_93 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_460 (Batch  (None, 57)          228         ['input_93[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_460 (Dense)              (None, 16)           928         ['batch_normalization_460[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_461 (Batch  (None, 16)          64          ['dense_460[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_368 (LeakyReLU)    (None, 16)           0           ['batch_normalization_461[0][0]']\n",
      "                                                                                                  \n",
      " dense_461 (Dense)              (None, 32)           544         ['leaky_re_lu_368[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_462 (Batch  (None, 32)          128         ['dense_461[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_369 (LeakyReLU)    (None, 32)           0           ['batch_normalization_462[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_369[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_369[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_92 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_462 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_463 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_370 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_463 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_464 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_371 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_464 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_94 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_465 (Batch  (None, 57)          228         ['input_94[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_465 (Dense)              (None, 16)           928         ['batch_normalization_465[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_466 (Batch  (None, 16)          64          ['dense_465[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_372 (LeakyReLU)    (None, 16)           0           ['batch_normalization_466[0][0]']\n",
      "                                                                                                  \n",
      " dense_466 (Dense)              (None, 32)           544         ['leaky_re_lu_372[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_467 (Batch  (None, 32)          128         ['dense_466[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_373 (LeakyReLU)    (None, 32)           0           ['batch_normalization_467[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_373[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_373[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_93 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_467 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_468 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_374 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_468 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_469 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_375 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_469 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.4691989]\n",
      "Beta is  [0.4691989]\n",
      " 13/391 [..............................] - ETA: 13s - loss: 2.8716 - reconstruction_loss: 1.6365 - kl_loss: 0.8812Batch 14: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 1.6320 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_94 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_465 (Batch  (None, 57)          228         ['input_94[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_465 (Dense)              (None, 16)           928         ['batch_normalization_465[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_466 (Batch  (None, 16)          64          ['dense_465[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_372 (LeakyReLU)    (None, 16)           0           ['batch_normalization_466[0][0]']\n",
      "                                                                                                  \n",
      " dense_466 (Dense)              (None, 32)           544         ['leaky_re_lu_372[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_467 (Batch  (None, 32)          128         ['dense_466[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_373 (LeakyReLU)    (None, 32)           0           ['batch_normalization_467[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_373[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_373[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_93 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_467 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_468 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_374 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_468 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_469 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_375 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_469 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_94 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_465 (Batch  (None, 57)          228         ['input_94[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_465 (Dense)              (None, 16)           928         ['batch_normalization_465[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_466 (Batch  (None, 16)          64          ['dense_465[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_372 (LeakyReLU)    (None, 16)           0           ['batch_normalization_466[0][0]']\n",
      "                                                                                                  \n",
      " dense_466 (Dense)              (None, 32)           544         ['leaky_re_lu_372[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_467 (Batch  (None, 32)          128         ['dense_466[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_373 (LeakyReLU)    (None, 32)           0           ['batch_normalization_467[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_373[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_373[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_93 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_467 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_468 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_374 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_468 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_469 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_375 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_469 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_95 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_470 (Batch  (None, 57)          228         ['input_95[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_470 (Dense)              (None, 16)           928         ['batch_normalization_470[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_471 (Batch  (None, 16)          64          ['dense_470[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_376 (LeakyReLU)    (None, 16)           0           ['batch_normalization_471[0][0]']\n",
      "                                                                                                  \n",
      " dense_471 (Dense)              (None, 64)           1088        ['leaky_re_lu_376[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_472 (Batch  (None, 64)          256         ['dense_471[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_377 (LeakyReLU)    (None, 64)           0           ['batch_normalization_472[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_377[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_377[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_94 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_472 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_473 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_378 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_473 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_474 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_379 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_474 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.54593665]\n",
      "Beta is  [0.54593665]\n",
      "  1/391 [..............................] - ETA: 17:20 - loss: 3.3803 - reconstruction_loss: 1.4645 - kl_loss: 1.9157Batch 2: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.3832 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_95 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_470 (Batch  (None, 57)          228         ['input_95[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_470 (Dense)              (None, 16)           928         ['batch_normalization_470[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_471 (Batch  (None, 16)          64          ['dense_470[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_376 (LeakyReLU)    (None, 16)           0           ['batch_normalization_471[0][0]']\n",
      "                                                                                                  \n",
      " dense_471 (Dense)              (None, 64)           1088        ['leaky_re_lu_376[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_472 (Batch  (None, 64)          256         ['dense_471[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_377 (LeakyReLU)    (None, 64)           0           ['batch_normalization_472[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_377[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_377[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_94 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_472 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_473 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_378 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_473 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_474 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_379 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_474 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_95 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_470 (Batch  (None, 57)          228         ['input_95[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_470 (Dense)              (None, 16)           928         ['batch_normalization_470[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_471 (Batch  (None, 16)          64          ['dense_470[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_376 (LeakyReLU)    (None, 16)           0           ['batch_normalization_471[0][0]']\n",
      "                                                                                                  \n",
      " dense_471 (Dense)              (None, 64)           1088        ['leaky_re_lu_376[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_472 (Batch  (None, 64)          256         ['dense_471[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_377 (LeakyReLU)    (None, 64)           0           ['batch_normalization_472[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_377[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_377[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_94 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_472 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_473 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_378 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_473 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_474 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_379 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_474 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_96 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_475 (Batch  (None, 57)          228         ['input_96[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_475 (Dense)              (None, 16)           928         ['batch_normalization_475[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_476 (Batch  (None, 16)          64          ['dense_475[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_380 (LeakyReLU)    (None, 16)           0           ['batch_normalization_476[0][0]']\n",
      "                                                                                                  \n",
      " dense_476 (Dense)              (None, 64)           1088        ['leaky_re_lu_380[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_477 (Batch  (None, 64)          256         ['dense_476[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_381 (LeakyReLU)    (None, 64)           0           ['batch_normalization_477[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_381[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_381[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_95 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_477 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_478 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_382 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_478 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_479 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_383 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_479 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.56760541]\n",
      "Beta is  [0.56760541]\n",
      "391/391 [==============================] - 17s 37ms/step - loss: 1.0941 - reconstruction_loss: 0.7340 - kl_loss: 0.0867 - val_loss: 0.6598 - val_reconstruction_loss: 0.6363 - val_kl_loss: 0.0235 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6361 - reconstruction_loss: 0.6151 - kl_loss: 0.0166 - val_loss: 0.6292 - val_reconstruction_loss: 0.6159 - val_kl_loss: 0.0133 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6257 - reconstruction_loss: 0.6102 - kl_loss: 0.0106 - val_loss: 0.6208 - val_reconstruction_loss: 0.6094 - val_kl_loss: 0.0115 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6179 - reconstruction_loss: 0.6083 - kl_loss: 0.0089 - val_loss: 0.6177 - val_reconstruction_loss: 0.6075 - val_kl_loss: 0.0103 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6178 - reconstruction_loss: 0.6072 - kl_loss: 0.0083 - val_loss: 0.6193 - val_reconstruction_loss: 0.6114 - val_kl_loss: 0.0079 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6170 - reconstruction_loss: 0.6062 - kl_loss: 0.0082 - val_loss: 0.6155 - val_reconstruction_loss: 0.6061 - val_kl_loss: 0.0095 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6129 - reconstruction_loss: 0.6053 - kl_loss: 0.0081 - val_loss: 0.6162 - val_reconstruction_loss: 0.6081 - val_kl_loss: 0.0082 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6135 - reconstruction_loss: 0.6045 - kl_loss: 0.0083\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6135 - reconstruction_loss: 0.6044 - kl_loss: 0.0083 - val_loss: 0.6163 - val_reconstruction_loss: 0.6084 - val_kl_loss: 0.0079 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6123 - reconstruction_loss: 0.6040 - kl_loss: 0.0079 - val_loss: 0.6148 - val_reconstruction_loss: 0.6060 - val_kl_loss: 0.0089 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6124 - reconstruction_loss: 0.6037 - kl_loss: 0.0082 - val_loss: 0.6142 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6128 - reconstruction_loss: 0.6036 - kl_loss: 0.0082 - val_loss: 0.6141 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6107 - reconstruction_loss: 0.6035 - kl_loss: 0.0082 - val_loss: 0.6143 - val_reconstruction_loss: 0.6055 - val_kl_loss: 0.0088 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6111 - reconstruction_loss: 0.6035 - kl_loss: 0.0082 - val_loss: 0.6138 - val_reconstruction_loss: 0.6047 - val_kl_loss: 0.0091 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6122 - reconstruction_loss: 0.6031 - kl_loss: 0.0082 - val_loss: 0.6141 - val_reconstruction_loss: 0.6049 - val_kl_loss: 0.0093 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6103 - reconstruction_loss: 0.6029 - kl_loss: 0.0084\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6103 - reconstruction_loss: 0.6029 - kl_loss: 0.0084 - val_loss: 0.6140 - val_reconstruction_loss: 0.6050 - val_kl_loss: 0.0090 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6124 - reconstruction_loss: 0.6026 - kl_loss: 0.0085 - val_loss: 0.6137 - val_reconstruction_loss: 0.6045 - val_kl_loss: 0.0093 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6107 - reconstruction_loss: 0.6030 - kl_loss: 0.0084 - val_loss: 0.6142 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0089 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6136 - reconstruction_loss: 0.6027 - kl_loss: 0.0085 - val_loss: 0.6136 - val_reconstruction_loss: 0.6044 - val_kl_loss: 0.0092 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6119 - reconstruction_loss: 0.6029 - kl_loss: 0.0084 - val_loss: 0.6135 - val_reconstruction_loss: 0.6044 - val_kl_loss: 0.0092 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6110 - reconstruction_loss: 0.6025 - kl_loss: 0.0084\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6110 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6138 - val_reconstruction_loss: 0.6048 - val_kl_loss: 0.0091 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6115 - reconstruction_loss: 0.6030 - kl_loss: 0.0083 - val_loss: 0.6140 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6125 - reconstruction_loss: 0.6030 - kl_loss: 0.0083 - val_loss: 0.6135 - val_reconstruction_loss: 0.6046 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 23/150\n",
      "389/391 [============================>.] - ETA: 0s - loss: 0.6116 - reconstruction_loss: 0.6030 - kl_loss: 0.0083\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6116 - reconstruction_loss: 0.6029 - kl_loss: 0.0083 - val_loss: 0.6140 - val_reconstruction_loss: 0.6052 - val_kl_loss: 0.0088 - lr: 1.0000e-06\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6116 - reconstruction_loss: 0.6031 - kl_loss: 0.0083 - val_loss: 0.6136 - val_reconstruction_loss: 0.6043 - val_kl_loss: 0.0093 - lr: 1.0000e-06\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6112 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6135 - val_reconstruction_loss: 0.6045 - val_kl_loss: 0.0091 - lr: 1.0000e-06\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6106 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6138 - val_reconstruction_loss: 0.6049 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6115 - reconstruction_loss: 0.6026 - kl_loss: 0.0084 - val_loss: 0.6135 - val_reconstruction_loss: 0.6044 - val_kl_loss: 0.0092 - lr: 1.0000e-06\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6101 - reconstruction_loss: 0.6027 - kl_loss: 0.0083 - val_loss: 0.6133 - val_reconstruction_loss: 0.6042 - val_kl_loss: 0.0092 - lr: 1.0000e-06\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6095 - reconstruction_loss: 0.6030 - kl_loss: 0.0083 - val_loss: 0.6144 - val_reconstruction_loss: 0.6056 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6098 - reconstruction_loss: 0.6028 - kl_loss: 0.0083 - val_loss: 0.6142 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6140 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6134 - val_reconstruction_loss: 0.6041 - val_kl_loss: 0.0094 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6116 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6137 - val_reconstruction_loss: 0.6046 - val_kl_loss: 0.0091 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6110 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6136 - val_reconstruction_loss: 0.6047 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 12s 32ms/step - loss: 0.6105 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6137 - val_reconstruction_loss: 0.6047 - val_kl_loss: 0.0091 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 6s 15ms/step - loss: 0.6100 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6139 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 13s 34ms/step - loss: 0.6093 - reconstruction_loss: 0.6029 - kl_loss: 0.0084 - val_loss: 0.6144 - val_reconstruction_loss: 0.6057 - val_kl_loss: 0.0087 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6107 - reconstruction_loss: 0.6029 - kl_loss: 0.0084 - val_loss: 0.6144 - val_reconstruction_loss: 0.6056 - val_kl_loss: 0.0088 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6144 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6133 - val_reconstruction_loss: 0.6041 - val_kl_loss: 0.0093 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6126 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6135 - val_reconstruction_loss: 0.6042 - val_kl_loss: 0.0093 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6110 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6136 - val_reconstruction_loss: 0.6048 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6099 - reconstruction_loss: 0.6025 - kl_loss: 0.0084 - val_loss: 0.6137 - val_reconstruction_loss: 0.6048 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6107 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6136 - val_reconstruction_loss: 0.6045 - val_kl_loss: 0.0092 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6124 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6139 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6103 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6140 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6107 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6130 - val_reconstruction_loss: 0.6038 - val_kl_loss: 0.0093 - lr: 1.0000e-06\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6108 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6140 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6092 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6146 - val_reconstruction_loss: 0.6060 - val_kl_loss: 0.0086 - lr: 1.0000e-06\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6115 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6136 - val_reconstruction_loss: 0.6047 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6132 - reconstruction_loss: 0.6027 - kl_loss: 0.0084 - val_loss: 0.6136 - val_reconstruction_loss: 0.6046 - val_kl_loss: 0.0091 - lr: 1.0000e-06\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6100 - reconstruction_loss: 0.6029 - kl_loss: 0.0084 - val_loss: 0.6137 - val_reconstruction_loss: 0.6048 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6116 - reconstruction_loss: 0.6030 - kl_loss: 0.0084 - val_loss: 0.6136 - val_reconstruction_loss: 0.6045 - val_kl_loss: 0.0092 - lr: 1.0000e-06\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6127 - reconstruction_loss: 0.6026 - kl_loss: 0.0084 - val_loss: 0.6135 - val_reconstruction_loss: 0.6044 - val_kl_loss: 0.0091 - lr: 1.0000e-06\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6102 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6139 - val_reconstruction_loss: 0.6051 - val_kl_loss: 0.0089 - lr: 1.0000e-06\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 14s 35ms/step - loss: 0.6106 - reconstruction_loss: 0.6028 - kl_loss: 0.0084 - val_loss: 0.6135 - val_reconstruction_loss: 0.6046 - val_kl_loss: 0.0090 - lr: 1.0000e-06\n",
      "Epoch 55/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6097 - reconstruction_loss: 0.6028 - kl_loss: 0.0084Restoring model weights from the end of the best epoch: 45.\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.6097 - reconstruction_loss: 0.6029 - kl_loss: 0.0084 - val_loss: 0.6142 - val_reconstruction_loss: 0.6054 - val_kl_loss: 0.0088 - lr: 1.0000e-06\n",
      "Epoch 00055: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_96 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_475 (Batch  (None, 57)          228         ['input_96[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_475 (Dense)              (None, 16)           928         ['batch_normalization_475[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_476 (Batch  (None, 16)          64          ['dense_475[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_380 (LeakyReLU)    (None, 16)           0           ['batch_normalization_476[0][0]']\n",
      "                                                                                                  \n",
      " dense_476 (Dense)              (None, 64)           1088        ['leaky_re_lu_380[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_477 (Batch  (None, 64)          256         ['dense_476[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_381 (LeakyReLU)    (None, 64)           0           ['batch_normalization_477[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_381[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_381[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_95 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_477 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_478 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_382 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_478 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_479 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_383 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_479 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_96 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_475 (Batch  (None, 57)          228         ['input_96[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_475 (Dense)              (None, 16)           928         ['batch_normalization_475[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_476 (Batch  (None, 16)          64          ['dense_475[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_380 (LeakyReLU)    (None, 16)           0           ['batch_normalization_476[0][0]']\n",
      "                                                                                                  \n",
      " dense_476 (Dense)              (None, 64)           1088        ['leaky_re_lu_380[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_477 (Batch  (None, 64)          256         ['dense_476[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_381 (LeakyReLU)    (None, 64)           0           ['batch_normalization_477[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            520         ['leaky_re_lu_381[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            520         ['leaky_re_lu_381[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_95 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,604\n",
      "Trainable params: 3,330\n",
      "Non-trainable params: 274\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_477 (Dense)           (None, 64)                576       \n",
      "                                                                 \n",
      " batch_normalization_478 (Ba  (None, 64)               256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_382 (LeakyReLU)  (None, 64)               0         \n",
      "                                                                 \n",
      " dense_478 (Dense)           (None, 16)                1040      \n",
      "                                                                 \n",
      " batch_normalization_479 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_383 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_479 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,905\n",
      "Trainable params: 2,745\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_97 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_480 (Batch  (None, 57)          228         ['input_97[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_480 (Dense)              (None, 16)           928         ['batch_normalization_480[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_481 (Batch  (None, 16)          64          ['dense_480[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_384 (LeakyReLU)    (None, 16)           0           ['batch_normalization_481[0][0]']\n",
      "                                                                                                  \n",
      " dense_481 (Dense)              (None, 32)           544         ['leaky_re_lu_384[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_482 (Batch  (None, 32)          128         ['dense_481[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_385 (LeakyReLU)    (None, 32)           0           ['batch_normalization_482[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_385[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_385[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_96 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_482 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_483 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_386 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_483 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_484 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_387 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_484 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.25755214]\n",
      "Beta is  [0.25755214]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.7659 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_97 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_480 (Batch  (None, 57)          228         ['input_97[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_480 (Dense)              (None, 16)           928         ['batch_normalization_480[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_481 (Batch  (None, 16)          64          ['dense_480[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_384 (LeakyReLU)    (None, 16)           0           ['batch_normalization_481[0][0]']\n",
      "                                                                                                  \n",
      " dense_481 (Dense)              (None, 32)           544         ['leaky_re_lu_384[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_482 (Batch  (None, 32)          128         ['dense_481[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_385 (LeakyReLU)    (None, 32)           0           ['batch_normalization_482[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_385[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_385[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_96 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_482 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_483 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_386 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_483 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_484 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_387 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_484 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_97 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_480 (Batch  (None, 57)          228         ['input_97[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_480 (Dense)              (None, 16)           928         ['batch_normalization_480[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_481 (Batch  (None, 16)          64          ['dense_480[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_384 (LeakyReLU)    (None, 16)           0           ['batch_normalization_481[0][0]']\n",
      "                                                                                                  \n",
      " dense_481 (Dense)              (None, 32)           544         ['leaky_re_lu_384[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_482 (Batch  (None, 32)          128         ['dense_481[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_385 (LeakyReLU)    (None, 32)           0           ['batch_normalization_482[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_385[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_385[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_96 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_482 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_483 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_386 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_483 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_484 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_387 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_484 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_98 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_485 (Batch  (None, 57)          228         ['input_98[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_485 (Dense)              (None, 16)           928         ['batch_normalization_485[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_486 (Batch  (None, 16)          64          ['dense_485[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_388 (LeakyReLU)    (None, 16)           0           ['batch_normalization_486[0][0]']\n",
      "                                                                                                  \n",
      " dense_486 (Dense)              (None, 32)           544         ['leaky_re_lu_388[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_487 (Batch  (None, 32)          128         ['dense_486[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_389 (LeakyReLU)    (None, 32)           0           ['batch_normalization_487[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_389[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_389[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_97 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_487 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_488 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_390 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_488 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_489 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_391 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_489 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.33036304]\n",
      "Beta is  [0.33036304]\n",
      "  3/391 [..............................] - ETA: 14s - loss: 2.8694 - reconstruction_loss: 2.1472 - kl_loss: 0.7548  Batch 3: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.0420 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_98 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_485 (Batch  (None, 57)          228         ['input_98[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_485 (Dense)              (None, 16)           928         ['batch_normalization_485[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_486 (Batch  (None, 16)          64          ['dense_485[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_388 (LeakyReLU)    (None, 16)           0           ['batch_normalization_486[0][0]']\n",
      "                                                                                                  \n",
      " dense_486 (Dense)              (None, 32)           544         ['leaky_re_lu_388[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_487 (Batch  (None, 32)          128         ['dense_486[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_389 (LeakyReLU)    (None, 32)           0           ['batch_normalization_487[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_389[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_389[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_97 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_487 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_488 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_390 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_488 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_489 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_391 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_489 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_98 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_485 (Batch  (None, 57)          228         ['input_98[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_485 (Dense)              (None, 16)           928         ['batch_normalization_485[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_486 (Batch  (None, 16)          64          ['dense_485[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_388 (LeakyReLU)    (None, 16)           0           ['batch_normalization_486[0][0]']\n",
      "                                                                                                  \n",
      " dense_486 (Dense)              (None, 32)           544         ['leaky_re_lu_388[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_487 (Batch  (None, 32)          128         ['dense_486[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_389 (LeakyReLU)    (None, 32)           0           ['batch_normalization_487[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_389[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_389[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_97 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_487 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_488 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_390 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_488 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_489 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_391 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_489 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_99 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_490 (Batch  (None, 57)          228         ['input_99[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_490 (Dense)              (None, 16)           928         ['batch_normalization_490[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_491 (Batch  (None, 16)          64          ['dense_490[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_392 (LeakyReLU)    (None, 16)           0           ['batch_normalization_491[0][0]']\n",
      "                                                                                                  \n",
      " dense_491 (Dense)              (None, 32)           544         ['leaky_re_lu_392[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_492 (Batch  (None, 32)          128         ['dense_491[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_393 (LeakyReLU)    (None, 32)           0           ['batch_normalization_492[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_393[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_393[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_98 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_492 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_493 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_394 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_493 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_494 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_395 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.39984523]\n",
      "Beta is  [0.39984523]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 5s 5ms/step - loss: inf - reconstruction_loss: 1.2939 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_99 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_490 (Batch  (None, 57)          228         ['input_99[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_490 (Dense)              (None, 16)           928         ['batch_normalization_490[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_491 (Batch  (None, 16)          64          ['dense_490[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_392 (LeakyReLU)    (None, 16)           0           ['batch_normalization_491[0][0]']\n",
      "                                                                                                  \n",
      " dense_491 (Dense)              (None, 32)           544         ['leaky_re_lu_392[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_492 (Batch  (None, 32)          128         ['dense_491[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_393 (LeakyReLU)    (None, 32)           0           ['batch_normalization_492[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_393[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_393[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_98 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_492 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_493 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_394 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_493 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_494 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_395 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_99 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_490 (Batch  (None, 57)          228         ['input_99[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_490 (Dense)              (None, 16)           928         ['batch_normalization_490[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_491 (Batch  (None, 16)          64          ['dense_490[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_392 (LeakyReLU)    (None, 16)           0           ['batch_normalization_491[0][0]']\n",
      "                                                                                                  \n",
      " dense_491 (Dense)              (None, 32)           544         ['leaky_re_lu_392[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_492 (Batch  (None, 32)          128         ['dense_491[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_393 (LeakyReLU)    (None, 32)           0           ['batch_normalization_492[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_393[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_393[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_98 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_492 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_493 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_394 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_493 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_494 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_395 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_100 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_495 (Batch  (None, 57)          228         ['input_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_495 (Dense)              (None, 16)           928         ['batch_normalization_495[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_496 (Batch  (None, 16)          64          ['dense_495[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_396 (LeakyReLU)    (None, 16)           0           ['batch_normalization_496[0][0]']\n",
      "                                                                                                  \n",
      " dense_496 (Dense)              (None, 32)           544         ['leaky_re_lu_396[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_497 (Batch  (None, 32)          128         ['dense_496[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_397 (LeakyReLU)    (None, 32)           0           ['batch_normalization_497[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_397[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_397[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_99 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_498 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_398 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_499 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_399 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.35058895]\n",
      "Beta is  [0.35058895]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.8373 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_100 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_495 (Batch  (None, 57)          228         ['input_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_495 (Dense)              (None, 16)           928         ['batch_normalization_495[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_496 (Batch  (None, 16)          64          ['dense_495[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_396 (LeakyReLU)    (None, 16)           0           ['batch_normalization_496[0][0]']\n",
      "                                                                                                  \n",
      " dense_496 (Dense)              (None, 32)           544         ['leaky_re_lu_396[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_497 (Batch  (None, 32)          128         ['dense_496[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_397 (LeakyReLU)    (None, 32)           0           ['batch_normalization_497[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_397[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_397[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_99 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_498 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_398 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_499 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_399 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_100 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_495 (Batch  (None, 57)          228         ['input_100[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_495 (Dense)              (None, 16)           928         ['batch_normalization_495[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_496 (Batch  (None, 16)          64          ['dense_495[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_396 (LeakyReLU)    (None, 16)           0           ['batch_normalization_496[0][0]']\n",
      "                                                                                                  \n",
      " dense_496 (Dense)              (None, 32)           544         ['leaky_re_lu_396[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_497 (Batch  (None, 32)          128         ['dense_496[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_397 (LeakyReLU)    (None, 32)           0           ['batch_normalization_497[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_397[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_397[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_99 (Sampling)         (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_498 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_398 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_499 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_399 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_101 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_500 (Batch  (None, 57)          228         ['input_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_500 (Dense)              (None, 16)           928         ['batch_normalization_500[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_501 (Batch  (None, 16)          64          ['dense_500[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_400 (LeakyReLU)    (None, 16)           0           ['batch_normalization_501[0][0]']\n",
      "                                                                                                  \n",
      " dense_501 (Dense)              (None, 32)           544         ['leaky_re_lu_400[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_502 (Batch  (None, 32)          128         ['dense_501[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_401 (LeakyReLU)    (None, 32)           0           ['batch_normalization_502[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_401[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_401[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_100 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_502 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_503 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_402 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_504 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_403 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_504 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.27720669]\n",
      "Beta is  [0.27720669]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.9791 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_101 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_500 (Batch  (None, 57)          228         ['input_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_500 (Dense)              (None, 16)           928         ['batch_normalization_500[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_501 (Batch  (None, 16)          64          ['dense_500[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_400 (LeakyReLU)    (None, 16)           0           ['batch_normalization_501[0][0]']\n",
      "                                                                                                  \n",
      " dense_501 (Dense)              (None, 32)           544         ['leaky_re_lu_400[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_502 (Batch  (None, 32)          128         ['dense_501[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_401 (LeakyReLU)    (None, 32)           0           ['batch_normalization_502[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_401[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_401[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_100 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_502 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_503 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_402 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_504 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_403 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_504 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_101 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_500 (Batch  (None, 57)          228         ['input_101[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_500 (Dense)              (None, 16)           928         ['batch_normalization_500[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_501 (Batch  (None, 16)          64          ['dense_500[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_400 (LeakyReLU)    (None, 16)           0           ['batch_normalization_501[0][0]']\n",
      "                                                                                                  \n",
      " dense_501 (Dense)              (None, 32)           544         ['leaky_re_lu_400[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_502 (Batch  (None, 32)          128         ['dense_501[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_401 (LeakyReLU)    (None, 32)           0           ['batch_normalization_502[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_401[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_401[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_100 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_502 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_503 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_402 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_504 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_403 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_504 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_102 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_505 (Batch  (None, 57)          228         ['input_102[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_505 (Dense)              (None, 16)           928         ['batch_normalization_505[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_506 (Batch  (None, 16)          64          ['dense_505[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_404 (LeakyReLU)    (None, 16)           0           ['batch_normalization_506[0][0]']\n",
      "                                                                                                  \n",
      " dense_506 (Dense)              (None, 32)           544         ['leaky_re_lu_404[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_507 (Batch  (None, 32)          128         ['dense_506[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_405 (LeakyReLU)    (None, 32)           0           ['batch_normalization_507[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_405[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_405[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_101 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_508 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_406 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_508 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_509 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_407 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_509 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.24666543]\n",
      "Beta is  [0.24666543]\n",
      " 20/391 [>.............................] - ETA: 12s - loss: 2.5962 - reconstruction_loss: 2.2656 - kl_loss: 0.2491Batch 20: Invalid loss, terminating training\n",
      "391/391 [==============================] - 5s 5ms/step - loss: inf - reconstruction_loss: 2.2446 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_102 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_505 (Batch  (None, 57)          228         ['input_102[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_505 (Dense)              (None, 16)           928         ['batch_normalization_505[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_506 (Batch  (None, 16)          64          ['dense_505[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_404 (LeakyReLU)    (None, 16)           0           ['batch_normalization_506[0][0]']\n",
      "                                                                                                  \n",
      " dense_506 (Dense)              (None, 32)           544         ['leaky_re_lu_404[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_507 (Batch  (None, 32)          128         ['dense_506[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_405 (LeakyReLU)    (None, 32)           0           ['batch_normalization_507[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_405[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_405[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_101 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_508 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_406 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_508 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_509 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_407 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_509 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_102 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_505 (Batch  (None, 57)          228         ['input_102[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_505 (Dense)              (None, 16)           928         ['batch_normalization_505[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_506 (Batch  (None, 16)          64          ['dense_505[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_404 (LeakyReLU)    (None, 16)           0           ['batch_normalization_506[0][0]']\n",
      "                                                                                                  \n",
      " dense_506 (Dense)              (None, 32)           544         ['leaky_re_lu_404[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_507 (Batch  (None, 32)          128         ['dense_506[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_405 (LeakyReLU)    (None, 32)           0           ['batch_normalization_507[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_405[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_405[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_101 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_508 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_406 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_508 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_509 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_407 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_509 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_103 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_510 (Batch  (None, 57)          228         ['input_103[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_510 (Dense)              (None, 16)           928         ['batch_normalization_510[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_511 (Batch  (None, 16)          64          ['dense_510[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_408 (LeakyReLU)    (None, 16)           0           ['batch_normalization_511[0][0]']\n",
      "                                                                                                  \n",
      " dense_511 (Dense)              (None, 32)           544         ['leaky_re_lu_408[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_512 (Batch  (None, 32)          128         ['dense_511[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_409 (LeakyReLU)    (None, 32)           0           ['batch_normalization_512[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_409[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_409[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_102 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_512 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_513 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_410 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_513 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_514 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_411 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_514 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.30753829]\n",
      "Beta is  [0.30753829]\n",
      "  3/391 [..............................] - ETA: 14s - loss: 2.7353 - reconstruction_loss: 2.3202 - kl_loss: 0.4613  Batch 4: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 4ms/step - loss: inf - reconstruction_loss: 2.3423 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_103 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_510 (Batch  (None, 57)          228         ['input_103[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_510 (Dense)              (None, 16)           928         ['batch_normalization_510[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_511 (Batch  (None, 16)          64          ['dense_510[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_408 (LeakyReLU)    (None, 16)           0           ['batch_normalization_511[0][0]']\n",
      "                                                                                                  \n",
      " dense_511 (Dense)              (None, 32)           544         ['leaky_re_lu_408[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_512 (Batch  (None, 32)          128         ['dense_511[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_409 (LeakyReLU)    (None, 32)           0           ['batch_normalization_512[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_409[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_409[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_102 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_512 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_513 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_410 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_513 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_514 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_411 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_514 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_103 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_510 (Batch  (None, 57)          228         ['input_103[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_510 (Dense)              (None, 16)           928         ['batch_normalization_510[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_511 (Batch  (None, 16)          64          ['dense_510[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_408 (LeakyReLU)    (None, 16)           0           ['batch_normalization_511[0][0]']\n",
      "                                                                                                  \n",
      " dense_511 (Dense)              (None, 32)           544         ['leaky_re_lu_408[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_512 (Batch  (None, 32)          128         ['dense_511[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_409 (LeakyReLU)    (None, 32)           0           ['batch_normalization_512[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_409[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_409[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_102 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_512 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_513 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_410 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_513 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_514 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_411 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_514 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_104 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_515 (Batch  (None, 57)          228         ['input_104[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_515 (Dense)              (None, 16)           928         ['batch_normalization_515[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_516 (Batch  (None, 16)          64          ['dense_515[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_412 (LeakyReLU)    (None, 16)           0           ['batch_normalization_516[0][0]']\n",
      "                                                                                                  \n",
      " dense_516 (Dense)              (None, 32)           544         ['leaky_re_lu_412[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_517 (Batch  (None, 32)          128         ['dense_516[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_413 (LeakyReLU)    (None, 32)           0           ['batch_normalization_517[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_413[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_413[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_103 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_517 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_518 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_414 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_518 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_519 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_415 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_519 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Beta is  [0.35960947]\n",
      "Beta is  [0.35960947]\n",
      "  1/391 [..............................] - ETA: 17:58 - loss: 2.5656 - reconstruction_loss: 2.1378 - kl_loss: 0.4278Batch 1: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 2.0739 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_104 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_515 (Batch  (None, 57)          228         ['input_104[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_515 (Dense)              (None, 16)           928         ['batch_normalization_515[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_516 (Batch  (None, 16)          64          ['dense_515[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_412 (LeakyReLU)    (None, 16)           0           ['batch_normalization_516[0][0]']\n",
      "                                                                                                  \n",
      " dense_516 (Dense)              (None, 32)           544         ['leaky_re_lu_412[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_517 (Batch  (None, 32)          128         ['dense_516[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_413 (LeakyReLU)    (None, 32)           0           ['batch_normalization_517[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_413[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_413[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_103 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_517 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_518 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_414 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_518 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_519 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_415 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_519 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_104 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_515 (Batch  (None, 57)          228         ['input_104[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_515 (Dense)              (None, 16)           928         ['batch_normalization_515[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_516 (Batch  (None, 16)          64          ['dense_515[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_412 (LeakyReLU)    (None, 16)           0           ['batch_normalization_516[0][0]']\n",
      "                                                                                                  \n",
      " dense_516 (Dense)              (None, 32)           544         ['leaky_re_lu_412[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_517 (Batch  (None, 32)          128         ['dense_516[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_413 (LeakyReLU)    (None, 32)           0           ['batch_normalization_517[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_413[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_413[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_103 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_517 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_518 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_414 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_518 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_519 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_415 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_519 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_105 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_520 (Batch  (None, 57)          228         ['input_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_520 (Dense)              (None, 16)           928         ['batch_normalization_520[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_521 (Batch  (None, 16)          64          ['dense_520[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_416 (LeakyReLU)    (None, 16)           0           ['batch_normalization_521[0][0]']\n",
      "                                                                                                  \n",
      " dense_521 (Dense)              (None, 32)           544         ['leaky_re_lu_416[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_522 (Batch  (None, 32)          128         ['dense_521[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_417 (LeakyReLU)    (None, 32)           0           ['batch_normalization_522[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_417[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_417[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_104 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_522 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_523 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_418 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_524 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_419 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_524 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Beta is  [0.2312555]\n",
      "Beta is  [0.2312555]\n",
      "Batch 0: Invalid loss, terminating training\n",
      "391/391 [==============================] - 4s 3ms/step - loss: inf - reconstruction_loss: 1.5517 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_105 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_520 (Batch  (None, 57)          228         ['input_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_520 (Dense)              (None, 16)           928         ['batch_normalization_520[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_521 (Batch  (None, 16)          64          ['dense_520[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_416 (LeakyReLU)    (None, 16)           0           ['batch_normalization_521[0][0]']\n",
      "                                                                                                  \n",
      " dense_521 (Dense)              (None, 32)           544         ['leaky_re_lu_416[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_522 (Batch  (None, 32)          128         ['dense_521[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_417 (LeakyReLU)    (None, 32)           0           ['batch_normalization_522[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_417[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_417[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_104 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_522 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_523 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_418 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_524 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_419 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_524 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_105 (InputLayer)         [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_520 (Batch  (None, 57)          228         ['input_105[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_520 (Dense)              (None, 16)           928         ['batch_normalization_520[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_521 (Batch  (None, 16)          64          ['dense_520[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_416 (LeakyReLU)    (None, 16)           0           ['batch_normalization_521[0][0]']\n",
      "                                                                                                  \n",
      " dense_521 (Dense)              (None, 32)           544         ['leaky_re_lu_416[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_522 (Batch  (None, 32)          128         ['dense_521[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu_417 (LeakyReLU)    (None, 32)           0           ['batch_normalization_522[0][0]']\n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 8)            264         ['leaky_re_lu_417[0][0]']        \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 8)            264         ['leaky_re_lu_417[0][0]']        \n",
      "                                                                                                  \n",
      " sampling_104 (Sampling)        (None, 8)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,420\n",
      "Trainable params: 2,210\n",
      "Non-trainable params: 210\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_522 (Dense)           (None, 32)                288       \n",
      "                                                                 \n",
      " batch_normalization_523 (Ba  (None, 32)               128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_418 (LeakyReLU)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " batch_normalization_524 (Ba  (None, 16)               64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_419 (LeakyReLU)  (None, 16)               0         \n",
      "                                                                 \n",
      " dense_524 (Dense)           (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,977\n",
      "Trainable params: 1,881\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "from numpy.random import seed\n",
    "\n",
    "bounds = [{'name': 'latent_dim', 'type': 'discrete', 'domain':(3, 4, 5, 6, 7, 8)},\n",
    "          {'name': 'outer_layer_width', 'type': 'discrete', 'domain':(16, 32, 64, 128)},\n",
    "          {'name': 'inner_layer_width', 'type': 'discrete', 'domain':(16, 32, 64)},\n",
    "          {'name': 'beta', 'type': 'continuous', 'domain':(0,1)}]\n",
    "\n",
    "max_iter = 100\n",
    "myProblem = GPyOpt.methods.BayesianOptimization(main, domain=bounds)\n",
    "myProblem.run_optimization(max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4845f32d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation results... \n",
      "     latent_dim  outer_layer_width  inner_layer_width      beta  efficiency\n",
      "0           3.0               32.0               64.0  0.266643     0.00008\n",
      "1           4.0              128.0               64.0  0.841175     0.51408\n",
      "2           8.0               16.0               64.0  0.325322     0.74484\n",
      "3           3.0               16.0               64.0  0.284927     0.30786\n",
      "4           3.0               64.0               32.0  0.693627     0.00006\n",
      "5           8.0               16.0               64.0  0.620262     0.75102\n",
      "6           8.0               16.0               32.0  0.882036     0.67602\n",
      "7           3.0               16.0               32.0  0.000000     0.39872\n",
      "8           8.0               16.0               32.0  1.000000     0.08370\n",
      "9           8.0               16.0               64.0  0.474908     0.65730\n",
      "10          8.0               16.0               64.0  0.476619     0.93764\n",
      "11          8.0               16.0               64.0  0.481481     0.76584\n",
      "12          8.0               16.0               64.0  0.488674     0.88256\n",
      "13          8.0               16.0               64.0  0.537404     0.00006\n",
      "14          8.0               16.0               64.0  0.486869     0.77650\n",
      "15          8.0               16.0               64.0  0.485670     0.60480\n",
      "16          8.0               16.0               64.0  0.478861     0.50322\n",
      "17          8.0               16.0               64.0  0.337404     0.48328\n",
      "18          8.0               16.0               64.0  0.488504     0.51628\n",
      "19          8.0               16.0               64.0  0.627864     0.57956\n",
      "20          8.0               16.0               64.0  0.308262     0.32912\n",
      "21          8.0               16.0               64.0  0.476995     0.95702\n",
      "22          8.0               16.0               64.0  0.607298     0.98094\n",
      "23          8.0               16.0               32.0  0.692367     0.95780\n",
      "24          8.0               16.0               32.0  0.692367     0.72522\n",
      "25          8.0               16.0               64.0  0.602934     0.58656\n",
      "26          8.0               16.0               32.0  0.447730     0.80818\n",
      "27          8.0               16.0               32.0  0.692368     0.94520\n",
      "28          8.0               16.0               32.0  0.692413     0.92566\n",
      "29          8.0               16.0               32.0  0.701695     0.49972\n",
      "30          8.0               16.0               64.0  0.323700     0.84700\n",
      "31          8.0               16.0               32.0  0.688042     0.81204\n",
      "32          8.0               16.0               32.0  0.689916     0.93980\n",
      "33          8.0               16.0               32.0  0.458050     0.94230\n",
      "34          8.0               16.0               32.0  0.460056     0.83142\n",
      "35          8.0               16.0               32.0  0.455557     0.74336\n",
      "36          8.0               16.0               32.0  0.685135     0.60434\n",
      "37          8.0               16.0               32.0  0.466467     0.73338\n",
      "38          8.0               16.0               32.0  0.458255     0.77168\n",
      "39          8.0               16.0               32.0  0.435269     0.94026\n",
      "40          8.0               16.0               64.0  0.460861     0.72980\n",
      "41          8.0               16.0               32.0  0.431767     0.54118\n",
      "42          8.0               16.0               32.0  0.441299     0.92886\n",
      "43          8.0               16.0               64.0  0.611761     0.76618\n",
      "44          8.0               16.0               32.0  0.497750     0.86846\n",
      "45          8.0               16.0               32.0  0.521078     0.00008\n",
      "46          8.0               16.0               64.0  0.438467     0.98530\n",
      "47          8.0               16.0               64.0  0.435491     0.60830\n",
      "48          8.0               16.0               32.0  0.854017     0.84438\n",
      "49          8.0               16.0               32.0  0.441370     0.89624\n",
      "50          8.0               16.0               32.0  0.843666     0.00004\n",
      "51          3.0               32.0               64.0  0.106710     0.11682\n",
      "52          8.0               16.0               32.0  0.439745     0.75680\n",
      "53          4.0               32.0               64.0  0.566545     0.00008\n",
      "54          8.0               16.0               32.0  0.490383     0.14744\n",
      "55          8.0               16.0               32.0  0.443099     0.77700\n",
      "56          3.0               64.0               32.0  0.439344     0.61578\n",
      "57          8.0               16.0               32.0  0.296349     0.63742\n",
      "58          8.0               16.0               32.0  0.691295     0.92964\n",
      "59          8.0               16.0               64.0  0.442454     0.41226\n",
      "60          8.0               16.0               32.0  0.262874     0.27482\n",
      "61          8.0               16.0               32.0  0.501110     0.19268\n",
      "62          8.0               16.0               32.0  0.436270     0.74166\n",
      "63          8.0               16.0               64.0  0.492622     0.93838\n",
      "64          8.0               16.0               32.0  0.496288     0.67116\n",
      "65          8.0               16.0               64.0  0.493801     0.60600\n",
      "66          8.0               16.0               32.0  0.321012     0.58336\n",
      "67          8.0               16.0               32.0  0.918543     0.98872\n",
      "68          8.0               16.0               64.0  0.608147     0.46420\n",
      "69          8.0               16.0               32.0  0.918543     0.00002\n",
      "70          8.0               16.0               32.0  0.687537     0.12348\n",
      "71          8.0               16.0               32.0  0.694202     0.00010\n",
      "72          8.0               16.0               32.0  0.446738     0.88296\n",
      "73          8.0               16.0               64.0  0.477010     0.00004\n",
      "74          8.0               16.0               32.0  0.446688     0.15830\n",
      "75          8.0               16.0               32.0  0.445989     0.95970\n",
      "76          8.0               16.0               32.0  0.446025     0.58026\n",
      "77          8.0               16.0               32.0  0.445983     0.83158\n",
      "78          8.0               16.0               32.0  0.446001     0.88966\n",
      "79          8.0               16.0               32.0  0.446031     0.98678\n",
      "80          8.0               16.0               32.0  0.689764     0.93238\n",
      "81          8.0               16.0               32.0  0.446106     0.69238\n",
      "82          8.0               16.0               32.0  0.446085     0.00000\n",
      "83          8.0               16.0               32.0  0.445798     0.74382\n",
      "84          8.0               16.0               32.0  0.445788     0.00006\n",
      "85          8.0               16.0               32.0  0.445819     0.25486\n",
      "86          8.0               16.0               32.0  0.445868     0.83816\n",
      "87          8.0               16.0               32.0  0.445835     0.96420\n",
      "88          8.0               16.0               32.0  0.690159     0.83638\n",
      "89          8.0               16.0               32.0  0.445801     0.04840\n",
      "90          8.0               16.0               32.0  0.445881     0.68438\n",
      "91          8.0               16.0               32.0  0.445863     0.13956\n",
      "92          8.0               16.0               32.0  0.690359     0.00006\n",
      "93          8.0               16.0               32.0  0.469199     0.17764\n",
      "94          8.0               16.0               64.0  0.545937     0.75144\n",
      "95          8.0               16.0               64.0  0.567605     0.00000\n",
      "96          8.0               16.0               32.0  0.257552     0.68760\n",
      "97          8.0               16.0               32.0  0.330363     0.70644\n",
      "98          8.0               16.0               32.0  0.399845     0.91340\n",
      "99          8.0               16.0               32.0  0.350589     0.96122\n",
      "100         8.0               16.0               32.0  0.277207     0.94750\n",
      "101         8.0               16.0               32.0  0.246665     0.09780\n",
      "102         8.0               16.0               32.0  0.307538     0.78000\n",
      "103         8.0               16.0               32.0  0.359609     0.64666\n",
      "104         8.0               16.0               32.0  0.231256     0.68634\n",
      "The value of (latent_dim, outer_layer width, inner_layer_width, beta) that maximizes efficiency is: \n",
      "[ 8.         16.         32.          0.91854322]\n",
      "The the max efficiency found is: 0.98872\n"
     ]
    }
   ],
   "source": [
    "# evaluate results of optimization\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "myProblem.plot_convergence()\n",
    "print('Writing evaluation results... ')\n",
    "param1 = myProblem.get_evaluations()[0][:,0].flatten()\n",
    "param2 = myProblem.get_evaluations()[0][:,1].flatten()\n",
    "param3 = myProblem.get_evaluations()[0][:,2].flatten()\n",
    "param4 = myProblem.get_evaluations()[0][:,3].flatten()\n",
    "out = -1*myProblem.get_evaluations()[1][:].flatten()\n",
    "opt_results = {'latent_dim': param1,\n",
    "               'outer_layer_width': param2,\n",
    "               'inner_layer_width': param3,\n",
    "               'beta': param4,\n",
    "               'efficiency': out}\n",
    "\n",
    "evals = pd.DataFrame(opt_results)\n",
    "print(evals)\n",
    "\n",
    "print('The value of (latent_dim, outer_layer width, inner_layer_width, beta) that maximizes efficiency is: \\n'+str(myProblem.x_opt))\n",
    "print('The the max efficiency found is: '+str(-1*myProblem.fx_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fcb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
