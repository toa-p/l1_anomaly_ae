{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26598827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Tesla P100-PCIE-12GB, compute capability 6.0\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Instal MPL HEP for style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "from model import build_AE, build_VAE, Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9013e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "####configuration####\n",
    "global input_qcd, input_bsm, events, load_pickle, input_pickle, output_pfile, \\\n",
    "        output_model_h5, output_model_json, output_history, output_result, \\\n",
    "        model_type, batch_size, n_epochs\n",
    "\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/BSM_preprocessed.h5\"\n",
    "events = 1000000\n",
    "load_pickle=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "batch_size= 1024\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0127d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(latent_dim):\n",
    "    \n",
    "    if(load_pickle):\n",
    "        if(input_pickle==''):\n",
    "            print('Please provide input pickle files')\n",
    "        with open(input_pickle, 'rb') as f:\n",
    "            X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "            bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "                  'VectorZPrimeToQQ__M100',\n",
    "                  'VectorZPrimeToQQ__M200',\n",
    "                  'VBF_HToInvisible_M125',\n",
    "                  'VBF_HToInvisible_M125_private',\n",
    "                  'ZprimeToZH_MZprime1000',\n",
    "                  'ZprimeToZH_MZprime800',\n",
    "                  'ZprimeToZH_MZprime600',\n",
    "                  'GluGluToHHTo4B',\n",
    "                  'HTo2LongLivedTo4mu_1000',\n",
    "                  'HTo2LongLivedTo4mu_125_12',\n",
    "                  'HTo2LongLivedTo4mu_125_25',\n",
    "                  'HTo2LongLivedTo4mu_125_50',\n",
    "                  'VBFHToTauTau',\n",
    "                  'VBF_HH']\n",
    "    else:\n",
    "        if(input_qcd==''or input_bsm==''):\n",
    "            print('Please provide input H5 files')\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, output_pfile,True)\n",
    "        \n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1], latent_dim)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1], latent_dim)\n",
    "        model = VAE(encoder, decoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    \n",
    "    #load model\n",
    "    model_dir = output_model_h5.split('.')[0]\n",
    "    if(model_type=='AE'):\n",
    "        with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "        ae = tf.keras.models.model_from_json(config)    \n",
    "        ae.load_weights(model_dir+\"/model.h5\")\n",
    "        ae.summary()\n",
    "        model = AE(ae)\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "        encoder.summary()\n",
    "        decoder.summary()\n",
    "        model = VAE(encoder, decoder)\n",
    "    \n",
    "    from end2end import get_results\n",
    "    data_file = input_pickle\n",
    "    outdir = output_model_h5.split('.')[0]\n",
    "    if not load_pickle: data_file = output_pfile\n",
    "    results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim)   \n",
    "    \n",
    "    for key in results.keys():\n",
    "        results[key]['loss'] = results[key]['loss'][np.isfinite(results[key]['loss'])]\n",
    "        results[key]['total_loss'] = results[key]['total_loss'][np.isfinite(results[key]['total_loss'])]\n",
    "        results[key]['radius'] = results[key]['radius'][np.isfinite(results[key]['radius'])]\n",
    "\n",
    "    signal_eff={}\n",
    "\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        signal_eff[key]={}\n",
    "        true_label = np.concatenate(( np.ones(results[key]['loss'].shape[0]), np.zeros(results['QCD']['loss'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "\n",
    "    if(model_type=='VAE'):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['total_loss'].shape[0]), np.zeros(results['QCD']['total_loss'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "  \n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['radius'].shape[0]), np.zeros(results['QCD']['radius'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()\n",
    "\n",
    "    #return auc_loss\n",
    "    return -(tpr_loss[fpr_loss<0.000125][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b8dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading QCD\n",
      "QCD: (1000000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "from numpy.random import seed\n",
    "\n",
    "bounds = [{'name': 'latent_dim', 'type': 'discrete', 'domain':(2, 3, 4, 5, 8)}]\n",
    "\n",
    "max_iter = 30\n",
    "myProblem = GPyOpt.methods.BayesianOptimization(main, bounds)\n",
    "myProblem.run_optimization(max_iter)\n",
    "\n",
    "# evaluate results of optimization\n",
    "myProblem.plot_convergence()\n",
    "print('Writing evaluation results... ')\n",
    "ins = myProblem.get_evaluations()[1].flatten()\n",
    "outs = myProblem.get_evaluations()[0].flatten()\n",
    "evals = pd.DataFrame(ins, outs)\n",
    "print(evals.sort_index())\n",
    "print('The minumum value is %.4f (x = %.4f)' % (myProblem.fx_opt, myProblem.x_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773dd0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
