{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bac3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  Tesla P100-PCIE-12GB, compute capability 6.0\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Instal MPL HEP for style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "from autoencoder_classes import AE,VAE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from data_preprocessing import prepare_data\n",
    "#from model import build_AE, build_VAE, Sampling\n",
    "from model import Sampling\n",
    "\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8eafd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####configuration####\n",
    "global input_qcd, input_bsm, events, load_pickle, input_pickle, output_pfile, \\\n",
    "        output_model_h5, output_model_json, output_history, output_result, \\\n",
    "        model_type, batch_size, n_epochs\n",
    "\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_bsm=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/BSM_preprocessed.h5\"\n",
    "events = 500000\n",
    "load_pickle=False\n",
    "input_pickle=\"data.pickle\"\n",
    "output_pfile=\"data.pickle\"\n",
    "output_model_h5='model.h5'\n",
    "output_model_json='model.json'\n",
    "output_history='history.h5'\n",
    "output_result='results.h5'\n",
    "model_type='VAE'\n",
    "batch_size= 1024\n",
    "n_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478765bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hyperparameters):\n",
    "    \n",
    "    latent_dim = hyperparameters[:,0]\n",
    "    outer_layer_width = hyperparameters[:,1]\n",
    "    #outer_layer_width = 32\n",
    "    #inner_layer_width = hyperparameters[:,1]\n",
    "    inner_layer_width = 16\n",
    "    \n",
    "    \n",
    "    def build_AE(input_shape,latent_dim, outer_layer_width, inner_layer_width):\n",
    "        inputArray = Input(shape=(input_shape))\n",
    "        x = BatchNormalization()(inputArray)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        # x = BatchNormalization()(x)\n",
    "        # encoder = LeakyReLU(alpha=0.3)(x)\n",
    "        #decoder\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(encoder)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        decoder = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "\n",
    "        #create autoencoder\n",
    "        autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "        autoencoder.summary()\n",
    "        # ae = AE(autoencoder)\n",
    "        # ae.compile(optimizer=keras.optimizers.Adam(lr=0.00001))\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "    def build_VAE(input_shape, latent_dim, outer_layer_width, inner_layer_width):\n",
    "    \n",
    "        #encoder\n",
    "        inputArray = Input(shape=(input_shape))\n",
    "        x = BatchNormalization()(inputArray)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        mu = Dense(latent_dim, name = 'latent_mu', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        logvar = Dense(latent_dim, name = 'latent_logvar', kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Use reparameterization trick to ensure correct gradient\n",
    "        z = Sampling()([mu, logvar])\n",
    "\n",
    "        # Create encoder\n",
    "        encoder = Model(inputArray, [mu, logvar, z], name='encoder')\n",
    "        encoder.summary()\n",
    "\n",
    "        #decoder\n",
    "        d_input = Input(shape=(int(latent_dim),), name='decoder_input')\n",
    "        x = Dense(inner_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(d_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        x = Dense(outer_layer_width, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        dec = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "\n",
    "        # Create decoder \n",
    "        decoder = Model(d_input, dec, name='decoder')\n",
    "        decoder.summary()\n",
    "    \n",
    "        # vae = VAE(encoder, decoder)\n",
    "        # vae.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        return encoder,decoder\n",
    "    \n",
    "    if(load_pickle):\n",
    "        if(input_pickle==''):\n",
    "            print('Please provide input pickle files')\n",
    "        with open(input_pickle, 'rb') as f:\n",
    "            X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = pickle.load(f)\n",
    "            bsm_labels=['VectorZPrimeToQQ__M50',\n",
    "                  'VectorZPrimeToQQ__M100',\n",
    "                  'VectorZPrimeToQQ__M200',\n",
    "                  'VBF_HToInvisible_M125',\n",
    "                  'VBF_HToInvisible_M125_private',\n",
    "                  'ZprimeToZH_MZprime1000',\n",
    "                  'ZprimeToZH_MZprime800',\n",
    "                  'ZprimeToZH_MZprime600',\n",
    "                  'GluGluToHHTo4B',\n",
    "                  'HTo2LongLivedTo4mu_1000',\n",
    "                  'HTo2LongLivedTo4mu_125_12',\n",
    "                  'HTo2LongLivedTo4mu_125_25',\n",
    "                  'HTo2LongLivedTo4mu_125_50',\n",
    "                  'VBFHToTauTau',\n",
    "                  'VBF_HH']\n",
    "    else:\n",
    "        if(input_qcd==''or input_bsm==''):\n",
    "            print('Please provide input H5 files')\n",
    "        X_train_flatten, X_train_scaled, X_test_flatten, X_test_scaled, bsm_data, bsm_target, pt_scaler, bsm_labels = prepare_data(input_qcd, input_bsm, events, output_pfile,True)\n",
    "        \n",
    "    if(model_type=='AE'):\n",
    "        autoencoder = build_AE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = AE(autoencoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = build_VAE(X_train_flatten.shape[-1], latent_dim, outer_layer_width, inner_layer_width)\n",
    "        model = VAE(encoder, decoder)\n",
    "        model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "        callbacks=[]\n",
    "        callbacks.append(ReduceLROnPlateau(monitor='val_loss',  factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "        callbacks.append(TerminateOnNaN())\n",
    "        callbacks.append(NeptuneMonitor())\n",
    "        callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=10, restore_best_weights=True))\n",
    "\n",
    "    history = model.fit(X_train_flatten, X_train_scaled,\n",
    "                        epochs=n_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    del X_train_flatten, X_train_scaled\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if(output_model_h5!=''):\n",
    "        if(model_type=='VAE'):\n",
    "            model.save(os.path.join(os.getcwd(),output_model_h5.split('.')[0]))\n",
    "        else:\n",
    "            model_json = autoencoder.to_json()\n",
    "            with open(output_model_json, 'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            autoencoder.save_weights(output_model_h5)\n",
    "\n",
    "\n",
    "    if(output_history!=''):\n",
    "        with open(output_history, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "    \n",
    "    #load model\n",
    "    model_dir = output_model_h5.split('.')[0]\n",
    "    if(model_type=='AE'):\n",
    "        with open(model_dir+\"/model.json\", 'r') as jsonfile: config = jsonfile.read()\n",
    "        ae = tf.keras.models.model_from_json(config)    \n",
    "        ae.load_weights(model_dir+\"/model.h5\")\n",
    "        ae.summary()\n",
    "        model = AE(ae)\n",
    "    elif(model_type=='VAE'):\n",
    "        encoder, decoder = VAE.load(model_dir, custom_objects={'Sampling': Sampling})\n",
    "        encoder.summary()\n",
    "        decoder.summary()\n",
    "        model = VAE(encoder, decoder)\n",
    "    \n",
    "    from end2end import get_results\n",
    "    data_file = input_pickle\n",
    "    outdir = output_model_h5.split('.')[0]\n",
    "    if not load_pickle: data_file = output_pfile\n",
    "    results = get_results(input_qcd,input_bsm,data_file,outdir,events,model_type,latent_dim)   \n",
    "    \n",
    "    for key in results.keys():\n",
    "        results[key]['loss'] = results[key]['loss'][np.isfinite(results[key]['loss'])]\n",
    "        results[key]['total_loss'] = results[key]['total_loss'][np.isfinite(results[key]['total_loss'])]\n",
    "        results[key]['radius'] = results[key]['radius'][np.isfinite(results[key]['radius'])]\n",
    "\n",
    "    signal_eff={}\n",
    "\n",
    "    for key in results.keys():\n",
    "        if key=='QCD': continue\n",
    "        signal_eff[key]={}\n",
    "        true_label = np.concatenate(( np.ones(results[key]['loss'].shape[0]), np.zeros(results['QCD']['loss'].shape[0]) ))\n",
    "        pred_loss = np.concatenate(( results[key]['loss'], results['QCD']['loss'] ))\n",
    "        fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "        signal_eff[key]['MSE_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "        auc_loss = auc(fpr_loss, tpr_loss)\n",
    "\n",
    "    if(model_type=='VAE'):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['total_loss'].shape[0]), np.zeros(results['QCD']['total_loss'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['total_loss'], results['QCD']['total_loss'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['KL_loss']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "\n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "  \n",
    "        for key in results.keys():\n",
    "            if key=='QCD': continue\n",
    "\n",
    "            true_label = np.concatenate(( np.ones(results[key]['radius'].shape[0]), np.zeros(results['QCD']['radius'].shape[0]) ))\n",
    "            pred_loss = np.concatenate(( results[key]['radius'], results['QCD']['radius'] ))\n",
    "            fpr_loss, tpr_loss, threshold_loss = roc_curve(true_label, pred_loss)\n",
    "            signal_eff[key]['radius']=tpr_loss[fpr_loss<0.000125][-1]\n",
    "        \n",
    "        \n",
    "            auc_loss = auc(fpr_loss, tpr_loss)\n",
    "    \n",
    "    signal_eff_pd = pd.DataFrame.from_dict(signal_eff).transpose()\n",
    "\n",
    "    #return auc_loss\n",
    "    return -(tpr_loss[fpr_loss<0.000125][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1acea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 14:55:15.755015: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-28 14:55:21.088532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1579 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           928         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16)          64          ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 16)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           272         ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16)          64          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 15s 11ms/step - loss: inf - reconstruction_loss: 0.5135 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           928         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16)          64          ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 16)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           272         ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16)          64          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Instal MPL HEP for style formating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 57)          228         ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           928         ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16)          64          ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 16)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           272         ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16)          64          ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 8)            464         ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8)           32          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 8)            0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           144         ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16)          64          ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 7)            119         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 7)            119         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 7)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 825\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 17:00 - loss: 2.5496 - reconstruction_loss: 0.6352 - kl_loss: 1.9144Batch 4: Invalid loss, terminating training\n",
      "196/196 [==============================] - 7s 7ms/step - loss: inf - reconstruction_loss: 0.5832 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 8)            464         ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8)           32          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 8)            0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           144         ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16)          64          ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 7)            119         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 7)            119         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 7)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 825\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 57)          228         ['input_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 8)            464         ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8)           32          ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 8)            0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           144         ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16)          64          ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16)           0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 7)            119         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 7)            119         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_1 (Sampling)          (None, 7)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 825\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 8)            464         ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8)           32          ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 8)            0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 16)           144         ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16)          64          ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 16)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 906\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 777\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 22:41 - loss: 2.3570 - reconstruction_loss: 0.6060 - kl_loss: 1.7510Batch 1: Invalid loss, terminating training\n",
      "196/196 [==============================] - 8s 6ms/step - loss: inf - reconstruction_loss: 0.4905 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 8)            464         ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8)           32          ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 8)            0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 16)           144         ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16)          64          ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 16)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 906\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 777\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 57)          228         ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 8)            464         ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8)           32          ['dense_10[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 8)            0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 16)           144         ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16)          64          ['dense_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 16)           0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " sampling_2 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 906\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 777\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 57)          228         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 16)           928         ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16)          64          ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 16)           272         ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16)          64          ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 16)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            51          ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            51          ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,658\n",
      "Trainable params: 1,480\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                64        \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,369\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 13s 26ms/step - loss: 0.8967 - reconstruction_loss: 0.4322 - kl_loss: 0.2180 - val_loss: 0.4028 - val_reconstruction_loss: 0.3144 - val_kl_loss: 0.0884 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.3630 - reconstruction_loss: 0.2966 - kl_loss: 0.0480 - val_loss: 0.3223 - val_reconstruction_loss: 0.2911 - val_kl_loss: 0.0312 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.3149 - reconstruction_loss: 0.2888 - kl_loss: 0.0215 - val_loss: 0.3021 - val_reconstruction_loss: 0.2861 - val_kl_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.3010 - reconstruction_loss: 0.2866 - kl_loss: 0.0118 - val_loss: 0.2942 - val_reconstruction_loss: 0.2847 - val_kl_loss: 0.0096 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2937 - reconstruction_loss: 0.2857 - kl_loss: 0.0074 - val_loss: 0.2902 - val_reconstruction_loss: 0.2840 - val_kl_loss: 0.0062 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2916 - reconstruction_loss: 0.2852 - kl_loss: 0.0050 - val_loss: 0.2878 - val_reconstruction_loss: 0.2835 - val_kl_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2880 - reconstruction_loss: 0.2847 - kl_loss: 0.0036 - val_loss: 0.2863 - val_reconstruction_loss: 0.2832 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2864 - reconstruction_loss: 0.2843 - kl_loss: 0.0027 - val_loss: 0.2852 - val_reconstruction_loss: 0.2828 - val_kl_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2858 - reconstruction_loss: 0.2842 - kl_loss: 0.0020 - val_loss: 0.2844 - val_reconstruction_loss: 0.2826 - val_kl_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2860 - reconstruction_loss: 0.2840 - kl_loss: 0.0016 - val_loss: 0.2839 - val_reconstruction_loss: 0.2825 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 0.2854 - reconstruction_loss: 0.2839 - kl_loss: 0.0013 - val_loss: 0.2835 - val_reconstruction_loss: 0.2823 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2849 - reconstruction_loss: 0.2836 - kl_loss: 0.0010 - val_loss: 0.2832 - val_reconstruction_loss: 0.2822 - val_kl_loss: 9.5536e-04 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2845 - reconstruction_loss: 0.2836 - kl_loss: 8.5827e-04 - val_loss: 0.2829 - val_reconstruction_loss: 0.2821 - val_kl_loss: 7.8725e-04 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2839 - reconstruction_loss: 0.2835 - kl_loss: 7.1128e-04 - val_loss: 0.2827 - val_reconstruction_loss: 0.2821 - val_kl_loss: 6.5513e-04 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2838 - reconstruction_loss: 0.2833 - kl_loss: 5.9804e-04 - val_loss: 0.2825 - val_reconstruction_loss: 0.2820 - val_kl_loss: 5.4715e-04 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2834 - reconstruction_loss: 0.2832 - kl_loss: 5.0422e-04 - val_loss: 0.2823 - val_reconstruction_loss: 0.2819 - val_kl_loss: 4.5537e-04 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2836 - reconstruction_loss: 0.2831 - kl_loss: 4.2740e-04 - val_loss: 0.2822 - val_reconstruction_loss: 0.2818 - val_kl_loss: 3.9536e-04 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2829 - reconstruction_loss: 0.2831 - kl_loss: 3.6678e-04 - val_loss: 0.2821 - val_reconstruction_loss: 0.2818 - val_kl_loss: 3.3747e-04 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2859 - reconstruction_loss: 0.2830 - kl_loss: 3.0886e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2817 - val_kl_loss: 2.9143e-04 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2847 - reconstruction_loss: 0.2830 - kl_loss: 2.7343e-04 - val_loss: 0.2819 - val_reconstruction_loss: 0.2817 - val_kl_loss: 2.5147e-04 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 4s 20ms/step - loss: 0.2837 - reconstruction_loss: 0.2830 - kl_loss: 2.3348e-04 - val_loss: 0.2818 - val_reconstruction_loss: 0.2816 - val_kl_loss: 2.1891e-04 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.2816 - reconstruction_loss: 0.2829 - kl_loss: 2.0139e-04 - val_loss: 0.2817 - val_reconstruction_loss: 0.2815 - val_kl_loss: 1.9386e-04 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.2829 - reconstruction_loss: 0.2829 - kl_loss: 1.8212e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2814 - val_kl_loss: 1.6662e-04 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.2816 - reconstruction_loss: 0.2829 - kl_loss: 1.5612e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2815 - val_kl_loss: 1.5191e-04 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2813 - reconstruction_loss: 0.2828 - kl_loss: 1.4045e-04\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.2813 - reconstruction_loss: 0.2828 - kl_loss: 1.4042e-04 - val_loss: 0.2816 - val_reconstruction_loss: 0.2815 - val_kl_loss: 1.3328e-04 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.2841 - reconstruction_loss: 0.2826 - kl_loss: 1.3480e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.3032e-04 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2843 - reconstruction_loss: 0.2826 - kl_loss: 1.3284e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.3175e-04 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2837 - reconstruction_loss: 0.2827 - kl_loss: 1.2889e-04\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2836 - reconstruction_loss: 0.2827 - kl_loss: 1.2890e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2522e-04 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2816 - reconstruction_loss: 0.2827 - kl_loss: 1.2518e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2085e-04 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 4s 20ms/step - loss: 0.2833 - reconstruction_loss: 0.2826 - kl_loss: 1.2622e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2362e-04 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "194/196 [============================>.] - ETA: 0s - loss: 0.2824 - reconstruction_loss: 0.2827 - kl_loss: 1.2671e-04\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.2824 - reconstruction_loss: 0.2826 - kl_loss: 1.2667e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2186e-04 - lr: 1.0000e-05\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 0.2837 - reconstruction_loss: 0.2826 - kl_loss: 1.2565e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2264e-04 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.2824 - reconstruction_loss: 0.2827 - kl_loss: 1.2430e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2124e-04 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "194/196 [============================>.] - ETA: 0s - loss: 0.2827 - reconstruction_loss: 0.2827 - kl_loss: 1.2601e-04\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2827 - reconstruction_loss: 0.2827 - kl_loss: 1.2606e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2341e-04 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2853 - reconstruction_loss: 0.2826 - kl_loss: 1.2633e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2509e-04 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2816 - reconstruction_loss: 0.2827 - kl_loss: 1.2412e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.2095e-04 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 0.2853 - reconstruction_loss: 0.2827 - kl_loss: 1.2544e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.2480e-04 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2831 - reconstruction_loss: 0.2827 - kl_loss: 1.2478e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.2281e-04 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2810 - reconstruction_loss: 0.2827 - kl_loss: 1.2455e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.2042e-04 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 5s 23ms/step - loss: 0.2820 - reconstruction_loss: 0.2826 - kl_loss: 1.2450e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.2166e-04 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2832 - reconstruction_loss: 0.2827 - kl_loss: 1.2438e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2265e-04 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2814 - reconstruction_loss: 0.2827 - kl_loss: 1.2481e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.1994e-04 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2826 - reconstruction_loss: 0.2827 - kl_loss: 1.2520e-04 - val_loss: 0.2813 - val_reconstruction_loss: 0.2812 - val_kl_loss: 1.1964e-04 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2822 - reconstruction_loss: 0.2827 - kl_loss: 1.2469e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2150e-04 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2813 - reconstruction_loss: 0.2827 - kl_loss: 1.2469e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2186e-04 - lr: 1.0000e-06\n",
      "Epoch 46/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2821 - reconstruction_loss: 0.2826 - kl_loss: 1.2471e-04Restoring model weights from the end of the best epoch: 36.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2821 - reconstruction_loss: 0.2826 - kl_loss: 1.2468e-04 - val_loss: 0.2814 - val_reconstruction_loss: 0.2813 - val_kl_loss: 1.2245e-04 - lr: 1.0000e-06\n",
      "Epoch 00046: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 57)          228         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 16)           928         ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16)          64          ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 16)           272         ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16)          64          ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 16)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            51          ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            51          ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,658\n",
      "Trainable params: 1,480\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                64        \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,369\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 57)          228         ['input_4[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 16)           928         ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16)          64          ['dense_15[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16)           0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 16)           272         ['leaky_re_lu_12[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16)          64          ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 16)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 3)            51          ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 3)            51          ['leaky_re_lu_13[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_3 (Sampling)          (None, 3)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,658\n",
      "Trainable params: 1,480\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 16)                64        \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,369\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 57)          228         ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 8)            464         ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8)           32          ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 8)            0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 16)           144         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16)          64          ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 16)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 906\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 777\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "196/196 [==============================] - 13s 26ms/step - loss: 0.9748 - reconstruction_loss: 0.4537 - kl_loss: 0.2307 - val_loss: 0.3991 - val_reconstruction_loss: 0.3383 - val_kl_loss: 0.0609 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.3446 - reconstruction_loss: 0.3058 - kl_loss: 0.0182 - val_loss: 0.3135 - val_reconstruction_loss: 0.3019 - val_kl_loss: 0.0117 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 0.2995 - reconstruction_loss: 0.2920 - kl_loss: 0.0065 - val_loss: 0.3017 - val_reconstruction_loss: 0.2963 - val_kl_loss: 0.0054 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2988 - reconstruction_loss: 0.2894 - kl_loss: 0.0035 - val_loss: 0.2974 - val_reconstruction_loss: 0.2943 - val_kl_loss: 0.0032 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2945 - reconstruction_loss: 0.2881 - kl_loss: 0.0022 - val_loss: 0.2953 - val_reconstruction_loss: 0.2932 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2879 - reconstruction_loss: 0.2875 - kl_loss: 0.0015 - val_loss: 0.2942 - val_reconstruction_loss: 0.2928 - val_kl_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2907 - reconstruction_loss: 0.2871 - kl_loss: 0.0011 - val_loss: 0.2932 - val_reconstruction_loss: 0.2922 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2891 - reconstruction_loss: 0.2867 - kl_loss: 7.9110e-04 - val_loss: 0.2925 - val_reconstruction_loss: 0.2917 - val_kl_loss: 9.0587e-04 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2844 - reconstruction_loss: 0.2865 - kl_loss: 6.2478e-04 - val_loss: 0.2921 - val_reconstruction_loss: 0.2914 - val_kl_loss: 7.0557e-04 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2870 - reconstruction_loss: 0.2863 - kl_loss: 5.3795e-04 - val_loss: 0.2916 - val_reconstruction_loss: 0.2910 - val_kl_loss: 6.7796e-04 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2891 - reconstruction_loss: 0.2862 - kl_loss: 4.6593e-04 - val_loss: 0.2912 - val_reconstruction_loss: 0.2905 - val_kl_loss: 7.0294e-04 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 0.2864 - reconstruction_loss: 0.2859 - kl_loss: 4.4144e-04 - val_loss: 0.2908 - val_reconstruction_loss: 0.2902 - val_kl_loss: 6.6158e-04 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2884 - reconstruction_loss: 0.2858 - kl_loss: 4.6558e-04 - val_loss: 0.2904 - val_reconstruction_loss: 0.2897 - val_kl_loss: 7.4409e-04 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2885 - reconstruction_loss: 0.2855 - kl_loss: 4.7362e-04 - val_loss: 0.2896 - val_reconstruction_loss: 0.2888 - val_kl_loss: 9.1912e-04 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2838 - reconstruction_loss: 0.2853 - kl_loss: 4.9526e-04 - val_loss: 0.2892 - val_reconstruction_loss: 0.2884 - val_kl_loss: 8.6914e-04 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2843 - reconstruction_loss: 0.2851 - kl_loss: 5.4241e-04 - val_loss: 0.2891 - val_reconstruction_loss: 0.2883 - val_kl_loss: 9.0351e-04 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2868 - reconstruction_loss: 0.2851 - kl_loss: 5.9872e-04 - val_loss: 0.2882 - val_reconstruction_loss: 0.2871 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2855 - reconstruction_loss: 0.2850 - kl_loss: 5.1076e-04 - val_loss: 0.2883 - val_reconstruction_loss: 0.2872 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2863 - reconstruction_loss: 0.2849 - kl_loss: 6.8430e-04 - val_loss: 0.2876 - val_reconstruction_loss: 0.2865 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2835 - reconstruction_loss: 0.2850 - kl_loss: 5.3572e-04 - val_loss: 0.2879 - val_reconstruction_loss: 0.2869 - val_kl_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2853 - reconstruction_loss: 0.2847 - kl_loss: 6.1716e-04 - val_loss: 0.2872 - val_reconstruction_loss: 0.2862 - val_kl_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 0.2853 - reconstruction_loss: 0.2848 - kl_loss: 5.3834e-04 - val_loss: 0.2872 - val_reconstruction_loss: 0.2861 - val_kl_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2849 - reconstruction_loss: 0.2856 - kl_loss: 5.3752e-04\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2849 - reconstruction_loss: 0.2856 - kl_loss: 5.3752e-04 - val_loss: 0.2879 - val_reconstruction_loss: 0.2870 - val_kl_loss: 9.8128e-04 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2824 - reconstruction_loss: 0.2848 - kl_loss: 4.4181e-04 - val_loss: 0.2873 - val_reconstruction_loss: 0.2864 - val_kl_loss: 9.3913e-04 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2923 - reconstruction_loss: 0.2846 - kl_loss: 5.2080e-04 - val_loss: 0.2866 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2870 - reconstruction_loss: 0.2846 - kl_loss: 5.0033e-04 - val_loss: 0.2867 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2874 - reconstruction_loss: 0.2846 - kl_loss: 5.0977e-04\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.2874 - reconstruction_loss: 0.2846 - kl_loss: 5.0977e-04 - val_loss: 0.2868 - val_reconstruction_loss: 0.2854 - val_kl_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2849 - reconstruction_loss: 0.2846 - kl_loss: 5.2154e-04 - val_loss: 0.2867 - val_reconstruction_loss: 0.2856 - val_kl_loss: 0.0012 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2842 - reconstruction_loss: 0.2845 - kl_loss: 5.2574e-04 - val_loss: 0.2872 - val_reconstruction_loss: 0.2864 - val_kl_loss: 9.0126e-04 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2873 - reconstruction_loss: 0.2847 - kl_loss: 5.0910e-04\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2873 - reconstruction_loss: 0.2846 - kl_loss: 5.0766e-04 - val_loss: 0.2869 - val_reconstruction_loss: 0.2856 - val_kl_loss: 0.0013 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 0.2834 - reconstruction_loss: 0.2846 - kl_loss: 4.8865e-04 - val_loss: 0.2871 - val_reconstruction_loss: 0.2862 - val_kl_loss: 9.5281e-04 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2833 - reconstruction_loss: 0.2848 - kl_loss: 5.0719e-04 - val_loss: 0.2870 - val_reconstruction_loss: 0.2860 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2818 - reconstruction_loss: 0.2845 - kl_loss: 5.1942e-04\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2818 - reconstruction_loss: 0.2845 - kl_loss: 5.1942e-04 - val_loss: 0.2874 - val_reconstruction_loss: 0.2866 - val_kl_loss: 8.2691e-04 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2850 - reconstruction_loss: 0.2845 - kl_loss: 5.0248e-04 - val_loss: 0.2866 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2831 - reconstruction_loss: 0.2845 - kl_loss: 5.1888e-04 - val_loss: 0.2872 - val_reconstruction_loss: 0.2863 - val_kl_loss: 9.7620e-04 - lr: 1.0000e-06\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2871 - reconstruction_loss: 0.2845 - kl_loss: 5.0770e-04 - val_loss: 0.2866 - val_reconstruction_loss: 0.2854 - val_kl_loss: 0.0013 - lr: 1.0000e-06\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 3s 13ms/step - loss: 0.2829 - reconstruction_loss: 0.2846 - kl_loss: 5.0765e-04 - val_loss: 0.2872 - val_reconstruction_loss: 0.2862 - val_kl_loss: 0.0011 - lr: 1.0000e-06\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2851 - reconstruction_loss: 0.2847 - kl_loss: 5.1943e-04 - val_loss: 0.2864 - val_reconstruction_loss: 0.2852 - val_kl_loss: 0.0013 - lr: 1.0000e-06\n",
      "Epoch 39/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2921 - reconstruction_loss: 0.2847 - kl_loss: 5.1472e-04 - val_loss: 0.2868 - val_reconstruction_loss: 0.2854 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 0.2889 - reconstruction_loss: 0.2845 - kl_loss: 5.0547e-04 - val_loss: 0.2867 - val_reconstruction_loss: 0.2852 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2859 - reconstruction_loss: 0.2846 - kl_loss: 5.0927e-04 - val_loss: 0.2866 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2996 - reconstruction_loss: 0.2847 - kl_loss: 5.2035e-04 - val_loss: 0.2865 - val_reconstruction_loss: 0.2850 - val_kl_loss: 0.0016 - lr: 1.0000e-06\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2868 - reconstruction_loss: 0.2846 - kl_loss: 5.1828e-04 - val_loss: 0.2869 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2851 - reconstruction_loss: 0.2845 - kl_loss: 5.1126e-04 - val_loss: 0.2866 - val_reconstruction_loss: 0.2854 - val_kl_loss: 0.0013 - lr: 1.0000e-06\n",
      "Epoch 45/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2833 - reconstruction_loss: 0.2847 - kl_loss: 5.0939e-04 - val_loss: 0.2870 - val_reconstruction_loss: 0.2861 - val_kl_loss: 9.9915e-04 - lr: 1.0000e-06\n",
      "Epoch 46/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2858 - reconstruction_loss: 0.2846 - kl_loss: 5.1290e-04 - val_loss: 0.2868 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0013 - lr: 1.0000e-06\n",
      "Epoch 47/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2859 - reconstruction_loss: 0.2846 - kl_loss: 5.1894e-04 - val_loss: 0.2865 - val_reconstruction_loss: 0.2851 - val_kl_loss: 0.0014 - lr: 1.0000e-06\n",
      "Epoch 48/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2849 - reconstruction_loss: 0.2845 - kl_loss: 5.0009e-04Restoring model weights from the end of the best epoch: 38.\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2849 - reconstruction_loss: 0.2845 - kl_loss: 5.0009e-04 - val_loss: 0.2868 - val_reconstruction_loss: 0.2857 - val_kl_loss: 0.0012 - lr: 1.0000e-06\n",
      "Epoch 00048: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 57)          228         ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 8)            464         ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8)           32          ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 8)            0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 16)           144         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16)          64          ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 16)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 906\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 777\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 57)          228         ['input_5[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 8)            464         ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8)           32          ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 8)            0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 16)           144         ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16)          64          ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 16)           0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 4)            68          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 4)            68          ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_4 (Sampling)          (None, 4)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 906\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                80        \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 777\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 57)          228         ['input_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 8)            464         ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8)           32          ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 8)            0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 16)           144         ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16)          64          ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 16)           0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 7)            119         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 7)            119         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 7)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 825\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "  1/196 [..............................] - ETA: 24:56 - loss: 1.1624 - reconstruction_loss: 0.6333 - kl_loss: 0.5291Batch 3: Invalid loss, terminating training\n",
      "196/196 [==============================] - 9s 7ms/step - loss: inf - reconstruction_loss: 0.5983 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 57)          228         ['input_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 8)            464         ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8)           32          ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 8)            0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 16)           144         ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16)          64          ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 16)           0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 7)            119         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 7)            119         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 7)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 825\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 57)          228         ['input_6[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 8)            464         ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8)           32          ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 8)            0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 16)           144         ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16)          64          ['dense_26[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 16)           0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 7)            119         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 7)            119         ['leaky_re_lu_21[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)          (None, 7)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,170\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 162\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 16)                128       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 8)                32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 57)                513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 825\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 57)          228         ['input_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 16)           928         ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16)          64          ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 16)           0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 16)           272         ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16)          64          ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 16)           0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "Batch 0: Invalid loss, terminating training\n",
      "196/196 [==============================] - 9s 9ms/step - loss: inf - reconstruction_loss: 0.4296 - kl_loss: inf - val_loss: nan - val_reconstruction_loss: nan - val_kl_loss: inf - lr: 0.0010\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 57)          228         ['input_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 16)           928         ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16)          64          ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 16)           0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 16)           272         ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16)          64          ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 16)           0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 57)          228         ['input_7[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 16)           928         ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16)          64          ['dense_30[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_24 (LeakyReLU)     (None, 16)           0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 16)           272         ['leaky_re_lu_24[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16)          64          ['dense_31[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_25 (LeakyReLU)     (None, 16)           0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_25[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_26 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_27 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n",
      "*** OutputFile Created\n",
      "*** Reading QCD\n",
      "QCD: (500000, 19, 3)\n",
      "GluGluToHHTo4B : (50000, 19, 3)\n",
      "HTo2LongLivedTo4mu_1000 : (39851, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_12 : (40000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_25 : (13000, 19, 3)\n",
      "HTo2LongLivedTo4mu_125_50 : (40000, 19, 3)\n",
      "VBFHToTauTau : (300000, 19, 3)\n",
      "VBF_HH : (30000, 19, 3)\n",
      "VBF_HToInvisible_M125 : (291000, 19, 3)\n",
      "VBF_HToInvisible_M125_private : (488000, 19, 3)\n",
      "VectorZPrimeToQQ__M100 : (1854, 19, 3)\n",
      "VectorZPrimeToQQ__M200 : (38023, 19, 3)\n",
      "VectorZPrimeToQQ__M50 : (6285, 19, 3)\n",
      "ZprimeToZH_MZprime1000 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime600 : (50000, 19, 3)\n",
      "ZprimeToZH_MZprime800 : (50000, 19, 3)\n",
      "*** Read BSM Data\n",
      "Wrote data to a pickle file\n",
      "returned data\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 57)          228         ['input_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           928         ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16)          64          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 16)           0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 16)           272         ['leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16)          64          ['dense_36[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 16)           0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_7 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 14s 23ms/step - loss: 0.9629 - reconstruction_loss: 0.4226 - kl_loss: 0.2967 - val_loss: 0.4899 - val_reconstruction_loss: 0.3194 - val_kl_loss: 0.1706 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.4188 - reconstruction_loss: 0.3004 - kl_loss: 0.0905 - val_loss: 0.3563 - val_reconstruction_loss: 0.2941 - val_kl_loss: 0.0621 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.3437 - reconstruction_loss: 0.2912 - kl_loss: 0.0441 - val_loss: 0.3248 - val_reconstruction_loss: 0.2903 - val_kl_loss: 0.0345 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 0.3175 - reconstruction_loss: 0.2893 - kl_loss: 0.0265 - val_loss: 0.3111 - val_reconstruction_loss: 0.2891 - val_kl_loss: 0.0219 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.3041 - reconstruction_loss: 0.2884 - kl_loss: 0.0176 - val_loss: 0.3032 - val_reconstruction_loss: 0.2881 - val_kl_loss: 0.0151 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.3009 - reconstruction_loss: 0.2876 - kl_loss: 0.0126 - val_loss: 0.2986 - val_reconstruction_loss: 0.2874 - val_kl_loss: 0.0112 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 4s 23ms/step - loss: 0.2968 - reconstruction_loss: 0.2870 - kl_loss: 0.0096 - val_loss: 0.2956 - val_reconstruction_loss: 0.2869 - val_kl_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2925 - reconstruction_loss: 0.2868 - kl_loss: 0.0076 - val_loss: 0.2934 - val_reconstruction_loss: 0.2862 - val_kl_loss: 0.0072 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2907 - reconstruction_loss: 0.2862 - kl_loss: 0.0063 - val_loss: 0.2919 - val_reconstruction_loss: 0.2859 - val_kl_loss: 0.0061 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 3s 15ms/step - loss: 0.2908 - reconstruction_loss: 0.2858 - kl_loss: 0.0054 - val_loss: 0.2906 - val_reconstruction_loss: 0.2855 - val_kl_loss: 0.0050 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2910 - reconstruction_loss: 0.2854 - kl_loss: 0.0047 - val_loss: 0.2897 - val_reconstruction_loss: 0.2853 - val_kl_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2884 - reconstruction_loss: 0.2853 - kl_loss: 0.0040 - val_loss: 0.2890 - val_reconstruction_loss: 0.2850 - val_kl_loss: 0.0040 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2881 - reconstruction_loss: 0.2851 - kl_loss: 0.0037 - val_loss: 0.2883 - val_reconstruction_loss: 0.2847 - val_kl_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2868 - reconstruction_loss: 0.2851 - kl_loss: 0.0032 - val_loss: 0.2881 - val_reconstruction_loss: 0.2850 - val_kl_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2879 - reconstruction_loss: 0.2850 - kl_loss: 0.0029 - val_loss: 0.2874 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 0.2905 - reconstruction_loss: 0.2846 - kl_loss: 0.0028 - val_loss: 0.2871 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 3s 18ms/step - loss: 0.2854 - reconstruction_loss: 0.2848 - kl_loss: 0.0025 - val_loss: 0.2871 - val_reconstruction_loss: 0.2850 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2892 - reconstruction_loss: 0.2848 - kl_loss: 0.0023 - val_loss: 0.2867 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 3s 14ms/step - loss: 0.2870 - reconstruction_loss: 0.2848 - kl_loss: 0.0021 - val_loss: 0.2868 - val_reconstruction_loss: 0.2847 - val_kl_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2845 - reconstruction_loss: 0.2846 - kl_loss: 0.0019 - val_loss: 0.2864 - val_reconstruction_loss: 0.2844 - val_kl_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2850 - reconstruction_loss: 0.2845 - kl_loss: 0.0018 - val_loss: 0.2864 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2864 - reconstruction_loss: 0.2844 - kl_loss: 0.0017 - val_loss: 0.2862 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2844 - reconstruction_loss: 0.2846 - kl_loss: 0.0016 - val_loss: 0.2862 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2859 - reconstruction_loss: 0.2844 - kl_loss: 0.0015\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2859 - reconstruction_loss: 0.2844 - kl_loss: 0.0015 - val_loss: 0.2861 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 5s 25ms/step - loss: 0.2853 - reconstruction_loss: 0.2844 - kl_loss: 0.0014 - val_loss: 0.2859 - val_reconstruction_loss: 0.2844 - val_kl_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2858 - reconstruction_loss: 0.2845 - kl_loss: 0.0014 - val_loss: 0.2860 - val_reconstruction_loss: 0.2846 - val_kl_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - ETA: 0s - loss: 0.2845 - reconstruction_loss: 0.2842 - kl_loss: 0.0014\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2845 - reconstruction_loss: 0.2842 - kl_loss: 0.0014 - val_loss: 0.2861 - val_reconstruction_loss: 0.2847 - val_kl_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2856 - reconstruction_loss: 0.2843 - kl_loss: 0.0014 - val_loss: 0.2860 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0015 - lr: 1.0000e-05\n",
      "Epoch 29/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2874 - reconstruction_loss: 0.2842 - kl_loss: 0.0014 - val_loss: 0.2861 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0016 - lr: 1.0000e-05\n",
      "Epoch 30/150\n",
      "193/196 [============================>.] - ETA: 0s - loss: 0.2843 - reconstruction_loss: 0.2845 - kl_loss: 0.0014\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "196/196 [==============================] - 3s 16ms/step - loss: 0.2843 - reconstruction_loss: 0.2844 - kl_loss: 0.0014 - val_loss: 0.2859 - val_reconstruction_loss: 0.2844 - val_kl_loss: 0.0014 - lr: 1.0000e-05\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2900 - reconstruction_loss: 0.2841 - kl_loss: 0.0014 - val_loss: 0.2860 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 3s 17ms/step - loss: 0.2865 - reconstruction_loss: 0.2843 - kl_loss: 0.0014 - val_loss: 0.2861 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 33/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2874 - reconstruction_loss: 0.2843 - kl_loss: 0.0014\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "196/196 [==============================] - 4s 18ms/step - loss: 0.2873 - reconstruction_loss: 0.2842 - kl_loss: 0.0014 - val_loss: 0.2860 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 0.2879 - reconstruction_loss: 0.2844 - kl_loss: 0.0014 - val_loss: 0.2859 - val_reconstruction_loss: 0.2844 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 35/150\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2858 - reconstruction_loss: 0.2842 - kl_loss: 0.0014Restoring model weights from the end of the best epoch: 25.\n",
      "196/196 [==============================] - 4s 19ms/step - loss: 0.2858 - reconstruction_loss: 0.2843 - kl_loss: 0.0014 - val_loss: 0.2860 - val_reconstruction_loss: 0.2845 - val_kl_loss: 0.0015 - lr: 1.0000e-06\n",
      "Epoch 00035: early stopping\n",
      "saving model to /uscms_data/d3/tphan/l1_anomaly_ae/dnn/model\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 57)          228         ['input_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           928         ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16)          64          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 16)           0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 16)           272         ['leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16)          64          ['dense_36[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 16)           0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_7 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 57)          228         ['input_8[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 16)           928         ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16)          64          ['dense_35[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_28 (LeakyReLU)     (None, 16)           0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 16)           272         ['leaky_re_lu_28[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16)          64          ['dense_36[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_29 (LeakyReLU)     (None, 16)           0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " latent_mu (Dense)              (None, 6)            102         ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " latent_logvar (Dense)          (None, 6)            102         ['leaky_re_lu_29[0][0]']         \n",
      "                                                                                                  \n",
      " sampling_7 (Sampling)          (None, 6)            0           ['latent_mu[0][0]',              \n",
      "                                                                  'latent_logvar[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,760\n",
      "Trainable params: 1,582\n",
      "Non-trainable params: 178\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 6)]               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 16)                112       \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_30 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 16)               64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_31 (LeakyReLU)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,481\n",
      "Trainable params: 1,417\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Evaluating the model - splitting prediction computation in 1 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** OutputFile Created\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import GPy\n",
    "import GPyOpt\n",
    "from numpy.random import seed\n",
    "\n",
    "bounds = [{'name': 'latent_dim', 'type': 'discrete', 'domain':(2, 3, 4, 5, 6, 7, 8)},\n",
    "          {'name': 'layer_width', 'type': 'discrete', 'domain':(8, 16, 32)},]\n",
    "\n",
    "max_iter = 30\n",
    "myProblem = GPyOpt.methods.BayesianOptimization(main, domain=bounds)\n",
    "myProblem.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e324bd3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation results... \n",
      "   latent_dim  inner_layer_width  efficiency\n",
      "0         6.0               16.0     0.68030\n",
      "1         7.0                8.0     0.78144\n",
      "2         4.0                8.0     0.85394\n",
      "3         3.0               16.0     0.00022\n",
      "4         4.0                8.0     0.00008\n",
      "5         7.0                8.0     0.18220\n",
      "6         6.0               16.0     0.85038\n",
      "7         6.0               16.0     0.00012\n",
      "The value of (latent_dim, inner_layer width) that maximizes efficiency is:[4. 8.]\n",
      "The the max efficiency found is:0.85394\n"
     ]
    }
   ],
   "source": [
    "# evaluate results of optimization\n",
    "myProblem.plot_convergence()\n",
    "print('Writing evaluation results... ')\n",
    "param1 = myProblem.get_evaluations()[0][:,0].flatten()\n",
    "param2 = myProblem.get_evaluations()[0][:,1].flatten()\n",
    "out = -1*myProblem.get_evaluations()[1][:].flatten()\n",
    "opt_results = {'latent_dim': param1,\n",
    "               'inner_layer_width': param2,\n",
    "               'efficiency': out}\n",
    "\n",
    "evals = pd.DataFrame(opt_results)\n",
    "print(evals)\n",
    "\n",
    "print('The value of (latent_dim, inner_layer width) that maximizes efficiency is:'+str(myProblem.x_opt))\n",
    "print('The the max efficiency found is:'+str(-1*myProblem.fx_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a18b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
