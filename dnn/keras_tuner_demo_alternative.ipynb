{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b40e5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPL HEP for ROOT style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "tsk = tfmot.sparsity.keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.layers import (\n",
    "    Lambda,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    AveragePooling2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    ZeroPadding2D,\n",
    "    Conv2DTranspose,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Reshape,\n",
    "    Activation,\n",
    "    ReLU,\n",
    "    LeakyReLU,\n",
    "    Dropout,\n",
    "    Concatenate,\n",
    "    Cropping1D,\n",
    "    Layer,\n",
    "    )\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "\n",
    "#from autoencoder_classes import AE,VAE\n",
    "#from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "#from losses import mse_split_loss, radius, kl_loss\n",
    "#from functions import make_mse_loss_numpy\n",
    "#from data_preprocessing import prepare_data\n",
    "#from model import build_AE, build_VAE, Sampling\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss\n",
    "\n",
    "#from qkeras.quantizers import quantized_bits\n",
    "from keras.utils import tf_utils\n",
    "quantize=False\n",
    "\n",
    "import time\n",
    "ktuner_results = f\"{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1291655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_numpy(inputs, outputs):\n",
    "    \n",
    "    # remove last dimension\n",
    "    inputs = np.reshape(inputs, (inputs.shape[0],19,3))\n",
    "    outputs = np.reshape(outputs, (outputs.shape[0],19,3)) \n",
    "    \n",
    "    mask0 = inputs[:,:,0]!=0\n",
    "    mask1 = inputs[:,:,1]!=0\n",
    "    mask2 = inputs[:,:,2]!=0\n",
    "    mask = (mask0 + mask1 + mask2)*1\n",
    "    mask = np.reshape(mask, (mask.shape[0],19,1))\n",
    "    inputs = inputs*mask\n",
    "    outputs = outputs*mask\n",
    "\n",
    "    # remove zero entries\n",
    "    loss = np.mean(np.square(inputs.reshape(inputs.shape[0],57)-outputs.reshape(outputs.shape[0],57)),axis=1)\n",
    "    return loss\n",
    "\n",
    "def mse_loss(inputs, outputs):\n",
    "    # remove last dimension\n",
    "    inputs = tf.reshape(inputs, (tf.shape(inputs)[0],19,3))\n",
    "    outputs = tf.reshape(outputs, (tf.shape(outputs)[0],19,3))\n",
    "    \n",
    "    mask0 = tf.math.not_equal(inputs[:,:,0],0)\n",
    "    mask1 = tf.math.not_equal(inputs[:,:,1],0)\n",
    "    mask2 = tf.math.not_equal(inputs[:,:,2],0)\n",
    "    mask = tf.math.logical_and(mask0, mask1)\n",
    "    mask = tf.math.logical_and(mask, mask2)\n",
    "    # tf.print(mask)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.reshape(mask, (tf.shape(mask)[0],19,1))\n",
    "\n",
    "    # remove zero entries\n",
    "    loss = reco_scale*tf.reduce_mean(tf.square(inputs[:,:,:]-outputs[:,:,:])*mask)\n",
    "    return loss\n",
    "\n",
    "def kl_loss(mu, logvar, beta=None):\n",
    "    kl_loss = 1 + logvar - np.power(mu, 2) - np.exp(logvar)\n",
    "    kl_loss = np.mean(kl_loss, axis=-1) # mean over latent dimensions\n",
    "    kl_loss *= -0.5\n",
    "    if beta!=None: return beta*kl_loss\n",
    "    else: return kl_loss\n",
    "\n",
    "def make_kl(z_mean, z_log_var):\n",
    "    @tf.function\n",
    "    def kl_loss(inputs, outputs):\n",
    "        kl =  - 0.5 * (1. + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl = tf.reduce_mean(kl, axis=-1) # multiplying mse by N -> using sum (instead of mean) in kl loss (todo: try with averages)\n",
    "        return kl\n",
    "    return kl_loss\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def make_mse_kl(z_mean, z_log_var, beta):\n",
    "    kl_loss = make_kl(z_mean, z_log_var)\n",
    "    # multiplying back by N because input is so sparse -> average error very small\n",
    "    @tf.function\n",
    "    def mse_kl_loss(inputs, outputs):\n",
    "        return mse_loss(inputs, outputs) + kl_loss(inputs, outputs) if beta==0 \\\n",
    "            else (1 - beta) * mse_loss(inputs, outputs) + kl_loss(inputs, outputs)\n",
    "    return mse_kl_loss\n",
    "\n",
    "#mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def total_objective(encoder, vae, qcd_data, bsm_data):\n",
    "    def total_objective_val(y_true, y_pred):\n",
    "        # Evaluate total loss for the two datasets\n",
    "        # First compute KL losses \n",
    "        z_mean_qcd = encoder.predict(qcd_data)[0]\n",
    "        z_logvar_qcd = encoder.predict(qcd_data)[1]\n",
    "        kl_loss_qcd = kl_loss(z_mean_qcd, z_logvar_qcd)\n",
    "\n",
    "        z_mean_bsm = encoder.predict(bsm_data)[0]\n",
    "        z_logvar_bsm = encoder.predict(bsm_data)[1]\n",
    "        kl_loss_bsm = kl_loss(z_mean_bsm, z_logvar_bsm)\n",
    "\n",
    "        # Then compute reconstruction loss (MSE) \n",
    "        qcd_predict = vae.predict(qcd_data)\n",
    "        reco_loss_qcd = mse_loss_numpy(tf_utils.sync_to_numpy_or_python_type(qcd_data), tf_utils.sync_to_numpy_or_python_type(qcd_predict))\n",
    "\n",
    "        bsm_predict = vae.predict(bsm_data)\n",
    "        reco_loss_bsm = mse_loss_numpy(tf_utils.sync_to_numpy_or_python_type(bsm_data), tf_utils.sync_to_numpy_or_python_type(bsm_predict))\n",
    "\n",
    "        # Total losses are\n",
    "        total_loss_qcd = kl_loss_qcd + reco_loss_qcd\n",
    "        total_loss_bsm = kl_loss_bsm + reco_loss_bsm\n",
    "\n",
    "        total_true_val = np.concatenate((np.ones(total_loss_bsm.shape[0]), np.zeros(total_loss_qcd.shape[0])))\n",
    "        total_pred_val = np.nan_to_num(np.concatenate((total_loss_bsm, total_loss_qcd)))\n",
    "\n",
    "        total_fpr_loss, total_tpr_loss, total_threshold_loss = roc_curve(total_true_val, total_pred_val)\n",
    "        total_objective = np.interp(10**(-5), total_fpr_loss, total_tpr_loss)\n",
    "        \n",
    "        return total_objective\n",
    "    return total_objective_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58eb9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(kt.HyperModel):\n",
    "    \n",
    "    def __init__(self, beta, qcd_target, bsm_target):\n",
    "        self.beta = beta\n",
    "        self.qcd_target = qcd_target\n",
    "        self.bsm_target = bsm_target\n",
    "        \n",
    "    def build(self, hp):\n",
    "\n",
    "        input_shape=57\n",
    "        inputs = keras.Input(shape=(input_shape,))\n",
    "        before_bottleneck = 0        \n",
    "        \n",
    "        W = hp.Int(\"first_layer_width\", min_value=16, max_value=256, step=16) # width of first layer\n",
    "        num_layers = hp.Int(\"number of layers\", 2, 5) # number of layers\n",
    "        z_width = hp.Int(\"latent_dim_width\", min_value=2, max_value=12, step=1)\n",
    "\n",
    "        fixed_step = W/num_layers # decrease/increase subsequent encoder/decoder layers by this amount \n",
    "\n",
    "        x = layers.Dense(W,kernel_initializer='lecun_uniform', activation='relu')(inputs) # create first layer\n",
    "        \n",
    "        for i in range(num_layers - 1): # subsequent encoder layer widths decrease in size by i*fixed_step\n",
    "            before_bottleneck = W - (i+1)*fixed_step\n",
    "            x = layers.Dense(int(before_bottleneck), kernel_initializer='lecun_uniform', activation='relu')(x)\n",
    "\n",
    "        z_mean = layers.Dense(z_width,kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        z_logvar = layers.Dense(z_width,kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "        z = Sampling()([z_mean, z_logvar])\n",
    "        encoder = keras.Model(inputs, [z_mean, z_logvar, z], name=\"encoder\")\n",
    "        encoder.summary()\n",
    "\n",
    "        latent_inputs = keras.Input(z_width,)\n",
    "        y = layers.Dense(int(before_bottleneck), kernel_initializer='lecun_uniform', activation='relu')(latent_inputs)\n",
    "\n",
    "        for i in range(1, num_layers - 1):\n",
    "            after_bottleneck = before_bottleneck + i*fixed_step\n",
    "            y = layers.Dense(int(after_bottleneck),kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "\n",
    "        y = layers.Dense(W,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "        decoded = layers.Dense(input_shape)(y)\n",
    "        decoder = keras.Model(latent_inputs, decoded, name=\"decoder\")\n",
    "        decoder.summary()\n",
    "\n",
    "        vae_outputs = decoder(encoder(inputs)[2])\n",
    "        vae = keras.Model(inputs, vae_outputs, name = 'vae')\n",
    "        vae.summary()\n",
    "\n",
    "        obj = total_objective(encoder, vae, self.qcd_target, self.bsm_target)\n",
    "\n",
    "        vae.compile(optimizer=keras.optimizers.Adam(),\n",
    "                    loss=make_mse_kl(z_mean, z_logvar, self.beta),\n",
    "                    metrics=[mse_loss, make_kl(z_mean, z_logvar), obj])\n",
    "\n",
    "        return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eaf8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.666666666666668\n",
      "5.333333333333334\n",
      "<put latent dimension layers here>\n",
      "5.333333333333334\n",
      "10.666666666666668\n"
     ]
    }
   ],
   "source": [
    "# Sample layer generations\n",
    "W = 16\n",
    "num_layers = 3\n",
    "fixed_step = W/num_layers\n",
    "\n",
    "for i in range(num_layers-1):\n",
    "    before = W - (i+1)*fixed_step\n",
    "    print(before)\n",
    "\n",
    "print('<put latent dimension layers here>')\n",
    "    \n",
    "for i in range(num_layers-1):\n",
    "    after = before + i*fixed_step\n",
    "    print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0110fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(input_qcd, input_bsm, beta):\n",
    "    \n",
    "    # magic trick to make sure that Lambda function works\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "    output={}\n",
    "    \n",
    "    with h5py.File(input_qcd, 'r') as h5f:\n",
    "        output['ZeroBias'] = {}\n",
    "    \n",
    "        data = np.array(h5f['full_data_cyl'][:events], dtype=np.float32)\n",
    "        ET = np.array(h5f['ET'][:events], dtype=np.float32)\n",
    "        L1bit = np.array(h5f['L1bit'][:events], dtype=np.int8)\n",
    "\n",
    "        #mask saturated ET\n",
    "        mask_ET = ET<2047.5\n",
    "        ET = ET[mask_ET]\n",
    "        data = data[mask_ET]\n",
    "        L1bit = L1bit[mask_ET]\n",
    "    \n",
    "        #mask saturated PT\n",
    "        mask_0  = data[:,0,0]<2047.5\n",
    "        mask_1_9  = data[:,1:9,0]<255.5\n",
    "        mask_9_20  = data[:,9:20,0]<1023.5\n",
    "        mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "        data = data*mask[:,:,np.newaxis]\n",
    "\n",
    "        pt = np.copy(data[:,:,0])\n",
    "        eta = np.copy(data[:,:,1])\n",
    "        phi = np.copy(data[:,:,2])\n",
    "    \n",
    "        data[:,:,0] = pt*np.cos(phi)\n",
    "        data[:,:,1] = pt*np.sin(phi)\n",
    "        data[:,:,2] = pt*np.sinh(eta)\n",
    "        data_target = np.copy(data)\n",
    "\n",
    "        del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "    \n",
    "        if(norm=='ET'):\n",
    "            data_target[:,:,:] = data[:,:,:]/ET[:,None,None]\n",
    "            std_xy = (np.std(data_target[:,:,0])+np.std(data_target[:,:,1]))/2\n",
    "            std_z = np.std(data_target[:,:,2])\n",
    "            data_target[:,:,2] = data_target[:,:,2]*(std_xy/std_z)\n",
    "        elif(norm=='std'):\n",
    "            mean_qcd = np.mean(data_target, axis=0)\n",
    "            std_qcd = np.std(data_target, axis=0)\n",
    "            data_target = (data_target[:,:,:] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "\n",
    "            # mean_qcd = np.array([np.mean(data_target[:,:,0]),np.mean(data_target[:,:,1]),np.mean(data_target[:,1:20,2])])\n",
    "            # std_qcd = np.array([np.std(data_target[:,:,0]),np.std(data_target[:,:,1]),np.std(data_target[:,1:20,2])])\n",
    "            # data_target[:,:,0] = (data_target[:,:,0]-mean_qcd[0])/std_qcd[0]\n",
    "            # data_target[:,:,1] = (data_target[:,:,1]-mean_qcd[1])/std_qcd[1]\n",
    "            # data_target[:,:,2] = (data_target[:,:,2]-mean_qcd[2])/std_qcd[2] \n",
    "            data_target[:,0,2] = 0\n",
    "        else:\n",
    "            data_target[:,0,:] = data[:,0,:]/2048\n",
    "            data_target[:,1:9,:] = data[:,1:9,:]/256\n",
    "            data_target[:,9:20,:] = data[:,9:20,:]/1024\n",
    "        \n",
    "\n",
    "        X_train, output['ZeroBias']['data'], Y_train, output['ZeroBias']['target'], _ , output['ZeroBias']['ET'], _ ,output['ZeroBias']['L1bit'] =  train_test_split( data, data_target, ET,L1bit, test_size=0.5)\n",
    "\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[1]*Y_train.shape[2])\n",
    "\n",
    "        del data, data_target, ET, L1bit\n",
    "        \n",
    "    with h5py.File(input_bsm,'r') as h5f2:\n",
    "        for key in h5f2.keys():\n",
    "            if('TT' not in key[:2]) and ('haa4b_ma15_powheg' not in key) and ('GluGluToHHTo4B_cHHH1' not in key): continue\n",
    "            if len(h5f2[key].shape) < 3: continue\n",
    "            \n",
    "            output[str(key)] = {}\n",
    "            output[str(key)]['data'] = np.array(h5f2[str(key)][:events,:,:],dtype=np.float32)\n",
    "            output[str(key)]['ET'] = np.array(h5f2[str(key)+'_ET'][:events],dtype=np.float32)\n",
    "            output[str(key)]['L1bit'] = np.array(h5f2[str(key)+'_l1bit'][:events],dtype=np.int8)\n",
    "\n",
    "            #mask saturated ET\n",
    "            mask_ET = output[str(key)]['ET']<2047.5\n",
    "            output[str(key)]['ET'] = output[str(key)]['ET'][mask_ET]\n",
    "            output[str(key)]['data'] = output[str(key)]['data'][mask_ET]\n",
    "            output[str(key)]['L1bit'] = output[str(key)]['L1bit'][mask_ET]\n",
    "        \n",
    "            #mask saturated PT\n",
    "            mask_0  = output[str(key)]['data'][:,0,0]<2047.5\n",
    "            mask_1_9  = output[str(key)]['data'][:,1:9,0]<255.5\n",
    "            mask_9_20  = output[str(key)]['data'][:,9:20,0]<1023.5\n",
    "            mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "            output[str(key)]['data'] = output[str(key)]['data']*mask[:,:,np.newaxis]\n",
    "\n",
    "            pt = np.copy(output[str(key)]['data'][:,:,0])\n",
    "            eta = np.copy(output[str(key)]['data'][:,:,1])\n",
    "            phi = np.copy(output[str(key)]['data'][:,:,2])\n",
    "        \n",
    "            output[str(key)]['data'][:,:,0] = pt*np.cos(phi)\n",
    "            output[str(key)]['data'][:,:,1] = pt*np.sin(phi)\n",
    "            output[str(key)]['data'][:,:,2] = pt*np.sinh(eta)\n",
    "\n",
    "            del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "\n",
    "\n",
    "            output[str(key)]['target'] = np.copy(output[str(key)]['data'])\n",
    "            if(norm=='ET'):\n",
    "                output[str(key)]['target'] = output[str(key)]['data']/output[str(key)]['ET'][:,None,None]\n",
    "                output[str(key)]['target'][:,:,2] = output[str(key)]['target'][:,:,2]*(std_xy/std_z)\n",
    "            elif(norm=='std'):\n",
    "                output[str(key)]['target'] = (output[str(key)]['target'] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "                # output[str(key)]['target'][:,:,0]= (output[str(key)]['data'][:,:,0]-mean_qcd[0])/std_qcd[0]\n",
    "                # output[str(key)]['target'][:,:,1]= (output[str(key)]['data'][:,:,1]-mean_qcd[1])/std_qcd[1]\n",
    "                # output[str(key)]['target'][:,:,2]= (output[str(key)]['data'][:,:,2]-mean_qcd[2])/std_qcd[2]\n",
    "                output[str(key)]['target'][:,0,2] = 0\n",
    "            elif(norm=='max_PT'):\n",
    "                output[str(key)]['target'][:,0,:] = output[str(key)]['data'][:,0,:]/2048\n",
    "                output[str(key)]['target'][:,1:9,:] = output[str(key)]['data'][:,1:9,:]/256\n",
    "                output[str(key)]['target'][:,9:20,:] = output[str(key)]['data'][:,9:20,:]/1024\n",
    "                \n",
    "    qcd_target = np.copy(output['ZeroBias']['target'])\n",
    "    qcd_target = qcd_target.reshape(qcd_target.shape[0],57)\n",
    "    bsm_target = np.copy(output['haa4b_ma15_powheg']['target'])\n",
    "    bsm_target = bsm_target.reshape(bsm_target.shape[0],57)\n",
    "    print('qcd_target is a ', type(qcd_target))\n",
    "    \n",
    "    hypermodel = VAE(beta, qcd_target, bsm_target)\n",
    "    \n",
    "    ktuner = kt.BayesianOptimization(\n",
    "            hypermodel,\n",
    "            objective = kt.Objective('val_total_objective_val', direction='min'),\n",
    "            max_trials = 1,\n",
    "            executions_per_trial = 3,\n",
    "            directory = ktuner_results)\n",
    "    \n",
    "    ktuner.search_space_summary()\n",
    "    \n",
    "    ktuner.search(x=X_train,\n",
    "                 y=Y_train,\n",
    "                 epochs=150,\n",
    "                 batch_size=1024,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping('val_loss',patience=5)])\n",
    "    \n",
    "    with open(f\"ktuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ktuner, f)\n",
    "    \n",
    "    ktuner.results_summary()\n",
    "    \n",
    "    logging.info('Get the optimal hyperparameters')\n",
    "    best_hps = ktuner.get_best_hyperparameters(num_trials=5)[0]\n",
    "    logging.info('Getting and printing best hyperparameters!')\n",
    "    print(best_hps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51cb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hardqcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-ZB-h5-extended-v2/ZB_preprocessed.h5\"\n",
    "input_bsm = \"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2-120X/BSM_preprocessed.h5\"\n",
    "events=5000000\n",
    "norm = 'std'\n",
    "beta = 0.8\n",
    "reco_scale = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ced3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimization(input_qcd, input_bsm, beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
