{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b40e5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPL HEP for ROOT style formating\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "\n",
    "# import setGPU\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "tsk = tfmot.sparsity.keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.layers import (\n",
    "    Lambda,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    AveragePooling2D,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    ZeroPadding2D,\n",
    "    Conv2DTranspose,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Reshape,\n",
    "    Activation,\n",
    "    ReLU,\n",
    "    LeakyReLU,\n",
    "    Dropout,\n",
    "    Concatenate,\n",
    "    Cropping1D,\n",
    "    Layer,\n",
    "    )\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import mplhep as hep\n",
    "    hep.style.use(hep.style.ROOT)\n",
    "    print(\"Using MPL HEP for ROOT style formating\")\n",
    "except:\n",
    "    print(\"Instal MPL HEP for style formating\")\n",
    "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=[\"#DB4437\", \"#4285F4\", \"#F4B400\", \"#0F9D58\", \"purple\", \"goldenrod\", \"peru\", \"coral\",\"turquoise\",'gray','navy','m','darkgreen','fuchsia','steelblue']) \n",
    "\n",
    "#from autoencoder_classes import AE,VAE\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "from losses import mse_split_loss, radius, kl_loss\n",
    "from functions import make_mse_loss_numpy\n",
    "from data_preprocessing import prepare_data\n",
    "from model import build_AE, build_VAE, Sampling\n",
    "\n",
    "def return_total_loss(loss, bsm_t, bsm_pred):\n",
    "    total_loss = loss(bsm_t, bsm_pred.astype(np.float32))\n",
    "    return total_loss\n",
    "\n",
    "from qkeras.quantizers import quantized_bits\n",
    "from keras.utils import tf_utils\n",
    "quantize=False\n",
    "\n",
    "import time\n",
    "ktuner_results = f\"{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6721b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.total_val_loss_tracker = keras.metrics.Mean(name=\"total_val_loss\")\n",
    "        self.reconstruction_val_loss_tracker = keras.metrics.Mean(name=\"reconstruction_val_loss\")\n",
    "        self.kl_val_loss_tracker = keras.metrics.Mean(name=\"kl_val_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.total_val_loss_tracker,\n",
    "            self.reconstruction_val_loss_tracker,\n",
    "            self.kl_val_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        data_in, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data_in)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = (1-beta)*mse_loss(target, reconstruction) #one value\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = beta*tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        # tf.print(reconstruction_loss,kl_loss)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "   \n",
    "    def test_step(self, data):\n",
    "        data_in, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data_in)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = (1-beta)*mse_loss(target, reconstruction) #one value\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = beta*tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.total_val_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_val_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_val_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_val_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_val_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_val_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    @tf.function\n",
    "    def predict(self, data_in, batch_size=32, return_latent=False):\n",
    "        # print(data_in)\n",
    "        data_batch = tf.data.Dataset.from_tensor_slices((data_in))\n",
    "        output, z_mean, z_logvar = [],[],[]\n",
    "        for data in data_batch.batch(batch_size):\n",
    "            # print(data.shape)\n",
    "            z_mean_, z_logvar_, z_ = self.encoder(data)\n",
    "            output_ = self.decoder(z_)\n",
    "            output.append(output_)\n",
    "            z_mean.append(z_mean_)\n",
    "            z_logvar.append(z_logvar_)\n",
    "        # print(len(output),len(z_mean),len(z_logvar))\n",
    "        # print(output[0])\n",
    "        output = tf.concat(output,0)\n",
    "        z_mean = tf.concat(z_mean,0)\n",
    "        z_logvar = tf.concat(z_logvar,0)\n",
    "    \n",
    "        if(return_latent):\n",
    "            return tf_utils.sync_to_numpy_or_python_type(output), tf_utils.sync_to_numpy_or_python_type(z_mean), tf_utils.sync_to_numpy_or_python_type(z_logvar)\n",
    "        else:\n",
    "            return tf_utils.sync_to_numpy_or_python_type(output)\n",
    "    \n",
    "\n",
    "    \n",
    "def mse_loss(inputs, outputs):\n",
    "    # remove last dimension\n",
    "    inputs = tf.reshape(inputs, (tf.shape(inputs)[0],19,3))\n",
    "    outputs = tf.reshape(outputs, (tf.shape(outputs)[0],19,3))\n",
    "    \n",
    "    mask0 = tf.math.not_equal(inputs[:,:,0],0)\n",
    "    mask1 = tf.math.not_equal(inputs[:,:,1],0)\n",
    "    mask2 = tf.math.not_equal(inputs[:,:,2],0)\n",
    "    mask = tf.math.logical_and(mask0, mask1)\n",
    "    mask = tf.math.logical_and(mask, mask2)\n",
    "    # tf.print(mask)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.reshape(mask, (tf.shape(mask)[0],19,1))\n",
    "\n",
    "    # remove zero entries\n",
    "    loss = reco_scale*tf.reduce_mean(tf.square(inputs[:,:,:]-outputs[:,:,:])*mask)\n",
    "    return loss\n",
    "\n",
    "def mse_loss_numpy(inputs, outputs):\n",
    "    # remove last dimension\n",
    "    inputs = np.reshape(inputs, (inputs.shape[0],19,3))\n",
    "    outputs = np.reshape(outputs, (outputs.shape[0],19,3))\n",
    "    \n",
    "    mask0 = inputs[:,:,0]!=0\n",
    "    mask1 = inputs[:,:,1]!=0\n",
    "    mask2 = inputs[:,:,2]!=0\n",
    "    mask = (mask0 + mask1 + mask2)*1\n",
    "    mask = np.reshape(mask, (mask.shape[0],19,1))\n",
    "    inputs = inputs*mask\n",
    "    outputs = outputs*mask\n",
    "\n",
    "    # remove zero entries\n",
    "    loss = np.mean(np.square(inputs.reshape(inputs.shape[0],57)-outputs.reshape(outputs.shape[0],57)),axis=1)\n",
    "    return loss\n",
    "\n",
    "def radius(mean, logvar):\n",
    "    sigma = np.sqrt(np.exp(logvar))\n",
    "    radius = mean*mean/sigma/sigma\n",
    "    return np.sum(radius, axis=-1)\n",
    "\n",
    "def kl_loss(mu, logvar, beta=None):\n",
    "    kl_loss = 1 + logvar - np.square(mu) - np.exp(logvar)\n",
    "    kl_loss = np.mean(kl_loss, axis=-1) # mean over latent dimensions\n",
    "    kl_loss *= -0.5\n",
    "    if beta!=None: return beta*kl_loss\n",
    "    else: return kl_loss\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0] \n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        epsilon = tf.cast(epsilon, tf.float32)\n",
    "        temp = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        temp = tf.cast(temp, tf.float32)\n",
    "        return temp\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def total_objective(vae, output):\n",
    "    \n",
    "    for key in output.keys():\n",
    "        Y_predict, z_mean , z_logvar = vae.predict(output[key]['target'].reshape(output[key]['target'].shape[0],57),batch_size=1024*4,return_latent=True)\n",
    "        Y_predict = Y_predict.reshape(Y_predict.shape[0],19,3)\n",
    "        output[key]['reco_loss'] = mse_loss_numpy(output[key]['target'], Y_predict)\n",
    "        output[key]['kl_loss'] = kl_loss(z_mean, z_logvar)\n",
    "        output[key]['total_loss'] = output[key]['reco_loss'] + output[key]['kl_loss']\n",
    "    \n",
    "    qcd = output['ZeroBias']['target']\n",
    "    bsm = output['haa4b_ma15_powheg']['target']\n",
    "\n",
    "    total_qcd = output['ZeroBias']['total_loss']\n",
    "    total_bsm = output['haa4b_ma15_powheg']['total_loss']\n",
    "\n",
    "    total_true_val = np.concatenate((np.ones(total_bsm.shape[0]), np.zeros(total_qcd.shape[0])))\n",
    "    total_pred_val = np.nan_to_num(np.concatenate((total_bsm, total_qcd)))\n",
    "\n",
    "    total_fpr_loss, total_tpr_loss, total_threshold_loss = roc_curve(total_true_val, total_pred_val)\n",
    "    total_objective = np.interp(10**(-5), total_fpr_loss, total_tpr_loss)\n",
    "\n",
    "    return total_objective\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58eb9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(hp):\n",
    "    \n",
    "    input_shape=57\n",
    "    inputs = keras.Input(shape=(input_shape,))\n",
    "    before_bottleneck = 0\n",
    "    \n",
    "    W = hp.Int(\"first_layer_width\", min_value=16, max_value=256, step=32) # width of first layer\n",
    "    num_layers = hp.Int(\"number of layers\", 1, 5) # number of layers\n",
    "    fixed_step = W/num_layers # decrease/increase subsequent encoder/decoder layers by this amount \n",
    "\n",
    "    x = layers.Dense(W,kernel_initializer='lecun_uniform', activation='relu')(inputs) # create first layer\n",
    "\n",
    "    for i in range(num_layers - 1): # subsequent encoder layer widths decrease in size by i*fixed_step\n",
    "        before_bottleneck = W - (i+1)*fixed_step\n",
    "        x = layers.Dense(before_bottleneck, kernel_initializer='lecun_uniform', activation='relu')(x)\n",
    "        \n",
    "    z_width = hp.Int(\"latent_dim_width\", min_value=2, max_value=before_bottleneck, step=4)\n",
    "    # ^ choose latent dimension to be between 2 and the bottleneck value. \n",
    "\n",
    "    z_mean = layers.Dense(z_width,kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    z_mean = tf.cast(z_mean, tf.float32)\n",
    "    z_logvar = layers.Dense(z_width,kernel_initializer=tf.keras.initializers.HeUniform(seed=42))(x)\n",
    "    z_logvar = tf.cast(z_logvar, tf.float32)\n",
    "    z = Sampling()([z_mean, z_logvar])\n",
    "    encoder = keras.Model(inputs, [z_mean, z_logvar, z], name=\"encoder\")\n",
    "    encoder.summary()\n",
    "\n",
    "    latent_inputs = keras.Input(z_width,)\n",
    "    y = layers.Dense(z_width, kernel_initializer='lecun_uniform', activation='relu')(latent_inputs)\n",
    "    \n",
    "    for i in range(num_layers - 1):\n",
    "        after_bottleneck = before_bottleneck + i*fixed_step\n",
    "        y = layers.Dense(after_bottleneck,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "    \n",
    "    y = layers.Dense(W,kernel_initializer='lecun_uniform', activation='relu')(y)\n",
    "    decoded = layers.Dense(input_shape)(y)\n",
    "    decoder = keras.Model(latent_inputs, decoded, name=\"decoder\")\n",
    "    decoder.summary()\n",
    "    \n",
    "    vae = VAE(encoder, decoder)\n",
    "    obj = total_objective(vae, output)\n",
    "    \n",
    "    vae.compile(optimizer=keras.optimizers.Adam(),\n",
    "                metrics=[mse_loss, kl_loss(z_mean, z_logvar), obj])\n",
    "    \n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eaf8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.0\n",
      "128.0\n",
      "64.0\n",
      "<put latent dimension layers here>\n",
      "64.0\n",
      "128.0\n",
      "192.0\n"
     ]
    }
   ],
   "source": [
    "# Sample layer generations\n",
    "W = 256\n",
    "num_layers = 4\n",
    "fixed_step = W/num_layers\n",
    "\n",
    "for i in range(num_layers-1):\n",
    "    before = W - (i+1)*fixed_step\n",
    "    print(before)\n",
    "\n",
    "print('<put latent dimension layers here>')\n",
    "    \n",
    "for i in range(num_layers-1):\n",
    "    after = before + i*fixed_step\n",
    "    print(after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0110fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(input_qcd, input_bsm, beta):\n",
    "    \n",
    "    # magic trick to make sure that Lambda function works\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "    global output\n",
    "    output={}\n",
    "    \n",
    "    with h5py.File(input_qcd, 'r') as h5f:\n",
    "        output['ZeroBias'] = {}\n",
    "    \n",
    "        data = np.array(h5f['full_data_cyl'][:events], dtype=np.float32)\n",
    "        ET = np.array(h5f['ET'][:events], dtype=np.float32)\n",
    "        L1bit = np.array(h5f['L1bit'][:events], dtype=np.int8)\n",
    "\n",
    "        #mask saturated ET\n",
    "        mask_ET = ET<2047.5\n",
    "        ET = ET[mask_ET]\n",
    "        data = data[mask_ET]\n",
    "        L1bit = L1bit[mask_ET]\n",
    "    \n",
    "        #mask saturated PT\n",
    "        mask_0  = data[:,0,0]<2047.5\n",
    "        mask_1_9  = data[:,1:9,0]<255.5\n",
    "        mask_9_20  = data[:,9:20,0]<1023.5\n",
    "        mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "        data = data*mask[:,:,np.newaxis]\n",
    "\n",
    "        pt = np.copy(data[:,:,0])\n",
    "        eta = np.copy(data[:,:,1])\n",
    "        phi = np.copy(data[:,:,2])\n",
    "    \n",
    "        data[:,:,0] = pt*np.cos(phi)\n",
    "        data[:,:,1] = pt*np.sin(phi)\n",
    "        data[:,:,2] = pt*np.sinh(eta)\n",
    "        data_target = np.copy(data)\n",
    "\n",
    "        del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "    \n",
    "        if(norm=='ET'):\n",
    "            data_target[:,:,:] = data[:,:,:]/ET[:,None,None]\n",
    "            std_xy = (np.std(data_target[:,:,0])+np.std(data_target[:,:,1]))/2\n",
    "            std_z = np.std(data_target[:,:,2])\n",
    "            data_target[:,:,2] = data_target[:,:,2]*(std_xy/std_z)\n",
    "        elif(norm=='std'):\n",
    "            mean_qcd = np.mean(data_target, axis=0)\n",
    "            std_qcd = np.std(data_target, axis=0)\n",
    "            data_target = (data_target[:,:,:] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "\n",
    "            # mean_qcd = np.array([np.mean(data_target[:,:,0]),np.mean(data_target[:,:,1]),np.mean(data_target[:,1:20,2])])\n",
    "            # std_qcd = np.array([np.std(data_target[:,:,0]),np.std(data_target[:,:,1]),np.std(data_target[:,1:20,2])])\n",
    "            # data_target[:,:,0] = (data_target[:,:,0]-mean_qcd[0])/std_qcd[0]\n",
    "            # data_target[:,:,1] = (data_target[:,:,1]-mean_qcd[1])/std_qcd[1]\n",
    "            # data_target[:,:,2] = (data_target[:,:,2]-mean_qcd[2])/std_qcd[2] \n",
    "            data_target[:,0,2] = 0\n",
    "        else:\n",
    "            data_target[:,0,:] = data[:,0,:]/2048\n",
    "            data_target[:,1:9,:] = data[:,1:9,:]/256\n",
    "            data_target[:,9:20,:] = data[:,9:20,:]/1024\n",
    "        \n",
    "\n",
    "        X_train, output['ZeroBias']['data'], Y_train, output['ZeroBias']['target'], _ , output['ZeroBias']['ET'], _ ,output['ZeroBias']['L1bit'] =  train_test_split( data, data_target, ET,L1bit, test_size=0.5)\n",
    "\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[1]*Y_train.shape[2])\n",
    "\n",
    "        del data, data_target, ET, L1bit\n",
    "        \n",
    "    with h5py.File(input_bsm,'r') as h5f2:\n",
    "        for key in h5f2.keys():\n",
    "            if('TT' not in key[:2]) and ('haa4b_ma15_powheg' not in key) and ('GluGluToHHTo4B_cHHH1' not in key): continue\n",
    "            if len(h5f2[key].shape) < 3: continue\n",
    "            \n",
    "            output[str(key)] = {}\n",
    "            output[str(key)]['data'] = np.array(h5f2[str(key)][:events,:,:],dtype=np.float32)\n",
    "            output[str(key)]['ET'] = np.array(h5f2[str(key)+'_ET'][:events],dtype=np.float32)\n",
    "            output[str(key)]['L1bit'] = np.array(h5f2[str(key)+'_l1bit'][:events],dtype=np.int8)\n",
    "\n",
    "            #mask saturated ET\n",
    "            mask_ET = output[str(key)]['ET']<2047.5\n",
    "            output[str(key)]['ET'] = output[str(key)]['ET'][mask_ET]\n",
    "            output[str(key)]['data'] = output[str(key)]['data'][mask_ET]\n",
    "            output[str(key)]['L1bit'] = output[str(key)]['L1bit'][mask_ET]\n",
    "        \n",
    "            #mask saturated PT\n",
    "            mask_0  = output[str(key)]['data'][:,0,0]<2047.5\n",
    "            mask_1_9  = output[str(key)]['data'][:,1:9,0]<255.5\n",
    "            mask_9_20  = output[str(key)]['data'][:,9:20,0]<1023.5\n",
    "            mask = np.concatenate((mask_0[:,np.newaxis],mask_1_9,mask_9_20),axis=1)*1\n",
    "            output[str(key)]['data'] = output[str(key)]['data']*mask[:,:,np.newaxis]\n",
    "\n",
    "            pt = np.copy(output[str(key)]['data'][:,:,0])\n",
    "            eta = np.copy(output[str(key)]['data'][:,:,1])\n",
    "            phi = np.copy(output[str(key)]['data'][:,:,2])\n",
    "        \n",
    "            output[str(key)]['data'][:,:,0] = pt*np.cos(phi)\n",
    "            output[str(key)]['data'][:,:,1] = pt*np.sin(phi)\n",
    "            output[str(key)]['data'][:,:,2] = pt*np.sinh(eta)\n",
    "\n",
    "            del pt, eta, phi, mask_ET, mask_0, mask_1_9, mask_9_20, mask\n",
    "\n",
    "\n",
    "            output[str(key)]['target'] = np.copy(output[str(key)]['data'])\n",
    "            if(norm=='ET'):\n",
    "                output[str(key)]['target'] = output[str(key)]['data']/output[str(key)]['ET'][:,None,None]\n",
    "                output[str(key)]['target'][:,:,2] = output[str(key)]['target'][:,:,2]*(std_xy/std_z)\n",
    "            elif(norm=='std'):\n",
    "                output[str(key)]['target'] = (output[str(key)]['target'] - mean_qcd[None,:,:])/std_qcd[None,:,:]\n",
    "                # output[str(key)]['target'][:,:,0]= (output[str(key)]['data'][:,:,0]-mean_qcd[0])/std_qcd[0]\n",
    "                # output[str(key)]['target'][:,:,1]= (output[str(key)]['data'][:,:,1]-mean_qcd[1])/std_qcd[1]\n",
    "                # output[str(key)]['target'][:,:,2]= (output[str(key)]['data'][:,:,2]-mean_qcd[2])/std_qcd[2]\n",
    "                output[str(key)]['target'][:,0,2] = 0\n",
    "            elif(norm=='max_PT'):\n",
    "                output[str(key)]['target'][:,0,:] = output[str(key)]['data'][:,0,:]/2048\n",
    "                output[str(key)]['target'][:,1:9,:] = output[str(key)]['data'][:,1:9,:]/256\n",
    "                output[str(key)]['target'][:,9:20,:] = output[str(key)]['data'][:,9:20,:]/1024\n",
    "        \n",
    "    ktuner = kt.BayesianOptimization(\n",
    "            hypermodel=make_model,\n",
    "            objective = kt.Objective('val_total_objective', direction='min'),\n",
    "            max_trials = 1,\n",
    "            executions_per_trial = 3,\n",
    "            directory = ktuner_results)\n",
    "    \n",
    "    ktuner.search(x=X_train,\n",
    "                 y=Y_train,\n",
    "                 epochs=5,\n",
    "                 batch_size=1024,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[tf.keras.callbacks.EarlyStopping('val_loss',patience=5)])\n",
    "    \n",
    "    with open(f\"ktuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ktuner, f)\n",
    "    \n",
    "    ktuner.results_summary()\n",
    "    \n",
    "    logging.info('Get the optimal hyperparameters')\n",
    "    best_hps = ktuner.get_best_hyperparameters(num_trials=5)[0]\n",
    "    logging.info('Getting and printing best hyperparameters!')\n",
    "    print(best_hps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51cb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hardqcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2/QCD_preprocessed.h5\"\n",
    "input_qcd=\"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-ZB-h5-extended-v2/ZB_preprocessed.h5\"\n",
    "input_bsm = \"/eos/uscms/store/group/lpctrig/jngadiub/L1TNtupleRun3-h5-extended-v2-120X/BSM_preprocessed.h5\"\n",
    "events=500000\n",
    "norm = 'std'\n",
    "beta = 0.8\n",
    "reco_scale = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5ced3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           928         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 996\n",
      "Trainable params: 996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                48        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 57)                969       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,023\n",
      "Trainable params: 1,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 16:36:08.496568: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-14 16:36:15.590721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11303 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:65:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_111449/2112351997.py\", line 80, in predict  *\n        output = tf.concat(output,0)\n\n    InaccessibleTensorError: tf.Graph captured an external symbolic tensor. The symbolic tensor <tf.Tensor 'decoder/dense_5/BiasAdd:0' shape=(None, 57) dtype=float32> is captured by FuncGraph(name=predict, id=140070007550208), but it is defined at FuncGraph(name=Dataset_scan_scan_body, id=140070006487552). A tf.Graph is not allowed to capture symoblic tensors from another graph. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_qcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_bsm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36moptimization\u001b[0;34m(input_qcd, input_bsm, beta)\u001b[0m\n\u001b[1;32m    115\u001b[0m             output[\u001b[38;5;28mstr\u001b[39m(key)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m9\u001b[39m,:] \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;28mstr\u001b[39m(key)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m9\u001b[39m,:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    116\u001b[0m             output[\u001b[38;5;28mstr\u001b[39m(key)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m20\u001b[39m,:] \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;28mstr\u001b[39m(key)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m20\u001b[39m,:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m--> 118\u001b[0m ktuner \u001b[38;5;241m=\u001b[39m \u001b[43mkt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBayesianOptimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mObjective\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_total_objective\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutions_per_trial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mktuner_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m ktuner\u001b[38;5;241m.\u001b[39msearch(x\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[1;32m    126\u001b[0m              y\u001b[38;5;241m=\u001b[39mY_train,\n\u001b[1;32m    127\u001b[0m              epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    128\u001b[0m              batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m    129\u001b[0m              validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m    130\u001b[0m              callbacks\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)])\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mktuner_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/uscms_data/d3/tphan/miniforge3/envs/l1ad/lib/python3.9/site-packages/keras_tuner/tuners/bayesian.py:413\u001b[0m, in \u001b[0;36mBayesianOptimization.__init__\u001b[0;34m(self, hypermodel, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    401\u001b[0m ):\n\u001b[1;32m    402\u001b[0m     oracle \u001b[38;5;241m=\u001b[39m BayesianOptimizationOracle(\n\u001b[1;32m    403\u001b[0m         objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m    404\u001b[0m         max_trials\u001b[38;5;241m=\u001b[39mmax_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m         allow_new_entries\u001b[38;5;241m=\u001b[39mallow_new_entries,\n\u001b[1;32m    412\u001b[0m     )\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBayesianOptimization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/tphan/miniforge3/envs/l1ad/lib/python3.9/site-packages/keras_tuner/engine/tuner.py:111\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hypermodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial \u001b[38;5;129;01mis\u001b[39;00m Tuner\u001b[38;5;241m.\u001b[39mrun_trial:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hypermodel` if the user defines the search space in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing a `HyperModel` instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTuner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(oracle\u001b[38;5;241m.\u001b[39mobjective, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(oracle\u001b[38;5;241m.\u001b[39mobjective) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-objective is not supported, found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    123\u001b[0m             oracle\u001b[38;5;241m.\u001b[39mobjective\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m     )\n",
      "File \u001b[0;32m/uscms_data/d3/tphan/miniforge3/envs/l1ad/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:103\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, logger, overwrite)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m logger\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display \u001b[38;5;241m=\u001b[39m tuner_utils\u001b[38;5;241m.\u001b[39mDisplay(oracle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate_initial_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tuner_fname()):\n\u001b[1;32m    106\u001b[0m     tf\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReloading Tuner from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tuner_fname())\n\u001b[1;32m    108\u001b[0m     )\n",
      "File \u001b[0;32m/uscms_data/d3/tphan/miniforge3/envs/l1ad/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:132\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m scopes_once_active \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Update the recored scopes.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conditions \u001b[38;5;129;01min\u001b[39;00m hp\u001b[38;5;241m.\u001b[39mactive_scopes:\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mmake_model\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m     38\u001b[0m decoder\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     40\u001b[0m vae \u001b[38;5;241m=\u001b[39m VAE(encoder, decoder)\n\u001b[0;32m---> 41\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m     44\u001b[0m             metrics\u001b[38;5;241m=\u001b[39m[mse_loss, kl_loss(z_mean, z_logvar), obj])\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vae\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtotal_objective\u001b[0;34m(vae, output)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_objective\u001b[39m(vae, output):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 157\u001b[0m         Y_predict, z_mean , z_logvar \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m57\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreturn_latent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m         Y_predict \u001b[38;5;241m=\u001b[39m Y_predict\u001b[38;5;241m.\u001b[39mreshape(Y_predict\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m19\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    159\u001b[0m         output[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreco_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mse_loss_numpy(output[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], Y_predict)\n",
      "File \u001b[0;32m/uscms_data/d3/tphan/miniforge3/envs/l1ad/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/uscms_data/d3/tphan/miniforge3/envs/l1ad/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_111449/2112351997.py\", line 80, in predict  *\n        output = tf.concat(output,0)\n\n    InaccessibleTensorError: tf.Graph captured an external symbolic tensor. The symbolic tensor <tf.Tensor 'decoder/dense_5/BiasAdd:0' shape=(None, 57) dtype=float32> is captured by FuncGraph(name=predict, id=140070007550208), but it is defined at FuncGraph(name=Dataset_scan_scan_body, id=140070006487552). A tf.Graph is not allowed to capture symoblic tensors from another graph. Use return values, explicit Python locals or TensorFlow collections to access it. Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n"
     ]
    }
   ],
   "source": [
    "optimization(input_qcd, input_bsm, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e782eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
